---
toc: true
toc_label: "Table of Contents"
toc_icon: "cog"
toc_sticky: true

layout: post-with-comments # single
title: '[DL-05] Lect6. CNN-Convolution Neural Network'
excerpt: "[AIAS] 인하대학교 홍성은 교수님의 인공지능 응용 시스템 수업을 듣고 공부한 자료입니다."
date: 2022-02-27
last_modified_at: 2022-02-27
categories:
  - ai-deepLearning
tags: 
   - [Deep Learning, AIAS]

use_math: true
comments: true
share : false
---

AIAS-Lect6_CNN: Convolutional Neural Networks(1)



# Introduction

이번 장에서는 Convolutional Neural Networks에 대해서 알아본다.



## CNN의 등장 배경

$$
Linear\ Score\ Function:\ f=Wx\\
2-layer\ Neural\ Network:\ f=W_2max()
$$

CNN 이전까지는 위와 같은 [fully-connected layer(=MLP, multilayer perceptron)](C:\Users\dazol\Desktop\0-Typora\AIAS\Multi-Layer Perceptron.md) 방식이 주로 사용되었다.



<img src="/files/2022-02-27-DL05/image-20210411153543775.png" alt="image-20210411153543775" style="zoom:50%;" />

하지만 이러한 알고리즘을 사용하는 경우 이미지 학습이 제대로 이루어지지 않는다는 단점이 있었다. 예를 들어 이미지의 경우 (채널을 무시한다고 할 때) 2차원 array의 형태로 구성되어 있다. 이때 학습시키기위해 image의 pixel value를 <u>flatten시킨다면 본래 이미지의 공간상의 영역을 제대로 표현하지 못해 좋은 구조라고 할 수 없다</u>.

이러한 니즈에서 나타난 구조가 바로 **convolutional neural networks(CNNs)**구조이다. <u>공간상의 정보를 유지하면서 hidden vector를 표현한다는 것이 특징</u>이다. 






# Convolutional Neural Networks(CNN)

## Fully Connected Layer

![image-20210413234950744](/files/2022-02-27-DL05/image-20210413234950744.png)

**Fully connected layer**란 하나의 layer의 모든 뉴런이 그 다음 layer의 모든 뉴런과 연결된 상태를 의미한다. 이때 이 layer는 **flatten**되어 <u>1차원 배열</u>의 형태라는 특징을 갖는다. fully connected layer는 기존의 image 인식에 주로 사용되었던 구조인데, 몇 가지 한계가 있다.

1. image가 고해상도인 경우, input layer의 뉴런의 개수가 증가하므로 전체 <u>parameters의 수</u>가 급격히 증가한다.
2. 이미지 pixel을 flatten한 형태이므로 영상 전체의 관계(**topology**)를 고려하지 못하여 <u>입력 데이터의 변형에 취약</u>하다. (따라서 굉장히 <u>많은 학습 데이터를 필요</u>로한다.)



## Convolution Layer

**Convolution layer(CL)**은 fully connected layer(FCL)의 단점을 보완하여 이미지 데이터에 대해 보다 나은 성능을 도출하기 위해 고안된 layer 모델이다. FCL은 flatten된 1차원 배열의 형태이지만 CL은 <u>입력 데이터의 형태를 유지한 3차원 배열의 구조</u>를 갖는다. 한편 CL은 입력 데이터의 모든 뉴런에 연결되는 FCL와 달리, **filter**내에 존재하는 뉴런에만 연결된다. 아래의 이미지에서 파란색은 입력 데이터를, 짙은 파란색은 filter 영역을, 하얀색은 연산 결과를 의미한다.

<img src="https://t1.daumcdn.net/cfile/tistory/9989933E5BC97E652B" alt="img" style="zoom:50%;" />

한편 CL의 이러한 구조는 Hubel & Wiesel의 실험에서 보여준 아이디어를 잘 설명한다. 아래 그림은 layer별 결과를 이미지화 한 것이다.

![img](https://t1.daumcdn.net/cfile/tistory/990A613B5BC97E7D1B)

앞쪽 layer일 수록 저수준의 특성(edge, blob)을 판별하는 데에 유용하고 뒤로 갈 수록 점점 고수준의 특성으로 조합해나가는 것을 확인할 수 있다.



## Filter

위에서 설명한 바와 같이 input layer에 filter(or kernel)을 연산하여 output layer의 결과를 도출한다. filter의 구조는 아래와 같이 생겼다.

<img src="https://t1.daumcdn.net/cfile/tistory/99DF403C5BC97E912A" alt="img" style="zoom:80%;" />

filter 영역에는 weight parameter(W)가 존재한다. 입력 데이터 X에 이러한 W를 연산하여 feature map을 도출한다. 



### Filter 연산

일반적인 영상처리 관점에서 봤을때, 이미지에 필터로 연산을 할 때에는 convolution 연산을 사용한다. Convolution layer라는 이름처럼 convolution layer도 convolution연산을 사용할 것 같지만 실은 그렇지 않다. **convolution**은 "하나의 함수를 reverse & shift한 결과를 다른 함수에 곱하는 것"이다. 즉, reverse의 과정이 존재한다. 반면 **cross-correlation**은 convolution에서 '<u>reverse</u>'만 하지 않는다. 즉 수식으로 풀어쓰면 아래와 같다.
$$
convolution\ (f*g)(t)={\int^{∞}_{-∞}}{f(τ)g(t-τ)}dτ\\
cross-correlation\ (f*g)(t)={\int^{∞}_{-∞}}{f(τ)g(t+τ)}dτ\\
$$

이때 deep learning관점에서 봤을 때 filter의 weights는 정해진 값이 아니라 학습시키는 값이다. 따라서 result가 올바른 결과를 도출하게끔 개선된다. 따라서 convolution연산이나 cross-correlation이나 결과적으로 같은 결과를 도출한다. 따라서 tensorflow와 같은 deep learning framework에서는 <u>convolution이 아닌 cross-correlation으로 CNN이 구현</u>된다. cross-correlation은 입력데이터와 필터의 weights의 대응되는 원소간 곱의 합계이므로 **fused multiply-add(FMA)**라고도 부른다. 이러한 filter 연산을 통해 dimensional descent/expansion의 효과를 얻을 수 있다.

한편, Filter의 특성을 결정짓는 요소는 다음과 같다.

1. Input layer의 사이즈 ⇒ filter size를 결정
2. filter의 개수 ⇒ 출력 데이터의 depth를 결정
3. Padding ⇒ 출력 데이터의 width, height를 결정
4. Stride ⇒ 출력 데이터의 width, height를 결정



### Input layer의 사이즈

filter는 input layer size의 영향을 받는다. 먼저 input layer를 살펴보자.

<img src="/files/2022-02-27-DL05/image-20210414001507332.png" alt="image-20210414001507332" style="zoom:50%;" />

위에서 언급했듯 input layer는 3차원 데이터(height, width, depth)의 크기를 갖는다. 이때 depth가 3개인 RGB image data를 생각해보자. 이미지에서 R channel, G channel, B channel은 각각 동시성을 가지며 같은 공간 영역을 나타낸다. 따라서 spatial structure를 유지한다는 convolution layer의 철학에 맞게 filter 또한 이러한 성질을 반영해야 한다. 따라서 <u>filter의 depth는 input layer의 depth와 같은 크기를 갖는다</u>.



### Filter의 개수

filter를 간단하게 표현하자면, <u>input data의 어떠한 특성을 도출하기 위한 값</u>이다. 예를 들어, filter1은 input data의 edge특성을 나타낼 수 있고, filter2는 input data의 검정색 영역의 분포 특성을 잘 나타낼 수 있다. 한편 하나의 이미지에서 이러한 특성은 동시에 발생한다. 따라서 convolution layer를 통과한 결과의 depth로 특성을 동시에 표현할 수 있다. 기본적으로 filter의 depth는 input data의 depth와 같으므로 연산 결과의 depth는 1의 크기를 갖는다. 이러한 결과를 얼마나 겹치느냐에 따라 output data의 depth가 결정되는 것이다. 예를 들어, filter를 N개 사용하는 경우에는 output data의 depth가 N이 된다.



### Padding

**padding**이란 필터를 적용시키기 전에 input data의 가장자리를 특정값으로 채우는 것을 의미한다. 예를 들어 아래와 같이 4x4의 사이즈를 갖는 input data 3x3 filter를 적용시키는 경우를 생각해보자. 이 경우 output data의 크기는 2x2가 된다. 이때 input data에 zero-padding=1을 주는 경우, input data가 5x5가 되어, 같은 filter를 적용해도 결과가 4x4가 된다는 것을 알 수 있다.

![img](https://t1.daumcdn.net/cfile/tistory/999408355BC97F062C)

만일 입력데이터에 padding을 하지 않는다면 출력되는 데이터의 크기는 무조건 입력 데이터보다 작아질 것이다. 레이어가 깊어질 수록 (=filter가 많아질수록) 출력 데이터의 사이즈가 심하게 줄어들어 학습이 불가능한 상태가 되는 것을 방지하기 위해 도입된 개념이 바로 padding이다. 일반적으로 P만큼의 padding을 줄 때, (X_Height,X_Width)의 사이즈를 갖는 input data는 (X_H+2P, X_W+2P)의 크기가 되며, filter의 크기가 (F_Height,F_Width)일 때 output data의 크기 (O_Heigth,O_Width)는 다음의 식을 만족하게 된다.
$$
(O_H,O_W)=(X_H+2P-F_H+1,X_W+2P-F_W+1)
$$


> **Q. 채워주는 Padding값이 result에 영향을 주지는 않을까?**
>
> padding을 주는 경우 대부분의 상황에서는 zero-padding이 일반적이다. 하지만 gray-padding이라고 해서 밝기값이 0~255인 경우에 128값으로 padding을 주는 경우도 존재한다. 중요한 점은, 이미지의 가장자리값이 조금 바뀌는 것은 전체 이미지에 큰 영향을 주지 못한다는 점이다. 일반적으로 classification에서 주요 인식 대상은 이미지의 가운데에 위치할 뿐만 아니라, padding을 주는 값은 전체 image의 size 중에서 극히 일부이기 때문이다. 



### Stride

**stride**란 필터의 <u>이동 간격</u>을 의미한다. stride=1인 경우 filter는 input data에서 1pixel씩 이동하며 연산을 수행한다. 예를 들어 위의 예시에서는 stride=1인 경우이다. 이때 stride에서 주의해야 할 점은 input data, filter의 size와 맞지 않는 경우 output data의 size가 정수가 아니게 되어 에러가 발생할 수 있다는 점이다. 예를 들어, 6x6 input data에 3x3 filter를 적용하는 경우를 생각해보자. stride=2라면 filter가 stride를 이동하는 과정에서 input data를 초과하여 접근하게 된다. 따라서 이러한 경우를 방지하기 위해 아래의 공식을 통해 <u>출력데이터의 사이즈가 자연수값이 되도록 적절히 stride를 조정해주어야 한다.</u> 아래의 식은 입력데이터(X_H, X_W), filter크기(F_H, F_W), Padding P, Stride S인 경우의 출력데이터의 크기(O_H, O_W)를 계산하는 공식이다.
$$
(O_H,O_W)=(\frac{X_H+2P-F_H}{S}+1,\frac{X_W+2P-F_W}{S}+1)
$$


> **Q. input의 volume이 32x32x3이며 10개의 5x5 filter를 stride=1, pad=2로 줄 때 output의 volume size는 어떻게 될까?**
>
> (1)  volume = (32+2x2, 32+2x2, 3) = (36, 36, 3) (∵ input data에 padding, pad=2)
> (2) volume = (36-5+1, 36-5+1, 3/3) = (32, 32, 1) (∵ 5x5 filter 적용, stride=1)
> (3) volume = (32, 32, 1*10) (∵ filter 10개 적용)
>
> ⇒ ∴ 32x32x10
>
> **Q. 이 layer에 존재하는 parameters의 개수는?**
>
> filter size는 5x5x3이며, 10개가 존재한다. 또한 filter별로 bias가 1개씩 존재한다. 따라서 (5x5x3)X10 + 10x1 = 750 + 10 = 760





## Pooling Layer

**pooling layer**는 CNN을 구성하는 layer중 하나이다. 일반적인 경우, convolution layer에서는 입력데이터의 사이즈를 유지하는 반면, pooling layer는 주로 데이터의 사이즈를 줄이기 위해 사용된다. 여기서 주의할 점은, pooling layer는 <u>parameters를 갖고있지 않다</u>는 점이다. pooling layer는 filter와 달리 따로 weight를 가지는 것이 아니라 정해진 연산을 수행할 뿐이다. 대표적인 pooling layer에는 max-pooling과 average pooling이 있다.

### Max-pooling

**max-pooling**이란, 해당하는 영역에서의 최대값(MAX)을 선택하는 방법이다. 이러한 방법은 window 내에서 가장 중요한 요소 정보만을 취하겠다는 철학이 바탕이 된다. 아래 이미지는 이러한 max-pooling의 결과를 잘 보여준다. 대부분의 이미지 인식 모델에서는 주로 max-pooling을 사용한다. 또한 pooling layer에서 pooling의 window size는 stride와 같은 값으로 취하는 경우가 일반적이다.

<img src="https://t1.daumcdn.net/cfile/tistory/2366B34458F47D7808" alt="img" style="zoom:80%;" />



### Average-pooling

**average-pooling**은 해당하는 영역 내의 평균값을 취하는 방법이다. 아래 그림은 average-pooling을 잘 보여준다. average-pooling은 max-pooling보다 덜 극단적이며, 중요한 요소든 덜 중요한 요소든 모두 혼합하여 사용한다. 

![img](https://www.machinecurve.com/wp-content/uploads/2020/01/Average-Pooling-1.png)





> 경우에 따라서는 pooling layer를 두는 대신 stride를 크게 주어 데이터의 사이즈를 줄이는 경우도 있다.



## Normalization Layer

**Normalization layer**는 입력을 정규화 하는 layer이다.



## Fully Connected Layer (FC layer)

CNN의 탄생 배경은, fully connected layer가 입력된 이미지 데이터에 대한 spatial space를 제대로 표현하지 못한다는 점이었다. 그럼에도 불구하고 CNN은 **fully connected layer**를 필요로 한다. 일반적으로는 아래와 같이 <u>CNN의 마지막 layer단에서 classification을 위해 사용</u>되는 경우가 많다. 

![image-20210414220839151](/files/2022-02-27-DL05/image-20210414220839151.png)





 


# CNN Architectures

지금까지 CNN의 구성요소(CONV Layer, Filter, POOL Layer, FC Layer 등)에 대해 살펴보았다. 정리하자면, CNN architectures는 convolution layer(CONV), pooling layer(POOL), fully connected layer(FC) 등으로 구성되며, CONV와 FC는 learnable filter(weights)를 갖는다. 여기서 유의할 점은 언급하지는 않았으나 각 layer를 통과할 때마다 activation function을 지나간다는 점이다. 이 부분에 대해 간단하게 설명하자면, activation function이 없는 경우 아무리 deep하게 layers가 구성되어있을지라도 결국에 linear할 수밖에 없다. 따라서 layer마다 activation function을 두어 non-linear하게 만드는 작업을 해야한다. 

이번에는 대표적인 CNN 모델에 대해 살펴보도록 한다. 아래 이미지는 세계적인 규모의 이미지 인식 대회인 ILSVRC에서의 역대 우승자를 나타낸 그림이다.

![image-20210414221154855](/files/2022-02-27-DL05/image-20210414221154855.png)

2012년으로 넘어오면서 error rate(y)가 16.4%로 급격히 낮아짐을 확인할 수 있다. 2012년 우승을 차지한 **AlexNet**은 최초로 대회에서 CNN 기반 모델을 이용하여 우승을 하였다.





## LeNet

**LeNet**은 최초의 CNN 기반 아키텍처이다. 1990년대에 대부분의 CNN에서의 성공적인 결과는 Yann LeCun가 만들었다. 그의 대표적인 architeture로는 zip code나 숫자를 인식하는 LeNet architeture가 있다.





## AlexNet

**AlexNet**은 최초의 CNN기반 우승자 모델로, 이미지 인식 분야에서 deep learning의 가능성을 보여준 모델이라 평가된다. AlexNet에 대해서 자세히 알고싶다면 AlexNet에 대해 기술한 ImageNet Classification with Deep Convolutional Neural Networks 논문을 참고하면 된다.

> 위에서 설명한 대부분의 개념은 이 논문에서 더 자세히 확인할 수 있다.

아래 이미지는 AlexNet의 architecture를 보여준다.

![image-20210414221658635](/files/2022-02-27-DL05/image-20210414221658635.png)

위 그림을 text로 풀어서 표현하면 아래와 같다.

<img src="/files/2022-02-27-DL05/image-20210414225431139.png" alt="image-20210414225431139" style="zoom: 67%;" />

눈여겨 봐야할 점은 다음과 같다.

1. first use of ReLU
2. used Norm layers (not common anymore)
   3. heavy data augmentation
3. dropout 0.5
4. batch size 128
5. SGD Momentum 0.9
6. Learning rate 1e-2, reduced by 10 manually when val accruacy plateaus
7. L2 weight decay 5e-4
8. 7 CNN ensemble: 18.2% -> 15.4%



## ZFNet

**ZFNet**은 AlexNet 다음년도 ILSVRC 우승자로, AlexNet 모델에서 hyperparameters를 향상시켜 더 나은 error rate(16.4%→11.7%)를 도출하였다. AlextNet에서 바뀐 hyperparameters는 다음과 같다.

1. CONV1: (11×11 stride 4) → (7×7 stride 2)
2. CONV3, 4, 5: (384, 384, 256 filters) → (512, 1024, 512)

> 사실상 AlexNet과 큰 차이가 없어서 잘 언급되지는 않는다.



## VGGNet

이전 CNN기반 모델(AlextNet, ZFNet)은 8 layers를 사용하였다. 하지만 **VGGNet**(2014년)를 기점으로 사용되는 layers의 개수가 점점 더 많아지기 시작했다. VGG는 19개의 layers를 이용하여 network를 더욱 깊게 만들 수 있다는 것에 대한 가능성을 열어주었다. 아래 그림은 VGGNet의 architecture를 그림으로 표현한 것이다.

<img src="/files/2022-02-27-DL05/image-20210414225553476.png" alt="image-20210414225553476" style="zoom:67%;" />

VGGNet는 더 작은 filters로 더 깊은 networks를 구현했다는 특징이 있다. 

1. 8 layers(AlexNet) → 16~19 layers(VGGNet) : deeper networks
2. 11×11, 5×5, 3×3과 같이 다양한 size의 CONV layer (AlexNet)
   → 오직 3×3 size, stride 1, pad 1를 갖는 CONV layer (VGGNet)
3. 3×3 stride 2의 MAX POOL (AlexNet)
   → 2×2 stride 2의 MAX POOL (VGGNet)
4. 11.7% top-5 error in ILSVRC'13 (ZFNet)
   → 7.3% top-5 error in ILSVRC'14 (VGGNet)

VGGNet이 더 작은 filters를 사용한 이유는 3개의 3×3 CONV layer (stride 1)가 7×7 CONV layer와 비슷한 receptive field를 갖기 때문이다. 그 이유는 아래의 그림을 보면 더 쉽게 체감할 수 있다.

<img src="/files/2022-02-27-DL05/image-20210414230418863.png" alt="image-20210414230418863" style="zoom:80%;" />

3개의 3×3 CONV layers가 input data의 7×7 receptive field를 갖는다는 것을 쉽게 알 수 있다. 그렇다면 1 7×7 CONV filter와 3 3×3 CONV filters 중 어느 것이 더 효율적일까? layer 당 channel 개수가 C개라고 할 때 parameters 개수는, 3개의 3×3 CONV layer는 3개\*(3\*3 kernel size \*C kernel depth \*C output depth)=27C²개이고, 7×7 CONV layer는 1개\*(7\*7 kernel size \*C kernel depth \*C output depth)=49C²개이다. 따라서 차라리 더 작은 size의 CONV layers를 여러 개 사용하는 것이 더 큰 size의 CONV layer 하나를 사용하는 것보다 효율적이다. 한편, VGGNet의 parameters 개수는 아래와 같이 계산된다.

![image-20210414231938865](/files/2022-02-27-DL05/image-20210414231938865.png)

위 이미지에서 눈여겨 볼 점은, 대부분의 memory를 초반 CONV가 차지하고, 대부분의 parameters는 후반 FC가 차지한다는 점이다.

DL 모델에서 메모리는 주로 backpropagation에서 chain rule을 적용하기 위해 모든 중간 변수를 저장하는 데에 사용되거나, mini batch를 사용하여 모델을 학습시킬 때 더 많은 중간 activation을 저장하는 데에 사용된다.

backpropagationi을 할 때 CONV layer의 중간 결과를 저장하는 데에 주로 사용된다. 따라서  ???????????????//



VGGNet의 세부사항은 아래와 같다.

- ILSVRC'14 2nd in classification, 1st in localization
- Similar training procedure as Krizhevsky 2012
- No Local Response Normalisation (LRN)
- Use VGG16 or VGG19 (VGG19 only slightly better, more memory)
- Use ensembles for best results
- FC7 features generalize well to other tasks






# References

1. https://excelsior-cjh.tistory.com/180



