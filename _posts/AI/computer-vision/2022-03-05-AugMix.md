---
toc: true
toc_label: "Table of Contents"
toc_icon: "cog"
toc_sticky: true

layout: post-with-comments
title: '[Summary] AugMix'
excerpt: "ë°ì´í„° ì¦ê°• ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ AugMixì˜ ë‚´ìš©ì„ ë…¼ë¬¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•œë‹¤."
date: 2022-03-05
last_modified_at: 2022-03-05
categories:
  - ai-computerVision
tags: 
   - [AugMix, Augmentation]

use_math: false
comments: true
share : false
---

# AugMix

## ë°°ê²½

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ì˜ ëŠ¥ë ¥ì— í¬ê²Œ ì˜ì¡´í•œë‹¤. ë”°ë¼ì„œ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¶„í¬ê°€ mismatchedì¸ data shift ìƒí™©ì—ì„œ accuracyê°€ ê°ì†Œë˜ëŠ” ë¬¸ì œì ì´ ìˆë‹¤. 

ì´ë¥¼ í•´ê²°í•˜ë ¤ëŠ” ê¸°ì¡´ì˜ ë°©ì‹ë“¤ì€ í¬ê²Œ ì„¸ ê°€ì§€ ë¬¸ì œì ì„ ê°€ì§€ê³  ìˆë‹¤.

1. **Data Memorizing** : í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ì…ë ¥ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì— ë¶ˆê³¼í•´ì„œ, ì™„ì „íˆ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì ‘í•˜ëŠ” ê²½ìš°ì—ëŠ” ì—­ì‹œ robustí•˜ì§€ ëª»í•˜ë‹¤. ì¦‰, corruptionsì„ í•™ìŠµí•˜ëŠ” ê±´ í•´ë‹¹ corruptionsì—ì„œë§Œ robustí•  ë¿ì´ì§€ ì™„ì „íˆ ìƒˆë¡œìš´ corruptionì— ëŒ€í•´ì„œëŠ” ì¼ë°˜í™”ë˜ì§€ ëª»í•œë‹¤.
2. **Trade-off** : clean accuracy*ì™€ robustnessëŠ” trade-off ê´€ê³„ì´ë‹¤. ë”°ë¼ì„œ robustí•´ì¡Œì§€ë§Œ clean accuracyëŠ” ê°ì†Œí•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.
   * (ì°¸ê³ ) clean accuracy* : corrupted ë˜ì§€ ì•Šì€ ì›ë³¸ ì…ë ¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì–»ì€ ì •í™•ë„
3. **Augmented Data with Degraded Performance** : ë‹¤ì–‘ì„±ì„ ì¦ê°€ì‹œí‚¤ê¸°ìœ„í•´ augmentation primitivesë¥¼ chainì— ì§ì ‘ì ìœ¼ë¡œ êµ¬ì„±í•œë‹¤. ì¦‰, augmented dataëŠ” original dataì˜ manifoldì—ì„œ í¬ê²Œ ë²—ì–´ë‚  ìˆ˜ ìˆë‹¤.

<br>

## ê°œë…

**AugMix**ëŠ” image classifierì—ì„œ robustnessì™€ uncertainty estimatesë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ë¡œ, êµ¬í˜„ì´ ì‰½ê³  í•™ìŠµ ë‹¨ê³„ì—ì„œ ë³¸ ì  ì—†ëŠ” corruptionsì—ë„ ëª¨ë¸ì´ ê°•ì¸í•´ì§ˆ ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤.

### íŠ¹ì§•

AugMixëŠ” ì´ì „ì˜ data augmentation ê¸°ë²•ê³¼ ì°¨ë³„ë˜ëŠ” ë‹¤ìŒì˜ íŠ¹ì§•ì„ ê°–ëŠ”ë‹¤.

* augmentation operationsê°€ í™•ë¥ ì ìœ¼ë¡œ ìƒ˜í”Œë§ë¨<br>â€» ì´í›„ "ì•Œê³ ë¦¬ì¦˜" ì„¹ì…˜ì—ì„œ ì„¤ëª…í•  "1. Augmentation"ê³¼ ê´€ë ¨ â€»
* ë‹¤ì–‘í•œ augmented imagesë¥¼ ìƒì‚°í•˜ê¸° ìœ„í•´ augmentation chainsì´ layeredë¨<br>â€» ì´í›„ "ì•Œê³ ë¦¬ì¦˜" ì„¹ì…˜ì—ì„œ ì„¤ëª…í•  "2. Mixing"ê³¼ ê´€ë ¨ â€»
* Jensen-Shannon divergenceë¥¼ consistency lossë¡œ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ë‹¤ì–‘í•œ augmentationsì—ì„œë„ ì¼ê´€ì„±ì„ ìœ ì§€í•  ìˆ˜ ìˆìŒ<br>â€» ì´í›„ "ì•Œê³ ë¦¬ì¦˜" ì„¹ì…˜ì—ì„œ ì„¤ëª…í•  "3. Jensen-Shannon Divergence Consistency Loss"ì™€ ê´€ë ¨ â€»

### ì•Œê³ ë¦¬ì¦˜

AugMixëŠ” <u>ëª‡ ê°€ì§€ augmentation chainsì„ convex combinations</u>í•˜ì—¬ ì‚¬ìš©í•¨ìœ¼ë¡œì¨, ë‹¤ì–‘ì„±ê³¼ ì¼ê´€ì„±ì„ ë³´ì¥í–ˆë‹¤.

> AugMixì—ì„œ "ë‹¤ì–‘ì„±"ê³¼ "ì¼ê´€ì„±"ì´ë¼ëŠ” ë§ì€ êµ‰ì¥íˆ ì¤‘ìš”í•˜ë‹¤. ê¸°ì¡´ì´ augmentation ê¸°ë²•ì€ "ë‹¤ì–‘ì„±"ê³¼ "ì¼ê´€ì„±"ì´ ì¼ì¢…ì˜ trade-off ê´€ê³„ì˜€ê¸° ë•Œë¬¸ì´ë‹¤. ê°€ë ¹, ë‹¤ì–‘ì„±ì„ ì£¼ë ¤ê³  ë‹¤ì–‘í•œ ì—°ì‚°ì„ ì ìš©í•˜ë‹¤ë³´ë©´ ì¦ê°•ëœ ë°ì´í„°ê°€ ì›ë³¸ ë°ì´í„°ì™€ í¬ê²Œ ë©€ì–´ì§€ëŠ” ê²½ìš°ê°€ ì´ì— í•´ë‹¹í•œë‹¤. AugMixëŠ” "ë‹¤ì–‘ì„±"ê³¼ "ì¼ê´€ì„±"ì„ ëª¨ë‘ ì§€ì¼°ë‹¤ëŠ” ì ì—ì„œ ì˜ì˜ê°€ í¬ë‹¤. 

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220302184953251.png" alt="image-20220302184953251" style="width:600px;" />

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220302184841118.png" alt="image-20220302184841118" style="width:600px;" />

AugMix ì•Œê³ ë¦¬ì¦˜ì€ í¬ê²Œ **â‘ Augmentation**ê³¼ **â‘¡Mixing**, ê·¸ë¦¬ê³  **â‘¢Jensen-Shannon Divergence Consistency Loss**ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

> â‘  Augmentationì€ "ë‹¤ì–‘ì„±"ê³¼ ê´€ë ¨ì´ ìˆê³ , â‘¡Mixingì€ "ì¼ê´€ì„±"ê³¼ ê´€ë ¨ì´ ìˆë‹¤.

<br>

#### 1. Augmentation

> Augmentationì€ data augmentationì˜ '**ë‹¤ì–‘ì„±**'ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ ê³¼ì •ì´ë‹¤.

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303101035678.png" alt="image-20220303101035678" style="width:400px;" />

```pseudocode
3:	Fill x_aug with zeros
		...
5:	for i = 1, ..., k do
6:		Sample operations op1, op2, op3 ~ ÎŸ
7:		Compose operations with varying depth op12 = op2Â·op1 and op123 = op3Â·op2Â·op1
8:		Sample uniformly from one of these operations chain ~ {op1, op12, op123}
```

AutoAugment<sup>[[ê°œë…]](#AutoAugment)</sup>ë¥¼ í†µí•´ ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ <u>ìµœì ì˜ augmentations ê¸°ë²•ì„ ì„ íƒ</u>í•œë‹¤. ì´í›„, í…ŒìŠ¤íŠ¸ ê³¼ì •ì—ì„œ ì‚¬ìš©í•  <u>ImageNet-C</u><sup>[[ê°œë…]](#ImageNetC)</sup><u> ë°ì´í„°ì™€ ê²¹ì¹˜ëŠ” augmentation ê¸°ë²•ì€ í•™ìŠµ ê³¼ì •ì—ì„œ ë°°ì œ</u>í•œë‹¤. (e.g., image noisingê³¼ image blurring ì—°ì‚°ì€ í•™ìŠµ ê³¼ì •ì—ì„œ ë°°ì œ) ë˜í•œ augmented dataê°€ ê¸°ì¡´ data manifoldì—ì„œ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ <u>ê° augmentationì˜ ê°•ë„ë¥¼ ì ì ˆíˆ ì„¤ì •</u>í•œë‹¤. (e.g., rotation operationsì˜ íšŒì „ ê°•ë„, like 2Âº or -15Âº)

augmentation ê³¼ì •ì€ <u>augmentation chainì„ ëœë¤ìœ¼ë¡œ $k$(default:3)ê°œ ìƒì„±</u>í•˜ëŠ”ë°, ì´ë•Œ ê° augmentation chainì€ ëœë¤ìœ¼ë¡œ ì„ íƒ ëœ 1~3ê°€ì§€ augmentation operationsë¡œ êµ¬ì„±ëœë‹¤.

<br>

#### 2. Mixing

> Mixingì€ augmented dataê°€ original dataì˜ manifoldì—ì„œ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ '**ì¼ê´€ì„±**'ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ ê³¼ì •ì´ë‹¤.

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303102414162.png" alt="image-20220303102414162" style="width:400px;" />

``` pseudocode
4:	Sample mixing weights (w1, w2, ..., wk) ~ Dirichlet(Î±, Î±, ..., Î±)
5:	for i = 1, ..., k do
		...
9:		x_aug += w_i Â· chain(x_orig)
10:	end for
11:	Sample weight m ~ Beta(Î±, Î±)
12:	Interpolate with rule x_augmix = m*x_orig + (1-m)*x_aug
13: return x_augmix
14: end function
```

Augmentations ê³¼ì •ì„ í†µí•´ **ìƒì„±ëœ $k$ê°œì˜ augmentation chainsì€ mixing ê³¼ì •ì—ì„œ ê²°í•©ëœë‹¤**. ì•ŒíŒŒ í•©ì„±(Alpha Compositing)ì— ì˜í•œ mixingì„ ë‹¨ìˆœí•˜ê¸° êµ¬í˜„í•˜ê¸° ìœ„í•´ ì—¬ê¸°ì„œëŠ” elementwise convex combinationsë¥¼ ì‚¬ìš©í•œë‹¤. <u>convex coefficientsì˜ $k$ì°¨ì› ë²¡í„°ëŠ” $\text{Dirichlet}{(\alpha, \alpha, ..., \alpha)}$ ë¶„í¬</u><sup>[[ê°œë…]](#Dirichlet_BetaDistribution)</sup><u>ë¡œë¶€í„° ëœë¤í•˜ê²Œ ìƒ˜í”Œë§ë˜ì–´ ê° augmentations chainsì˜ ê²°ê³¼ì— ê³±í•´ì§„ í›„ í•˜ë‚˜ì˜ ê²°ê³¼ë¡œ í•©ì³ì§„ë‹¤</u>. ì´í›„, <u>í•˜ë‚˜ë¡œ í•©ì³ì§„ ê²°ê³¼ëŠ” $\text{Beta}{(\alpha, \alpha)}$ ë¶„í¬</u><sup>[[ê°œë…]](#Dirichlet_BetaDistribution)</sup><u>ë¡œë¶€í„° ìƒ˜í”Œë§ëœ ë‘ ë²ˆì§¸ random convex combinationì„ í†µí•´ original imageì™€ í•©ì³ì§„ë‹¤</u>. 

ì¦‰, ìµœì¢…ì ì¸ ì´ë¯¸ì§€ëŠ” <u>ì—°ì‚° ì„ íƒ</u>, ì´ëŸ¬í•œ <u>ì—°ì‚°ì˜ ê°•ë„</u>, <u>augmentation chainsì˜ ê¸¸ì´</u>, ê·¸ë¦¬ê³  <u>mixing weightsì˜ ì„ íƒ</u>ì— ì˜í•´ ì—¬ëŸ¬ ê°œì˜ ëœë¤í•œ sourcesê°€ í†µí•©ëœ ê²°ê³¼ì´ë‹¤.

<br>

#### 3. Jensen-Shannon Divergence Consistency Loss

ìœ„ì˜ ê³¼ì •ì„ í†µí•´ êµ¬í•œ augmentation schemeì„ lossì— ê²°í•©í•˜ì—¬ ì‹ ê²½ë§ì˜ ì‘ë‹µì„ ë¶€ë“œëŸ½ê²Œ ë§Œë“ ë‹¤. (mixing ê³¼ì •ì„ í†µí•´) ì´ë¯¸ì§€ì˜ semantic contentë¥¼ ê±°ì˜ ë³´ì¡´í–ˆê¸° ë•Œë¬¸ì— lossë¥¼ ê³„ì‚°í•  ë•Œ $x_{\text{orig}}, x_{\text{augmix1}}, x_{\text{augmix2}}$ëŠ” ìœ ì‚¬í•œ ì •ë„ë¡œ ëª¨ë¸ì— ë‚´ì¥ëœë‹¤.

> ğŸ—£ï¸ "ìœ ì‚¬í•œ ì •ë„ë¡œ ëª¨ë¸ì— ë‚´ì¥ëœë‹¤"ëŠ” ê²ƒì€, $x_{\text{augmix}}$ì„ $x_{\text{orig}}$ì— ë¹„í•´ íŠ¹ë³„íˆ ì ì€ ê°€ì¤‘ì¹˜ë¡œ ë°˜ì˜í•˜ì§€ ì•Šê² ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.

ì´ë¥¼ ìœ„í•´, original sample $x_{\text{orig}}$ì™€ ì´ê²ƒì˜ augmented ë³€í˜•ë“¤ì˜ posterior distributionsì— ëŒ€í•´ Jensen-Shannon divergence<sup>[[ê°œë…]](#Jensen-ShannonDivergence)</sup>ë¥¼ ìµœì†Œí™”í•œë‹¤. ì¦‰, $p_{\text{orig}}=\hat{p}{(y \mid x_{\text{orig}})}$, $p_{\text{augmix1}}=\hat{p}{(y \mid x_{\text{augmix1}})}$, $p_{\text{augmix2}}=\hat{p}{(y \mid x_{\text{augmix2}})}$, ê·¸ë¦¬ê³  original loss $\mathcal{L}$ì— ëŒ€í•´ ìµœì¢…ì ì¸ lossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
$$
\mathcal{L}(p_{\text{orig}}, y) + Î»\textbf{JS}(p_{\text{orig}}; p_{\text{augmix1}}; p_{\text{augmix2}}).
$$

$$
\textbf{JS}(p_{\text{orig}}, p_{\text{augmix1}}, p_{\text{augmix2}}) = \frac{1}{3}(\textbf{KL}[p_{\text{orig}} \mid\mid M] + \textbf{KL}[p_{\text{augmix1}} \mid\mid M] + \textbf{KL}[p_{\text{augmix2}} \mid\mid M]).
$$

ì´ëŸ¬í•œ lossëŠ” ì„¸ ê°œì˜ $p_{\text{orig}}$, $p_{\text{augmix1}}$, $p_{\text{augmix2}}$ ë¶„í¬ ì¤‘ ì–´ë–¤ í•œ ìƒ˜í”Œì´ ë‚˜íƒ€ë‚´ëŠ” ìƒ˜í”Œ ë¶„í¬ì˜ ë™ì¼ì„±ì— ëŒ€í•œ í‰ê·  ì •ë³´ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.

* ì°¸ê³ 
  * <a name=Jensen-ShannonDivergence>Jensen-Shannon divergence</a>
    * KL Divergenceì— ë¹„í•´ upper bounded.
    * ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì…ë ¥ ë²”ìœ„ì— ëŒ€í•´ â‘ stable, â‘¡consistent, â‘¢intensiveí•˜ê²Œ ë§Œë“¤ì–´ì¤€ë‹¤ (Bachman et al., 2014; Zheng et al., 2016; Kannan et al., 2018).
  * ë‘ ê°œì˜ augmentation chainsì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ 
    * $\textbf{JS}(p_{\text{orig}}; p_{\text{augmix1}})$ì€ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸°ì— ë¶€ì¡±í•˜ë‹¤.
    * $\textbf{JS}(p_{\text{orig}}; p_{\text{augmix1}}; p_{\text{augmix2}}; p_{\text{augmix3}})$ì€ ì„±ëŠ¥ì„ ë„ˆë¬´ ì¡°ê¸ˆ í–¥ìƒì‹œí‚¨ë‹¤. ('êµ³ì´'ì¸ ëŠë‚Œ)

<br>

## Experiments

### Datasets

ì‹¤í—˜ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ì€ í¬ê²Œ CIFAR-10, CIFAR-100, ê·¸ë¦¬ê³  ImageNetìœ¼ë¡œ ë‚˜ë‰œë‹¤.

* **CIFAR** (Krizhevsky & Hinton, 2009)
  * 32Ã—32Ã—3 color natural images
  * 50,000 training & 10,000 testing images
  * CIFAR-$N$ has $N$ categories
* **ImageNet** (Deng et al., 2009)
  * 1,000 classes
  * approximately 1.2 milion large-scale color images

> ì‹¤í—˜ì—ëŠ” ì´ëŸ¬í•œ ë°ì´í„°ì…‹ì˜ ë³€í˜•ëœ ë²„ì „ë„ ì‚¬ìš©ë˜ëŠ”ë°, ë…¼ë¬¸ì— ë‚˜ì™€ìˆëŠ” ìš©ì–´ëŠ” ì•„ë‹ˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” í¸ì˜ì— ë”°ë¼ "Dataset-C"ì™€ "Dataset-P"ë¡œ ë‚˜ëˆˆë‹¤.

#### <a name="ImageNetC">Dataset-C</a>

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/imagenet-c.png" alt="img" style="width:400px;" />

<u>data shift ìƒí™©ì—ì„œì˜ ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ì„ í‰ê°€</u>í•˜ê¸° ìœ„í•´ ì›ë³¸ ë°ì´í„°ì…‹ì— corruptionì„ ì¶”ê°€í•œ **CIFAR-10-C**, **CIFAR-100-C**, **ImageNet-C** (Hendrycks & Dietterich, 2019) ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œë‹¤. ê° ë°ì´í„°ì…‹ì€ <u>ì„¸ ê°€ì§€ íƒ€ì…(blur, weather, and digital corruption)</u>, ì´ <u>15ê°œì˜ ë…¸ì´ì¦ˆ</u>ê°€ ê°ê° <u>5ê°€ì§€ì˜ ê°•ë„</u>ë¡œ ì¶”ê°€ëœë‹¤. 

#### Dataset-P

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/translate.gif" alt="img" style="width:200px;" /><img src="/files/2022-03-04-ai-ComputerVision-AugMix/tilt.gif" alt="img" style="width:200px;" /><img src="/files/2022-03-04-ai-ComputerVision-AugMix/spatter.gif" alt="img" style="width:200px;" />

<u>classifierì˜ ì˜ˆì¸¡ ì•ˆì •ì„±(prediction stability)ì„ í‰ê°€</u>í•˜ê¸° ìœ„í•´ ì›ë³¸ ë°ì´í„°ì…‹ì— dataset-Cì— ë¹„í•´ ì‘ì€ ì•½ê°„ì˜ ë³€í™”ë¥¼ ì¶”ê°€í•œ **CIFAR-10-P**, **CIFAR-100-P**, **ImageNet-P** ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œë‹¤. ê° ë°ì´í„°ì…‹ì€ <u>ì˜ìƒ</u> íƒ€ì…ì´ë©°, ì˜ˆë¥¼ ë“¤ì–´ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë°ê¸°ê°€ ì ì  ì¦ê°€í•œë‹¤ëŠ” ì‹ìœ¼ë¡œ ë³€í˜•ë˜ì—ˆë‹¤. ì´ë¥¼ í†µí•´ ë°ê¸°ê°€ ì¦ê°€í•¨ì— ë”°ë¼ <u>ì˜ìƒ í”„ë ˆì„ê°„ì˜ ì¼ê´€ë˜ì§€ì•Šê±°ë‚˜ ê¸‰ê²©í•œ ì˜ˆì¸¡ì´ ë°œìƒí•˜ëŠ” ì§€ë¥¼ í™•ì¸</u>í•œë‹¤.

<br>

### Metrics

ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë©”íŠ¸ë¦­ìœ¼ë¡œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì„¸ ê°€ì§€ë¥¼ ì œì•ˆí•œë‹¤.

#### mCE(Mean Corruption Error)

* **Clean Error** : ë³€í˜•ë˜ì§€ ì•Šì€ clean dataì— ëŒ€í•œ ì¼ë°˜ì ì¸ classification error
* **Corruption Error** : ë³€í˜•ëœ corrupted dataì— ëŒ€í•œ classification error
  * $E_{c,s}$ : corruption $c$ê°€ $s(1â‰¤sâ‰¤5)$ì˜ ê°•ë„ë¡œ ì£¼ì–´ì¡Œì„ ë•Œ error
  * $\text{u}CE_c = \sum_{s=1}^5{E_{c,s}}$ : unnormalized corruption error
  * $CE_c = \sum_{s=1}^5{E_{c,s}}/\sum_{s=1}^t{E_{c,s}^{\text{AlexNet}}}$ : normalized corruption error
  * $\text{m}CE$ : 15ê°€ì§€ corruptionsì— ëŒ€í•œ í‰ê· ì ì¸ error

#### mFP(mean Flip Probability), mFR(mean Flip Rate)

<u>ì˜ìƒ í”„ë ˆì„ê°„ì˜ ì˜ˆì¸¡ ì•ˆì •ì„±ê³¼ ê´€ë ¨ëœ Perturbation robustnessë¥¼ ì¸¡ì •</u>í•˜ê¸° ìœ„í•´ **flip probability**ë¥¼ ê³„ì‚°í•œë‹¤. flip probabilityëŠ” ì¸ì ‘ í”„ë ˆì„ê°„ì— ë¯¸ë¬˜í•œ ì°¨ì´ê°€ ì¡´ì¬í•  ë•Œ, ê·¸ ì˜ˆì¸¡ì´ ë‹¬ë¼ì§€ëŠ”(ì´ë¥¼ "flipped"ë¼ê³  í‘œí˜„í•œë‹¤) í™•ë¥ ì„ ì˜ë¯¸í•œë‹¤. 10ê°€ì§€perturbation ì¢…ë¥˜ì— ëŒ€í•´ í‰ê· ê°’ì„ ê³„ì‚°í•œ ê²ƒì„ **mean Flip Probability (mFP)**ë¼ê³  í•œë‹¤. ImageNet-Cì—ì„œëŠ”, AlexNetì˜ flip probabilitiesë¥¼ normalizationí•˜ì—¬ **mean Flip Rate(mFR)**ì„ ê³„ì‚°í•œë‹¤.

#### CE(Calibration Error)

<u>ëª¨ë¸ì˜ uncertainty estimatesë¥¼ í‰ê°€</u>í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ <u>miscalibration</u>ì„ ì¸¡ì •í•œë‹¤. ì—¬ê¸°ì„œ "calibrated"ëŠ” classifiersê°€ ì¶œë ¥í•œ ì •í™•ë„ë¥¼ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì˜ë¯¸í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, calibrated modelì´ë¼ë©´, 70%ì˜ ì •í™•ë„ë¡œ ì˜ˆì¸¡í–ˆì„ ë•Œ ê·¸ ì‹ ë¢°ë„ ì—­ì‹œ 70%ì—¬ì•¼ í•œë‹¤. 

* idealized RMS Calibration Error : $\sqrt{E_C[(P(Y=\hat{Y} \mid C=c) - c)^2]}$

  > ì£¼ì–´ì§„ ì‹ ë¢°ë„ cì—ì„œì˜ accuracyì™€ ì‹¤ì œ ì‹ ë¢°ë„ cì˜ squared differenceë¥¼ ì˜ë¯¸í•œë‹¤.

<br>

### CIFAR-10 and CIFAR-100ì— ëŒ€í•œ ì‹¤í—˜

#### Training Setup

* **Learning rate** : <u>initial learning rate = 0.1</u>, which decays following <u>a cosine learning rate</u>
* **Input images** : All input images are pre-processed with <u>standard random left-right ï¬‚ipping and cropping prior to any augmentations</u>.
* **AUGMIX parameters** : We <u>do not change AUGMIX parameters</u> across CIFAR-10 and CIFAR-100 experiments for consistency.
* **Epochs** : The All Convolutional Network and Wide ResNet train for <u>100 epochs</u>, and the DenseNet and ResNeXt require <u>200 epochs</u> for convergence.
* **Optimization** : We optimize with <u>stochastic gradient descent using Nesterov momentum</u>.
* **Weight decay** : We use <u>a weight decay of 0.0001 for Mixup and 0.0005 otherwise</u>.

<br>

#### Resultâ‘ : Corruption Error

CIFAR-10-Cì™€ CIFAR-100-Cì— ëŒ€í•´ ì—¬ëŸ¬ augmentation ê¸°ë²• ì ìš©ì— ë”°ë¥¸ corruption error ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. Figure 5ëŠ” CIFAR-10-Cì— ëŒ€í•œ Corruption Errorë¥¼ ê·¸ë˜í”„í™”í•œ ê²ƒì´ê³ , Table 1ì€ ê²°ê³¼ë¥¼ í‘œë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303145123150.png" alt="image-20220303145123150" style="width:500px;" />

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303145916172.png" alt="image-20220303145916172" style="width:500px;" />

AUGMIXëŠ” ë¹„êµí•œ ì—¬ëŸ¬ augmentation ì¤‘ ê°€ì¥ ë‚®ì€ Corruption Errorë¥¼ ë‹¬ì„±í–ˆë‹¤. 

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303151502989.png" alt="image-20220303151502989" style="width:500px;" />

* Standardì—ì„œ
  * ë¹„êµì  ì˜ˆì¸¡ì´ í˜ë“  corruption ì¢…ë¥˜ : `Gaussian Noise`, `Glass Blur`, `Impulse Noise`, `Shot Noise`
  * ë¹„êµì  ì˜ˆì¸¡ì´ ì‰¬ìš´ corruption ì¢…ë¥˜ :`Brightness`, `Fog`
* AugMixì—ì„œ
  * ë¹„êµì  ì˜ˆì¸¡ì´ í˜ë“  corruption ì¢…ë¥˜ : `Gaussian Noise`, `Glass Blur`, `Pixelate`, `Shot Noise`
  * ë¹„êµì  ì˜ˆì¸¡ì´ ì‰¬ìš´ corruption ì¢…ë¥˜ : `Brightness`, `Defocus Blur`, `Zoom Blur`, `Motion Blur`, `Fog`

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303151553607.png" alt="image-20220303151553607" style="width:500px;" />

ìœ„ ê·¸ë¦¼ì—ì„œ ë…¸ë€ìƒ‰ ë§‰ëŒ€ëŠ” Standardì™€ AugMix ê°„ì˜ corruption error ì°¨ì´ë¥¼ ì‹œê°í™”í•œ ê²ƒì´ë‹¤. ì´ë•Œ íŠ¹íˆë‚˜ ê·¸ ì°¨ì´ê°€ ì‹¬í•œ ë¶€ë¶„ì„ ê²€ì€ìƒ‰ ìœ¤ê³½ì„ ìœ¼ë¡œ í‘œí˜„í–ˆë‹¤.

* corruption errorì—ì„œ AugMixê°€ ë¹„êµì  ì¢‹ì€ ê²°ê³¼ë¥¼ ì•¼ê¸°í•˜ëŠ” corruption ì¢…ë¥˜ : `Gaussian Noise`, `Shot Noise`, `Impulse Noise`, `Glass Blur`
* corruption errorì—ì„œ AugMixê°€ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ ëª»í•˜ëŠ” corruption ì¢…ë¥˜ : `Fog`, `Brightness`

> * ì „ì²´ì ìœ¼ë¡œ AugMixëŠ” corruption errorë¥¼ ëŒ€ëµ 11~17.8%ì •ë„ ê°ì†Œì‹œí‚¨ë‹¤.
>
> * (Standardì™€ AugMix ë‘ ê²°ê³¼ì— ì˜í•˜ë©´) Noise corruptionsì™€ Glass Blur, ê·¸ë¦¬ê³  PixelateëŠ” ì˜ˆì¸¡ì´ ì–´ë µê³ , Brightnessì™€ FogëŠ” ì˜ˆì¸¡ì´ ì‰½ë‹¤.
>
>   í•˜ì§€ë§Œ Standardì™€ ë¹„êµí–ˆì„ ë•Œ, AugMixì—ì„œëŠ” corruption ì¢…ë¥˜ì— ë”°ë¥¸ Corruption Error ì°¨ì´ê°€ ëœí•˜ë‹¤. ì¦‰, AugMixì¼ ë•Œ ë„¤íŠ¸ì›Œí¬ê°€ ë” robustí•´ì¡Œë‹¤.

#### Resultâ‘¡: Flip Probability & Calibration Error

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303154039743.png" alt="image-20220303154039743" style="width:500px;" />

> * (Figure 6ì˜ ì¢Œì¸¡ ìë£Œë¡œë¶€í„°) AugMixëŠ” ë¹„êµì  perturbationì— robustí•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
> * (Figure 6ì˜ ìš°ì¸¡ ìë£Œë¡œë¶€í„°) AugMixê°€ clean dataì™€ corrupted dataê°„ì˜ RMS Calibration Error ì°¨ì´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê°ì†Œì‹œì¼°ìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303155419448.png" alt="image-20220303155419448" style="width:500px;" />

> * **CIFAR-10 Clean Error**
>   * Adversarial trainingì—ì„œ í˜„ì €íˆ ë†’ì€ ì—ëŸ¬ë¥¼ ë³´ì—¬ì¤€ë‹¤.
>   * ê·¸ ì™¸ ê¸°ë²•ê°„ì˜ ì—ëŸ¬ ì°¨ì´ëŠ” ê·¼ì†Œí•˜ë‹¤.
> * **CIFAR-10-P mean Flip Probability**
>   * Adversarial trainingê³¼ AugMixì—ì„œ ë‚®ì€ ì—ëŸ¬ë¥¼ ë³´ì—¬ì¤€ë‹¤.
>
> âˆ´ AugMixëŠ” clean errorëŠ” ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œë„ mean Flip ProbabilityëŠ” ë‚®ì¶°ì£¼ëŠ” ë°ì— íš¨ê³¼ì ì´ë‹¤.

### ImageNetì— ëŒ€í•œ ì‹¤í—˜

#### Baselines

í° ê·œëª¨ì˜ classesë¥¼ ê°–ëŠ” ImageNetì—ì„œì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•´, ë‹¤ìŒì˜ ê¸°ì¤€ìœ¼ë¡œ ê¸°ì¡´ì˜ ë°©ë²•ê³¼ AugMixì˜ ì„±ëŠ¥ì„ ë¹„êµí•  ëŒ€ìƒì„ ì„ ì •í•˜ì˜€ë‹¤.

* Cutoutì€ ImageNet ê·œëª¨ì—ì„œ íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì´ ì¦ëª…ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ImageNet-Cì—ì„œ ì„±ëŠ¥ì´ ê²€ì¦ëœ **Stylized ImageNet**ì„ ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ê²°ì •í–ˆë‹¤.
* **Patch Uniform**ì€ ëœë¤ìœ¼ë¡œ ì„ íƒëœ ì´ë¯¸ì§€ ì˜ì—­ì— uniform noiseë¥¼ ì£¼ì…í•œë‹¤ëŠ” ê²ƒì„ ì œì™¸í•˜ê³ ëŠ” Cutoutê³¼ ìœ ì‚¬í•˜ë¯€ë¡œ ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ì‚¼ì•˜ë‹¤. ì°¸ê³ ë¡œ ì›ë…¼ë¬¸ì—ì„œëŠ” í•™ìŠµì‹œ Gaussian noiseë¥¼ ì‚¬ìš©í•˜ë‚˜, í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ë°ì´í„°ì™€ ì¤‘ë³µë˜ë¯€ë¡œ ì´ë¥¼ uniform noiseë¡œ ë°”ê¾¸ì–´ ì„±ëŠ¥ì„ í‰ê°€í–ˆë‹¤. ëŒ€ëµ 30ê°œê°€ ë„˜ëŠ” hyperparametersë¥¼ ìˆ˜ì •í–ˆë‹¤.
* **AutoAugment**ëŠ” ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” augmentation policyë¥¼ ì°¾ì•„ì¤€ë‹¤. ì´ë•Œ ì¶œë ¥ ê²°ê³¼ë¡œ ë‚˜ì˜¨ ìµœì ì˜ augmentations ì¤‘ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ ê²¹ì¹˜ëŠ” corruptionsì€ ì œê±°í–ˆìœ¼ë¯€ë¡œ ë…¼ë¬¸ì—ì„œëŠ” AutoAugment*****ë¡œ í‘œê¸°í•œë‹¤.
* **Random AutoAugment**ëŠ” AutoAugment*ì„ ì‚¬ìš©í•˜ì—¬ ëœë¤ìœ¼ë¡œ ìƒ˜í”Œë§ëœ augmentation policyë¥¼ ê°–ëŠ”ë‹¤. AutoAugmentì™€ ë¹„êµí–ˆì„ ë•Œ ë” ì ì€ ê³„ì‚°ìœ¼ë¡œ ë” ë‹¤ì–‘í•œ augmentationë¥¼ ì œê³µí•œë‹¤ëŠ” íŠ¹ì§•ì´ ìˆë‹¤. 
* **MaxBlurPooling**ì€ ìµœê·¼ì— ì œì•ˆëœ architectural modificationìœ¼ë¡œ, pooling ê²°ê³¼ë¥¼ smoothí•˜ê²Œ ë§Œë“¤ì–´ì¤€ë‹¤.
* **Stylized ImageNet (SIN)**ì€ ì›ë³¸ ImageNet ë°ì´í„°ì— ì¶”ê°€ì ìœ¼ë¡œ style transferê°€ ì ìš©ëœ ImageNetì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê¸°ìˆ ì´ë‹¤

ì—¬ê¸°ì— ì¶”ê°€ì ìœ¼ë¡œ **SINê³¼ AugMixë¥¼ ê²°í•©**í•œ ê²ƒ ì—­ì‹œ ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ì‚¼ì•˜ë‹¤.

#### Training Setup

í•™ìŠµì—ëŠ” Goyal et al. (2017)ì˜ í‘œì¤€ training schemeì„ ë”°ë¥´ëŠ” ResNet-50ì´ ì‚¬ìš©ëœë‹¤.

* **learning rate** : we linearly scale the learning rate with the batch size, and use a learning rate warm-up for the ï¬rst 5 epochs.
* **epochs **: AutoAugment and AUGMIX train for 180 epochs.
* **Preprocessing** : All input images are ï¬rst pre-processed with standard random cropping horizontal mirroring.

#### Resultâ‘ : Corruption Error

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220305184335714.png" alt="image-20220305184335714" style="width:600px;" />

* AugMixëŠ” standardì™€ ë¹„êµí–ˆì„ ë•Œ Clean ErrorëŠ” 23.9â†’22.4%ë¡œ 1.5% ê°ì†Œì‹œí‚´ê³¼ ë™ì‹œì— mean Corruption ErrorëŠ” 80.6â†’68.4%ë¡œ 12.2% ê°ì†Œì‹œì¼°ë‹¤. ê¸°ì¡´ì˜ augmentation ê¸°ë²•ë“¤ì´ clean errorì™€ corruption errorê°€ trade-off ê´€ê³„ê°€ ìˆì—ˆë‹¤ëŠ” ì ì„ ê°ì•ˆí•  ë•Œ ì˜ë¯¸ìˆëŠ” ê²°ê³¼ì´ë‹¤.
* AugMixëŠ” ë‹¤ë¥¸ augmentation ê¸°ë²•ê³¼ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ê¸°ì—ë„ ì¢‹ê²Œ ë§Œë“¤ì–´ì¡Œë‹¤. ì—¬ê¸°ì„œëŠ” SIN(Stylized ImageNet) ê¸°ë²•ê³¼ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•´ ë³¸ ê²°ê³¼, AugMixë§Œì„ ì‚¬ìš©í–ˆì„ ë•Œ ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ê±°ë‘ì—ˆìŒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.

### Resultâ‘¡: Flip Rate

<img src="/files/2022-03-04-ai-ComputerVision-AugMix/image-20220305184649885.png" alt="image-20220305184649885" style="width:600px;" />

* Flip RateëŠ” Perturbation robustness ì¸¡ì • ì§€í‘œì´ë‹¤. ImageNet-Pì— ê° augmentation ê¸°ë²•ì„ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼, ê°€ì¥ ë‚®ì€ mean Flip Rateë¥¼ ë‹¬ì„±í•˜ì˜€ìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

* í•˜ì§€ë§Œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©ëœ corruptionì˜ ì¢…ë¥˜ì— ë”°ë¼ ë‹¤ë¥¸ Flip Rateë¥¼ ë³´ì—¬ì£¼ëŠ”ë°, noise corrruptionì— ëŒ€í•´ì„œëŠ” Patch Uniform ê¸°ë²•ì´ ë” ì„±ëŠ¥ì´ ì¢‹ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.

  > Patch Uniformì€ ëœë¤ìœ¼ë¡œ ì„ íƒëœ ì´ë¯¸ì§€ ì˜ì—­ì— uniform noiseë¥¼ ì£¼ì…â€í•˜ëŠ” augmentation ê¸°ë²•ì´ë‹¤. ì €ìëŠ” ëª¨ë“  ì¢…ë¥˜ì˜ augmentation ê¸°ë²•ì—ì„œ í…ŒìŠ¤íŠ¸ë‹¨ì—ì„œ ë§ˆì£¼ì¹  ìˆ˜ ìˆëŠ” ì¢…ë¥˜ì˜ corruptionsì€ í•™ìŠµì—ì„œ ë°°ì œí–ˆëŠ”ë°, ì—¬ê¸°ì„œëŠ” Gaussianì´ë‚˜ Shot noiseê°€ ì•„ë‹ˆë¼ëŠ” ì´ìœ ì—ì„œ í—ˆìš©ëœ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ì‚¬ì‹¤ìƒ, ë‹¤ë¥¸ ë¶„í¬ë¥¼ ê°–ëŠ” ë…¸ì´ì¦ˆë¥¼ í•™ìŠµí•œ ê²ƒì´ë¯€ë¡œ ì„±ëŠ¥ì´ íŠ¹íˆ ë” ì¢‹ì•„ì§„ ê²Œ ì•„ë‹ê¹Œ ì‹¶ë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ì—­ì‹œ ê¸°ì¡´ì˜ Corruption Memorizingí•œë‹¤ëŠ” ë¬¸ì œì ì— í•´ë‹¹í•œë‹¤.

<br>

<br>

## ì°¸ê³ ìë£Œ

### <a name="AutoAugment">AutoAugment</a>

<img src="https://qph.fs.quoracdn.net/main-qimg-28a78ec3843c310c9218ec697721411d" alt="What is the difference between 6 and 9? - Quora" style="width:200px;" />

ì§€ë‚˜ì¹œ data augmentationì€ manifold intrusionìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì´ì „ê¹Œì§€ëŠ” ë°ì´í„° ë„ë©”ì¸ì— ë”°ë¼ ì–´ë–¤ augmentationì„ ì ìš©í• ì§€ ì„ íƒí•˜ëŠ” ê³¼ì •ì´ ìˆ˜ë™ìœ¼ë¡œ ì´ë£¨ì–´ì¡Œë‹¤ (e.g., "ìƒˆì˜ ì¢…ë¥˜ë¥¼ íŒë³„í•˜ëŠ” ë¬¸ì œì—ì„œëŠ” histogram color swappingê³¼ ê°™ì€ augmentationì„ ì‚¬ìš©í•˜ë©´ ì•ˆë˜ê² êµ°"). **AutoAugment**ëŠ” <u>ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œì˜ íš¨ê³¼ì ì¸ augmentationì„ ì°¾ëŠ” ê³¼ì •ì„ '**ìë™ìœ¼ë¡œ**' ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤</u>. ì´ë¥¼ ìœ„í•´ íƒìƒ‰ê³µê°„(search space)ì„ ì •ì˜í•˜ì—¬ ìµœì í™”ëœ augmentation ê¸°ë²•ì„ ì°¾ëŠ”ë‹¤. ì´ë•Œ íƒìƒ‰ê³µê°„ì€ í•˜ë‚˜ì˜ augmentation policyëŠ” 5ê°œì˜ sub-policiesë¡œ êµ¬ì„±í•˜ê³ , ê° sub-policyëŠ” 2ê°œì˜ image operationìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ì •ì˜ëœë‹¤. ê° sub-policyê°€ ìƒì„±í•˜ëŠ” augmentation ê¸°ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
$$
\text{1ê°œì˜ augmentation policy}\\
= \text{2ê°œì˜ ì´ë¯¸ì§€ ì—°ì‚°}\\
=\text{ì´ë¯¸ì§€ ì—°ì‚°}Ã—\text{ì´ë¯¸ì§€ ì—°ì‚°}\\
=(\text{augmentation ê¸°ë²•} Ã— \text{í™•ë¥ } Ã— \text{ê°•ë„})^2
$$
AutoAugmentëŠ” ê·œëª¨ê°€ ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ íŠ¹íˆë‚˜ ë” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ, ë†’ì€ ê³„ì‚° ë³µì¡ë„ë¡œ ì¸í•´ ì¼ë°˜ì ì¸ ì—°êµ¬ í™˜ê²½ì— ì ìš©ì´ ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.

> AugMixì—ì„œëŠ” ì‹¤í—˜ ë°ì´í„°ì— ì ìš©í•  ì ì ˆí•œ augmentation ê¸°ë²•ì„ ë½‘ì•„ë‚´ê¸°ìœ„í•´ AutoAugmentë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´í›„, ë½‘ì•„ë‚¸ ìµœì í™”ëœ augmentation ê¸°ë²•ë“¤ ì¤‘ ImageNet-Cì™€ ê²¹ì¹˜ëŠ” corruptionsì— ëŒ€í•´ì„œëŠ” ë°°ì œí•˜ê³  ì‹¤í—˜í–ˆë‹¤.

<br>

### <a name="Dirichlet_BetaDistribution">Dirichlet & Beta Distribution</a>

**Dirichlet distribution**ê³¼ **Beta distribution**ì€ ì£¼ë¡œ image classificationì—ì„œ ì‚¬ìš©ë˜ëŠ” parametric distribution*ì´ë‹¤.

* (ì°¸ê³ ) parametric distribution* : ëª¨ìˆ˜ë¥¼ ê°€ì •í•˜ì—¬ ì¶”ì •í•˜ëŠ” ë¶„í¬. ë°ì´í„°ê°€ ì ì–´ë„ ì˜ ë™ì‘í•œë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ, ëª¨ìˆ˜ì— ì˜í–¥ì„ ë°›ìœ¼ë¯€ë¡œ ë°ì´í„°ì— ë”°ë¥¸ ë¶„í¬ ì—…ë°ì´íŠ¸ ë°˜ì˜ì´ ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.

**Beta distribution**ëŠ” univariate íŠ¹ì„±ì„ ê°€ì§€ë©° <u>ë§¤ê°œë³€ìˆ˜ $\alpha$, $\beta$ì— ëŒ€í•´ [0,1] ë²”ìœ„ì—ì„œ ì •ì˜ë˜ëŠ” ì—°ì†í™•ë¥  ë¶„í¬</u>ì´ë‹¤.

<img src="http://i.imgur.com/lna2sdm.jpg" alt="img" style="width:300px;" />

$$
f(x; Î±, Î²) = \frac{\Gamma{(\alpha+\beta)}}{\Gamma{(\alpha)}\Gamma{(\beta)}}x^{\alpha-1}(1-x)^{\beta-1}\\
\Gamma{(n)} = (n-1)!
$$

**Dirichlet distribution**ëŠ” multivariate íŠ¹ì„±ì„ ê°€ì§€ë©° ë‹¤ìŒì˜ ì—°ì†í™•ë¥  ë¶„í¬ì´ë‹¤.

$$
B(\alpha) = \frac{\Pi_{i=1}^k{\Gamma{(\alpha_i)}}}{\Gamma{(\sum_{i=1}^k{\alpha_i})}}
$$

<br>

<br>

## References

* [Hendrycks, Dan, et al. "Augmix: A simple data processing method to improve robustness and uncertainty." *arXiv preprint arXiv:1912.02781* (2019).](https://arxiv.org/pdf/1912.02781.pdf)
* ["google-research/augmix", github, 2022ë…„ 03ì›” 22ì¼ ì ‘ì†, https://github.com/google-research/augmix](https://github.com/google-research/augmix)

* AutoAugment
  * "Fast AutoAugment/1.ë°ì´í„° ì–´ê·¸ë¨¼í…Œì´ì…˜ ì—°êµ¬ ë™í–¥ì„ ì†Œê°œí•©ë‹ˆë‹¤.", kakaobrain ë¸”ë¡œê·¸, 2022ë…„ 03ì›” 02ì¼ ì ‘ì†, [https://www.kakaobrain.com/blog/64](https://www.kakaobrain.com/blog/64)

* Dirichlet & Beta Distribution
  * "Dirichlet distribution(Dirichlet prior)ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ", donghwa-kim.github.io, 2019ë…„ 04ì›” 02ì¼ ìˆ˜ì •, 2022ë…„ 03ì›” 02ì¼ ì ‘ì†, [https://donghwa-kim.github.io/distributions.html](https://donghwa-kim.github.io/distributions.html)
  * "ì´í•­ë¶„í¬, ë‹¤í•­ë¶„í¬, ë² íƒ€ë¶„í¬, ë””ë¦¬í´ë ˆë¶„í¬", ratsgo's blog, 2017ë…„ 05ì›” 28ì¼ ìˆ˜ì •, 2022ë…„ 03ì›” 02ì¼ ì ‘ì†, [https://ratsgo.github.io/statistics/2017/05/28/binomial/](https://ratsgo.github.io/statistics/2017/05/28/binomial/)

<br>

<br>