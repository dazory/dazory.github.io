---
toc: true
toc_label: "Table of Contents"
toc_icon: "cog"
toc_sticky: true

layout: post-with-comments # single
title: '[AIAS] Lect7. CNN-Convolution Neural Network(2)'
excerpt: "[AIAS] 인하대학교 홍성은 교수님의 인공지능 응용 시스템 수업을 듣고 공부한 자료입니다."
date: 2022-04-24
last_modified_at: 2022-04-24
categories:
  - deep_learning-lecture
tags: 
   - [AIAS]

use_math: true
comments: true
share : false
---

AIAS-Lect7_CNN: Convolutional Neural Networks(2)





# Fancier CNN Architectures

이전시간에는 가장 기초적인 CNN architectures에 대해서 배웠다. 이번 시간에는 좀 더 새로운 CNN architectures에 대해 살펴본다.

## CNN architectures 비교

유명한 몇몇 CNN architectures에 대한 비교 그림은 아래와 같다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419201118909.png" alt="image-20210419201118909" style="zoom:80%;" />

위 그림에서 x축은 operations(연산량)을, y축은 Top-1 accuracy를, 원의 크기는 parameters의 개수를 의미한다. 위 그림으로 부터 몇몇 architectures에 대해 분석해보자.

이전에 설명하지 않았던 **Inception-v4**모델이 정확도가 상당함을 확인할 수 있다. Inception-v4는 ResNet에 Inception모델을 합친 것으로, 비교적 적은 aprameters와 연산량으로 높은 성능을 도출한다. **VGG**는 parameters의 개수가 많고 그에 따라 연산량도 많다. 이를 통해 일반적으로 parameters 개수가 많은 경우 연산량도 그에 비례해서 많다는 것을 알 수 있다. 어떻게 생각하면 당연한 말인데, parameters(weights)는 각각의 tensor들과 곱해지고 더해지기 때문이다. **GoogLeNet**은 그래프의 왼쪽 중간부분에 위치한다. 아주 적은 parameters와 연산량만으로도 꽤 좋은 성능을 발휘하는 효율적인 architecture임을 알 수 있다. **AlexNet**은 왼쪽 하단부에 위치한다. 연산량은 매우 작지만 parameters가 매우 많아 꽤 많은 memory를 필요로한다. **ResNet**은 무난하게 사용할 수 있는 architecture이다. 



## Identity Mappings in Deep Residual Networks

이전에는 모델이 너무 deep하면 backpropagation이 제대로 되지 않아 성능이 떨어진다고 통상적으로 생각했다. 이러한 통념을 깨뜨린 architecture가 바로 ResNet이다. Kaiming He(ResNet 창시자)는 **ResNet**모델에 **identity mapping**이라는 개념을 처음 도입함으로써 feed forward와 backpropagation을 할 때 어떠한 연산 없이 직접적으로 전파할 수 있게 하였다. 이를 통해 <u>backpropagation시 발생하는 vanishing문제를 해결</u>하여 더 좋은 성능을 이끌어 냈다. Kaiming He는 ResNet에서 그치지 않고 ResNet을 더욱 개선시키고자 하였다. 따라서 아래와 같이 각 layer의 위치를 조금씩 바꿔가며 다양한 실험을 해봤다. 

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419202939354.png" alt="image-20210419202939354" style="zoom:100%;" />

그 결과 **(e) full pre-activation** 구조가 가장 성능이 좋다는 것을 알게 되었다.



## Wide Residual Networks

Sergey Zagoruyko는 depth보다 residuals가 더 중요한 요소라고 생각했다. 따라서 다음과 같은 실험을 했다.

> [여기](https://www.edwith.org/deeplearningchoi/lecture/15566?isDesc=false)에 실험에 대한 내용과 비교가 자세히 나와있다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419204241531.png" alt="image-20210419204241531" style="zoom:80%;" />

depth=40일 때 보다 오히려 depth=28인 경우 성능이 더 좋아졌다. 통상적으로 depth가 크면 더 좋은 성능을 낸다고 생각되어 왔으나 depth값이 다는 아니라는 걸 알 수 있다. GPU가 많을 수록 동시에 학습시킬 수 있는 양이 많아져 batch size를 늘릴 수 있다. 하지만 layer의 depth가 많아진다면 GPU가 아무리 많아도 오랜 시간이 소요된다. 이러한 실험 결과는 depth보다 width(channels 개수)가 성능을 향상시킬 수 있는 요소에 가깝다는 것을 말하고 있다. 이러한 실험 결과를 바탕으로 Sergey Zagoruyko는 아래와 같이 ResNet가 큰 width를 갖도록 변형한 wide residual networks을 제시하였다.

![image-20210419203133013](/files/2022-04-24-ai-deep_learning-DL06/image-20210419203133013.png)

depth보단 width를 증가시킨 이러한 모델은 좋은 성능을 낼 뿐만 아니라 병렬연산에도 더 효율적으로 작용하였다.



## Aggregated Residual Transformations for Deep Neural Networks (ResNeXt)

이 모델 역시 ResNet의 창시자인 Kaiming He에 의해 제안되었다. inception module과 같이 parallel pathways를 여러 개 두어 residual block의 width를 증가한 다음과 같은 구조를 제안하였다.

![image-20210419205256079](/files/2022-04-24-ai-deep_learning-DL06/image-20210419205256079.png)



## Densely Connected Convolutional Networks (DenseNet)

각 layer가 feedforward 방식으로 다른 모든 layers에 연결되는 dense blocks으로 구성되어 있다. 이를 통해 vanishing gradient를 제거하고 feature 재사용을 통해 feature propagation을 강화하였다. 단 50개의 layers만으로 152개의 layers를 가진 ResNet보다 더 나은 성능을 보여주었다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419205430148.png" alt="image-20210419205430148"  />





## Neural Architectures Search with Reinforcement Learning (NAS)

이전까지는 사람의 영역이었던 hyperparameters(filters의 개수, height, width, stride heigth 등)를 컴퓨터에게 학습시키는 구조이다. 하지만 computing 자원이 너무 많이 드는 문제로 인해 왠만한 회사 규모에서는 사용이 불가능하다는 단점이 있다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419205756468.png" alt="image-20210419205756468" style="zoom:80%;" />



## EfficientNet: Smart Compound Scaling

EfficientNet은 NAS의 단점을 보완하고자 나온 network이다. NAS가 각각의 parameter 단위로 control하는 모델이었다면, EfficientNet은 network의 경향을 정해준다. 가령 아래와 같은 다양한 형태의 networks가 있을 수 있다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419210128523.png" alt="image-20210419210128523" style="zoom:80%;" />

예를 들어, network는 길쭉할 수도 있고(deeper) 뚱뚱할 수도 있다(wider). NAS처럼 너무 디테일하게 학습하는 것이 아니라 이러한 경향성을 정해주어 학습시킴으로써 더 적은 자원으로 효율적인 결과를 내었다.

> 굉장히 유명한 network 중 하나이다. 관심이 있다면 논문을 읽어보는 것도 좋다.



# Different Types of Convolutions

이전까지 우리는 일반적인 convolutions에 대해 배웠다. 가령 convolution layers가 필요로 하는 hyperparameters는 다음과 같다.

- Number of filters **K**
- The filter size **F**
- The stride **S**
- The zero padding **P**

hyperparameters가 결정되면 이 layer의 출력 사이즈는 다음과 같이 결정된다.
$$
<<output\ size: W_2×H_2×K>>\\
W_2=\frac{W_1-F+2P}{S}+1\\
H_2 = \frac{H_1-F+2P}{S}+1
$$

또한 parameters의 개수는 아래와 같이 결정된다.
$$
F^2CK+K(biases)
$$
이번 시간에는 확장된 convolution의 종류에 대해 살펴본다.



## Dilated Convolution (a.k.a. atrous convolution)

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419211136063.png" alt="image-20210419211136063" style="zoom:80%;" />

standard convolution의 경우 연속적인 receptive region 내에서 convolution 연산을 수행했다. 반면 **dilated convolution은** parameters의 개수는 유지하되, 중간중간 영역을 skip하는 convolution 기법이다. 아래 그림은 dilate값에 따른 receptive region 영역의 크기를 보여준다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419211621134.png" alt="image-20210419211621134" style="zoom:80%;" />

위 그림에서 빨간 점의 부분은 convolution 연산에 사용되는 point를 의미한다. 가장 왼쪽의 1-dilated convolution은 우리가 잘 아는 standard convolution에 해당한다. 1-dilated conv는 3×3 receptive region을 갖는다. 한편, 가운데 2-dilated conv은 한 칸씩 간격을 둠으로써 receptive region이 7×7로 확장되었다. 4-dilated conv인 경우는 receptive region이 더욱 확장되어 15×15의 크기를 갖는다는 것을 알 수 있다. 이처럼 dilated convolution을 사용하면 <u>적은 parameters 개수만으로도 해상도의 손실을 줄이고 receptive region의 영역을 확장</u>할 수 있게 된다. 

![image-20210419212139131](/files/2022-04-24-ai-deep_learning-DL06/image-20210419212139131.png)

이미지의 전체적인 특징을 잡기 위해서는 receptive field가 클 수록 좋다. 하지만 너무 큰 receptive field는 과한 연산을 필요로하고 overfitting이 발생할 위험도 있다. 따라서 일반적인 CNN에서는 pooling을 통해 차원을 줄인 뒤 convolution을 수행함으로써 이러한 단점을 개선한다. 하지만 이런 경우 pooling layer로 인해 정보의 손실이 발생할 수 있다. dilated convolution은 pooling layer없이도 이러한 문제를 해결한다는 점에서 유용하다.



## (Spatial) Separable Convolution

separable convolution은 연산 경량화를 위한 일종의 트릭이다. 

![image-20210419213235933](/files/2022-04-24-ai-deep_learning-DL06/image-20210419213235933.png) 

일반적으로 convolution 연산은 2차원 영역에서 수행된다. 하지만 separable convolution은 이러한 kernel을 두 개의 1차원 kernel로 쪼개어 연산을 수행함으로써 더 간단한 연산을 통해 모델의 속도를 향상시킬 수 있다. 하지만 모든 kernel을 두 개의 작은 kernel로 분리할 수 없다는 문제가 있다.



## Depth-wise Separable Convolution

일반적인 convolution은 입력 이미지의 모든 channel이 의존적인 관계를 갖는다(즉 한꺼번에 묶어 연산을 수행한다.).  depth-wise separable convolution은 각 channel의 관계를 독립적으로 만든 뒤, spatial 영역에서의 convolution만을 수행한다. 

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419214305340.png" alt="image-20210419214305340" style="zoom:80%;" />

이러한 특징으로 인해 convolution layer를 지나도 input data와 output data의 channel수가 동일하게 된다.



## Grouped Convolution

![image-20210419214544542](/files/2022-04-24-ai-deep_learning-DL06/image-20210419214544542.png)

**Grouped convolution**은 input data의 channels을 여러 개의 group으로 나누어 독립적으로 convolution을 수행한다. 이러한 방식은 병렬처리에 유리하다는 장점이 있다. 또한 이러한 기법은 기존의 convolution보다 더 적은 parameters 개수를 갖는다. 아래의 수식은 grouped convolution이 더 적은 parameters 개수를 갖는다는 것을 잘 보여준다.

filter size K, input channel C, output channel M
$$
<<Standard\ Convolution>>\\
total\ parameters = K^2CM\\
operations: K^2CM*HW번\\
$$
input channels을 g개로 나눈다고 할 때 다음과 같다.
$$
<<Grouped\ Convolution>>\\
total\ parameters = K^2(C/g)(M/g)*g=K^2CM/g\\
operations: K^2CM/g*HW번\\
$$


# Semantic Segmentation

일반적으로 computer vision분야에서의 주요 관심 task는 아래와 같다.

![image-20210419215506815](/files/2022-04-24-ai-deep_learning-DL06/image-20210419215506815.png)

- **Classification**
- **Semantic Segmentation**은 scene을 어떤 의미 단위로 parsing한 뒤 labeling하는 것이다. 
- Object Detection
- Instance Segmentation

다양한 computer vision tasks 중 이번에는 semantic segmentation에 대해 자세히 살펴본다.

## The Problem

classification은 단순히 전체 image로부터 하나의 class를 도출해낸다면, semantic segmentation은 영역별로 class를 도출해야한다. 이러한 학습을 위해서는 영역별로 labeling된 data가 필요하므로 <u>data annotation 비용</u>이 크게 발생한다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419221737147.png" alt="image-20210419221737147" style="zoom:80%;" />

Data annotation(데이터 주석화)는 위 그림과 같이 이미지에 labeling작업을 한 데이터를 의미한다. 매우 까다로운 작업이므로 data annotation만을 전문적으로 하는 회사도 존재한다.

한편, semantic segmentation은 아래 그림과 같이 classification이 pixel단위로 수행된다.

![image-20210419222206920](/files/2022-04-24-ai-deep_learning-DL06/image-20210419222206920.png)

이때 세 번째 extract patch의 경우 완전한 grass라 하기엔 일부 이미지가 cow이므로 <u>classification하기 애매</u>하다는 문제점도 있다.

또한 각 pixel에 대해서 전부다 CNN network를 통과시켜 test하는 방식은 너무 <u>비용 소모적</u>이라는 단점이 있다. 이러한 문제를 해결하기 위해 아래와 같이 하나의 CNN 모델만으로 오른쪽 결과와 같은 semantic map을 만들어주는 방식을 생각해볼 수 있다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419222510051.png" alt="image-20210419222510051" style="zoom:80%;" />

하지만 이러한 모델은 결국 input image와 같은 size의 output image를 생성하는 모델과 같다. input image와 동일한 크기의 output image를 생성해야하기 위해 아래와 같이 pooling layer를 최대한 배제한 architecture를 생각해 볼 수 있다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419222936108.png" alt="image-20210419222936108" style="zoom:80%;" />

하지만 결국 각 convolution layer가 input image와 동일한 해상도의 data를 계속해서 담아둬야하므로 모델이 무거워진다는 문제가 발생한다. 이러한 문제를 해소하기 위해 아래와 같이 downsampling & upsampling의 방식을 생각해 볼 수 있다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419223152010.png" alt="image-20210419223152010" style="zoom:80%;" />

**downsampling**은 이미지 데이터를 줄이는 기법으로 pooling, strided convolution 등의 기법을 통해 수행될 수 있다. **upsampling**은 데이터 양을 늘리는 기법으로, unpooling, strided transpose convolution 등의 기법을 통해 수행될 수 있다. 통상적으로 위 이미지의 architecture에서 왼쪽 (붉은) 부분은 **encoder**, 오른쪽은 **decoder**라고 부른다. 

## Unpooling

**Unpooling**은 upsampling을 수행할 수 있는 기법 중 하나로, 다음의 이미지는 unpooling 과정을 잘 나타낸다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419223649980.png" alt="image-20210419223649980" style="zoom:80%;" />

**Nearest Neighbor** 기법은 가장 직관적인 unpooling 기법으로, 영역을 pixel의 값으로 모두 채워주는 작업을 수행한다. **Bed of Nails**는 일종의 padding과 같이 영역의 pixel값을 특정 value로 모두 채워준다.

또 다른 unpooling 기법으로 max unpooling이 있다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419223842839.png" alt="image-20210419223842839" style="zoom:80%;" />

**max unpooling**은 max pooling과 대치되는 작업이다. 일반적으로 downsampling 과정을 거친 뒤 upsampling을 수행하는데, downsampling 기법으로 max pooling을 사용하는 경우, downsampling된 결과를 max값이었던 영역에 채워넣는 방식을 의미한다. 

하지만 unpooling방식은 너무 단순하다는 문제가 있다. 이러한 문제를 해결하기 위해 등장한 개념이 바로 transpose convolution이다.



## Transpose Convolution

**Transpose convolution**은 convolution과정을 반대로 수행한다는 뜻에서 deconvolution이라고도 불린다(사실 엄밀히 말하면 transpose convolution이 맞는 개념이지만 학부수준을 넘어가므로 이 부분은 생략한다.). 하지만 transpose convolution는 아래 그림과 같이 부자연스러운 결과를 출력한다는 단점이 있다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419224502599.png" alt="image-20210419224502599" style="zoom:80%;" />



## Unpooling & Convolution

이러한 transpose convolution의 문제점으로 인해 실제로는 오히려 unpooling 기법이 더 많이 사용된다. 대신에 unpoolin기법의 단점을 보완하고자 아래와 같이 convolution layers를 추가로 두어 learnable하게 모델을 디자인하는 방식으로 semantic segmentation이 수행된다.

![image-20210419224800606](/files/2022-04-24-ai-deep_learning-DL06/image-20210419224800606.png)



## Conditional Random Field (CRF)

semantic segmentation을 수행할 때 아래 그림과 같이 분류하기에 경계가 모호한 이미지가 입력될 수 있다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419225151561.png" alt="image-20210419225151561" style="zoom:80%;" />

이러한 경우 **conditional random field(CRF)** 기법을 이용하여 입력 이미지에 대해 다음과 같이 경계를 자연스럽게 만들어줄 수 있다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419225256633.png" alt="image-20210419225256633" style="zoom:80%;" />



## Semantic segmentation의 관심사

예를 들어 cow가 두 마리 있는 이미지는 아래와 같이 하나의 영역으로 퉁 쳐서 cow라고 출력된다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419225608203.png" alt="image-20210419225608203" style="zoom:80%;" />

이처럼 semantic segmentation은 특정 영역의 class가 궁금할 뿐, class가 몇 개 존재하는지는 관심 영역이 아니다.





# Object Detection & Instance Segmentation

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419225326075.png" alt="image-20210419225326075" style="zoom:80%;" />

이번에는 object detection과 instance segmentation에 대해 자세히 살펴보자. 실제 field에서는 semantic segmentation보다는 object detection과 instance segmentation가 더 많이 활용된다.



## Semantic Segmentation vs. Instance Segmentation

semantic segmentation은 입력 이미지의 모든 영역이 관심 대상인 반면, instance segmentation은 predefined class에 대해서만 관심 대상이라는 점에 차이가 있다.



## Single Object

입력 이미지에 하나의 object만 있는 경우를 통해 object detection 방법을 이해해보자. 이러한 경우 모델은 classification과 localization의 tasks를 수행한다고 볼 수 있다. 따라서 이러한 두 개의 tasks를 수행하기 위해서는 각각 다른 목적을 가진 loss function이 따로 존재해야 한다. 이러한 loss function을 **multitask loss**라고 부른다. 

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419230240841.png" alt="image-20210419230240841" style="zoom:80%;" />

예를 들어 classification task를 수행하는 loss는 일반적으로 우리가 아는 classification loss를 가지며, localization task를 수행하는 loss는 bounding box를 표현하기 위한 x, y, width, height의 값에 대한 loss를 갖는다. 



## Multiple Objects

하나의 입력 이미지에 여러 개의 objects가 있는 경우는 모델이 어떻게 구성될까?

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419230610068.png" alt="image-20210419230610068" style="zoom:80%;" />

두 번째 이미지와 같이 3개의 objects가 있는 경우는 단순하게 13개의 숫자 결과를 출력하면 된다. 하지만 세 번째 이미지와 같이 하나의 이미지에 너무 많은 objects가 존재하는 경우에는 너무 많은 숫자가 출력된다는 문제가 발생한다.

이러한 문제를 해결하기 위해 아래와 같은 형태의 출력 형태를 생각해볼 수 있다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419230915189.png" alt="image-20210419230915189" style="zoom:80%;" />

위 이미지는 어떠한 box 영역을 슬라이딩시키며 이미지 전체에 대해 각 classification 결과를 출력하는 방법을 보여준다. 하지만 이러한 방법은 컴퓨팅 자원이 많이 소요된다는 문제가 있다.

이러한 문제를 해결하기 위해 deep learning 이전의 computer vision에서는 다음과 같이 object를 포함할 것 같은 이미지 영역을 찾아주는 알고리즘을 사용해왔다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419231130599.png" alt="image-20210419231130599" style="zoom:80%;" />

이러한 알고리즘을 **region proposals**이라고 부르며 비교적 연산이 적게 필요하다는 장점이 있다.



## R-CNN

**R-CNN**은 최초로 deep learning을 object detection영역에 적용시킨 모델로 잘 알려져 있다. R-CNN의 과정에 대해 살펴보자. 먼저 주어진 이미지에 대해 region proposal을 적용시켜서 object라 추정되는 영역을 도출한다. 이후 이러한 영역을 모두 같은 size를 갖도록 resize한다.  이렇게 resize된 영역은 **warped image region**라고 부른다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419231626283.png" alt="image-20210419231626283" style="zoom:80%;" />

이후 각 이미지 영역을 각각 동일한 pretrained network에 입력하여 classification(using SVMs)을 수행한다. 이때 CNN 내부에 추가로 box의 정보를 출력하는 Bbox regressor가 존재하여 box의 정보도 함께 출력한다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419231938061.png" alt="image-20210419231938061" style="zoom:80%;" />

이러한 R-CNN 구조는 잘 작동하지만 너무 느리다는 단점이 있었다. 이러한 문제를 해결하기 위해 등장한 모델이 바로 **Fast R-CNN**이다.



## Fast R-CNN

 Fast R-CNN은 전체 이미지에 대해서 Convolution network를 통과시킨다. 그 결과로 나온 features에 대해 region proposal을 적용한 뒤 crop&resize한다. 이러한 과정을 통해 최종적으로 도출된 각각의 patch을 보다 가벼운 CNN에 입력하여 object category와 box offset을 도출한다. 전체적인 과정은 아래 그림과 같다.

![image-20210419232459196](/files/2022-04-24-ai-deep_learning-DL06/image-20210419232459196.png)



### RoI Pooling

Fast R-CNN에서 crop + resize 과정을 **RoI Pooling**이라고 한다. RoI pooling에 대해서 좀 더 자세히 살펴보자. 먼저 crop된 이미지를 정수형태의 사이즈를 갖도록 반올림 등의 과정을 통해 align한다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419232727505.png" alt="image-20210419232727505" style="zoom:50%;" />

이후 원하는 결과 사이즈가 나오도록 해당 영역을 나누고 pooling과 같은 기법을 사용하여 결과 사이즈로 mapping시킨다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419232945938.png" alt="image-20210419232945938" style="zoom:50%;" />

하지만 이러한 방식은 결과 사이즈로 딱 떨어지지 않는 영역에 대해서는 미묘하게 misaligned된다는 단점이 있다.



### R-CNN vs. Fast R-CNN

다음 그래프는 R-CNN과 Fast R-CNN의 training time과 test time을 보여준다.

![image-20210419233337710](/files/2022-04-24-ai-deep_learning-DL06/image-20210419233337710.png)

R-CNN에 비해 fast R-CNN가 압도적으로 빠르다는 것을 확인할 수 있다. 이때 fast R-CNN은 테스트에 대략 2.3초가 소요되었는데, 이 중 대부분(2초)은 region proposal 과정에서 발생하였음을 확인할 수 있다. 이러한 단점을 해결하고자 **Faster R-CNN**이 제안되었다.



## Faster R-CNN

**Faster R-CNN**은 CNN에 Region proposal network(RPN)을 추가함으로써 fast R-CNN의 단점을 해결하였다. 전체적인 구조는 아래와 같다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419234344960.png" alt="image-20210419234344960" style="zoom:60%;" />

이때 region proposal network의 output과 최종적인 output이 모두 동일하게 classification loss와 bounding-box regression loss임을 알 수 있다. region proposal network는 물체인지 아닌지(yes or no, 일종의 binary classification)에 관심이 있다. 이러한 추론 영역을 기반으로 어떤 object인지에 대한 classification이 수행된다고 볼 수 있다.

정리하자면 faster R-CNN은 아래와 같이 region proposal network까지와 그 이후의 두 단계로 나눌 수 있다.

- First stage: Run once per image (CNN+feature map+Region Proposal Network)
  - backbone network
  - Region proposal network
- Second stage: Run once per region (RoI pooling)
  - Crop features: RoI pool / align
  - Predict object class
  - Prediction bbox offset

이러한 faster R-CNN은 아래와 같이 더 빠른 성능을 보여준다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419234932521.png" alt="image-20210419234932521" style="zoom:80%;" />

### RoI Align

fast R-CNN의 경우, RoI pooling기법을 사용한다. RoI pooling의 경우 원하는 출력 사이즈로 딱 떨어지지 않는 영역에 대해서는 미묘하게 misaligned된다는 단점이 있었다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419235224399.png" alt="image-20210419235224399" style="zoom:80%;" />

위 그림은 RoI Align을 나타낸다. RoI align은 정수에 집착하지 않는 bilinear interpolation 과정을 통해 RoI pooling기법의 단점을 해결할 수 있다.



## Single-stage O.D vs. Two-stage O.D.

> Object detection을 O.D라고 줄여서 부르기도 한다.

<img src="/files/2022-04-24-ai-deep_learning-DL06/image-20210419235611634.png" alt="image-20210419235611634" style="zoom:80%;" />

일반적으로는 two-stage object detection방법이 주로 사용되어왔다. 하지만 소요되는 시간을 좀 더 줄이기 위해 이러한 단계를 한꺼번에 수행하는 single-stage O.D architecture가 제안되었다. two-stage O.D는 느리지만 정확도가 높은 반면, single-stage O.D는 빠르지만 비교적 정확도가 낮다는 각각의 장단점을 갖는다. 실시간성이 중요한 경우에는 주로 single-stage O.D 모델이 사용된다.

대표적인 single-stage O.D 모델에는 YOLO, SSD, CornerNet, RetinaNet 등이 있으며, 대표적인 two-stage O.D모델에는 RCNN, Faster RCNN, SPP-net, R-FCN 등이 있다.



> 일반적으로 two-stage O.D모델로는 faster R-CNN을, single-stage O.D로는 YOLO나 SSD모델을 주로 사용한다.

