var store = [{
        "title": "[DL-01] Lect2. Machine Learning Basics",
        "excerpt":"  Machine Learning Overview   이번 페이지에서는 Machine Learning의 전반적인 개요에 대해 알아본다.   Machine Learning의 분류   Machine Learning(ML)은 크게 세 파트로 분류할 수 있다.      Supervised Learning   Unsupervised Learning   Reinforcement Learning   위 세 종류의 학습 방법을 좀 더 자세히 알아보자.   Supervised Learning (지도학습)   Supervised Learning은  target value가 존재하는 학습 방식으로, input값에 label을 할당한다. 예를 들어, 모델을 학습시킬 때 input값으로 강아지 이미지와 함께 “Dog”라는 label을 함께 주는 경우를 의미한다. supervised learning은 예측 결과가 연속적인지 혹은 불연속적인지 여부에 따라 classification model과 regression model로 나눌 수 있다. 간단히 말하자면, 예측 결과가 불연속적인 classification model의 결과값은 class 형태이고 예측 결과가 연속적인 regression model은 결과값이 value의 형태이다.      주의사항: 확률은 일반적으로 연속적인 형태를 띠므로 확률을 예측하는 문제는 regression model이라고 생각하기 쉽다. regression 문제가 아닌 것을 regression model로 푸는 경우 학습이 제대로 되지 않으므로 두 개념을 확실히 구분해야한다.     예를 들어, ‘욕설을 필터링하는 문제’를 해결하는 모델을 만든다고 생각해보자. 이 문제에서 결과값은 욕설이냐(1) 아니냐(0)로 구분될 수 있다. 즉 불연속적인 결과값을 갖는 supervised learning 문제이다. 하지만 누군가는 이를 욕설일 확률(0~1 사이의 연속적인 값)문제로 해결하고자 할 수 있다. 이렇게 문제를 해결하려는 경우 두가지 문제에 직면할 수 있다. 첫번째는, 입력 데이터에 대해 label을 어떻게 주는가에 관한 문제이다. 예를 들어 “이런 ㅅㅂ”이라는 문장이 욕설일 확률은 얼마인가? 설령 입력데이터를 특정 확률로 라벨링을 했다 할지라도 어느 범위까지를 욕설로 분류할 것인가에 대한 문제에 직면한다. 50% 이상인 경우를 욕이라고 판단한다고 하더라도 데이터가 많아짐에 따라 그러한 구분선은 또다시 무의미해질 수도 있다. 따라서 이런 문제의 경우 애초에 regression model로 풀어야 한다.    classification model과 regression model은 문제에 따라 다음과 같이 분류될 수 있다.   Classification Model (분류모델)   Classification Model은 예측값이 discrete한 경우를 의미한다. classification model은 다음과 같이 4가지 model로 나뉠 수 있다.      Binary Classification: 두 가지 class중 하나의 class로 예측되는 경우이다. 예를 들어, 입력된 이미지가 강아지인지 혹은 고양이인지 분류하는 경우에 해당한다.   Multi-class Classification: 두 가지 이상의 classes 중 하나의 class로 예측하는 경우이다. 예를 들어, 입력된 이미지가 강아지인지 고양이인지 혹은 자동차인지로 분류하는 경우에 해당한다.   Multi-label classification: 하나 이상의 classes 중 하나 이상의 classes로 예측하는 경우이다. 예를 들어, 특정 영화의 카테고리를 분류할 때 로맨스이면서 공포물일 수 있는 경우에 해당한다.   Imbalanced Classification: 각 class의 데이터 개수가 불균형한 경우의 예측이다. 예를 들어, 백인 남성의 이미지만 유독 많은 경우, 입력된 사람 이미지에 대해 백인 남성으로 분류할 가능성이 높아지는 분류 모델에 해당한다. 이 경우 모델이 적절하게 학습되지 않을 확률이 높으므로 적절한 데이터 처리를 해줘야한다.   Regression Model (회귀모델)   Regression Model 은 예측값이 continuous한 경우를 의미한다.      주의사항: logistic regression은 분류모델에 주로 사용되는 알고리즘이다. 이름에 regression이 들어간다고 착각하지 말자.    Unsupervised Learning (비지도학습)   Unsupervised Learning은 discrete 값이든 continuous값이든 예측할 수 있는 범위가 있는 supervised learning과 달리 target value가 없는 학습 방식이다. supervised learning이 정답을 알려주는 학습법이라면, unsupervised learning은 정답없이 input data의 특성만으로 특정 그룹으로 분류해주는 학습법이다.   Reinforcement Learning (강화학습)   Reinforcement Learning의 가장 대표적인 예로는 alphaGo가 있다. 마치 당근과 채찍을 번갈아 가면서 주는 방식으로, machine이 어떠한 action을 취하면 그것에 대해 reward를 주는 식으로 학습이 진행된다. reinforcement learning에서 가장 중요한 부분은 exploitation(익숙한 것)과 exploration(새로운 것) 사이의 균형을 맞추는 것이다. machine이 exploitation만을 선택한다면 정답률은 100%에 가깝겠지만 실제로는 학습되고 있는 것이 없는 경우이다. 반대로 exploration만을 선택한다면 정답률이 바닥을 칠 것이다. 따라서 machine이 양쪽의 경우를 골고루 선택하여 학습하도록 하는것이 중요하다.   Machine Learning vs. Deep Learning   머신러닝, 딥러닝, 인공지능.. 많이 들어는 봤는데 정확히 어떤 점에서 서로 다른지 헷갈린다. 이런 헷갈리는 개념에 대해서 확실히 하고 넘어가자. 먼저 AI, ML, DL의 관계를 보면 다음과 같다.      Artificial Intelligence(AI, 인공지능)은 이전까지는 인간만이 할 수 있다 여겨졌던 사고, 학습, 자기개발 등의 분야를 컴퓨터가 대체할 수 있도록 연구하는 분야이다. 기존에는 개발자가 입력과 출력사이의 관계를 직접 정의하여 명확한 규칙 아래에 모델이 돌아갔다면, AI는 자체적으로 구축한 규칙 하에 시스템이 돌아가게 된다. 즉, 이전까지는 사람에게 컴퓨터가 의존했다면 AI는 사람에 대한 의존도를 낮춘 방식이라고도 할 수 있다.   Machine Learning(ML, 머신러닝)은 AI의 하위 개념으로, 대량의 데이터와 알고리즘을 바탕으로 컴퓨터를 학습시켜 컴퓨터 스스로가 타겟 테스크를 수행하는 방식을 학습하는 것을 목표로 한다.   Deep Learning(DL, 딥러닝)은 ML의 하위 개념으로, 기계가 어떤 과제 수행을 위해 스스로 학습하도록 하는 neural networks에 기반을 둔다. ML의 경우 학습 데이터를 수동으로 제공하는 반면, DL은 분류에 사용할 데이터를 스스로 학습할 수 있다는 점에서 차이가 있다.   아래 이미지는 ML과 DL의 차이를 더욱 명확히 보여준다.      예를 들어 고양이 image가 입력될 때 “Cat”을 출력하도록 하는 시스템을 개발한다고 가정하자. 이때 ML의 경우 고양이에 관한 feature를 사람이 수동으로 입력해 줘야 한다. 이러한 과정을 feature extraction이라고 하는데, DL의 경우는 이러한 feature extraction 과정 마저도 machine이 알아서 처리해준다. 즉, feature extraction과정이 (user-defined) hyperparamters인지 (learnable) model paramters인지의 차이라고 볼 수 있다.      Model Parameters vs. Hyperparameters     model parameters는 기계가 학습할 수 있는 parameter를 의미하는 반면 hyperparamters는 사람이 직접 입력해주는 parameter를 의미한다. machine의 입장에서 사람은 초월적인(hyper) 존재이다. 따라서 machine은 hyperparameters를 건드릴 수 없고 본인이 control할 수 있는 model parameters 내에서 학습을 하게 된다.    Data-Driven ML   ML은 많은 양의 데이터를 필요로 한다. 이때 data를 어떻게 활용하는 지(나누는 지)도 중요한 이슈이다. 다음으로 data를 활용하는 몇 가지 case에 대해 장단점을 살펴보도록 한다.   Case1) Choose hyperparameters that work best on the data      모델을 학습시킬 때 내가 가진 모든 dataset을 활용하는 경우이다. 이 경우 개발자는 dataset으로 모델을 학습시킬 때 최적의 hyperparameters를 결정하게 된다. 이 경우  machine은 주어진 dataset에 대해서 최고의 성능(100%)을 발휘하게 된다. 하지만 학습에 사용되지 않은 새로운 data가 들어왔을 때 이 machine이 제대로 성능을 발휘하는 건지 확인 할 방법이 없다.   Case2) Split data into train and test, choose hyperparameters that work best on test data      case 1에서 새로운 data에 대해 모델의 성능을 확인할 수 없다는 문제점에 직면했다. 따라서 이번에는 dataset을 train용과 test용으로 나누었다. 이후 train data를 통해 모델을 학습시키고 해당 결과를 바탕으로 test data를 활용하여 최적의 hyperparameters를 결정한다. case1에 비해서 더 좋은 성능을 보이겠지만, 이 경우 역시 (test data에) 최적인 hyperparameters에 대해서 새로운 데이터가 들어왔을 때 성능을 평가할 수 없다.   Case3) Split data into train, val, and test; choose hyperparameters on val and evaluate on test      dataset을 train, validation, test용으로 나누었다. 먼저 train data를 통해 모델을 학습시킨다. 이후 학습된 모델을 바탕으로 validation data를 통해 최적의 hyperparameters를 결정한다. 그렇게 결정된 모델에 대하여 test data를 통해 성능을 평가한다.      피터드러커는 “측정할 수 없다면 더이상의 발전은 가능하지 않다.”고 말했다. 이처럼 성능평가는 공학분야에서 필수이다.    Case4) Cross-Validation: Split data into folds, try each fold as validation and average the results      case1~3을 통해 기본적으로 dataset을 train, validation, test용으로 나눠야한다는 것을 확인했다. 이번에는 dataset을 쪼갤 때 일종의 트릭을 사용해본다. case3의 경우 내가 나눠 준 train, validation, test data가 타당한지 잘 모르겠다. 우연히 data를 잘 쪼개서 결과가 잘 나온 것일 수도 있지 않는가? 따라서 이번에는 validation을 바꿔줘가며 학습을 수행한다. 이때 중요한 점은, test data는 한 번 결정되면 절대 학습에 사용되어서는 안된다는 점이다. 이렇게 dataset을 나누는 방식은 내가 가진 dataset이 너무 작을 경우에는 유용하지만, deep learning을 하는 경우 같은 data가 너무 반복적으로 사용되어 시간이 오래 걸린다는 단점으로 인해 잘 사용되지는 않는다.   Linear Regression   ML의 경우 학습 방식에 따라 supervised learning, unsupervised learning, reinforcement learning으로 나눌 수 있다. supervised learning의 일종인 linear regression에 대해서 자세히 알아본다.      까먹었을까봐 다시 언급     supervised learning은  target value가 존재하는 학습 방식으로, input값에 label을 할당한다. ( … ) 예측 결과가 연속적인지 혹은 불연속적인지 여부에 따라 classification model과 regression model로 나눌 수 있다. (…)     Linear Regression은 그 중 예측 결과가 연속적인 regression model에 해당한다.    Linear Regression이란?      주어진 input x에 대하여 target으로 하는 y를 예측하는 supervised learning의 일종이다. 이때 x와 y간에 linear한 관계를 갖는다는 점이 특징이다.   학습 과정   Linear Regression 모델에 대하여 machine이 학습하는 과정을 살펴본다. 이전에 언급했듯, 주어진 dataset에 대하여 train, validation, test용으로 나누었다고 가정한다.   Training      train data를 이용하여 모델을 학습시키는 과정이다. 위 그림에서 x는 input data, y는 labels(ground truth)를 의미한다. 먼저 주어진 n개의 samples data(x,y)에 대하여 학습을 통해 model parameters를 결정한다. linear regression model에서 학습할 때 사용되는 모델은 아래와 같다. \\(\\hat{y_i}=θ_0+\\sum_{j=1}^{d}{x_{ij}θ_j}\\\\ x_{ij}: input\\ data, features\\\\ θ_j: weights\\ (i.e. model\\ parameters)\\\\ d: input\\ dimension\\) 앞서 machine learning은 feature extraction 과정이 개발자에 의해 수행된다고 언급했다. 즉 주어진 input data에 대하여 개발자가 j개의 features를 뽑아 j차원 input data를 만든다. 이를 같은 차원을 갖는 weights와 곱한 결과가 바로 예측 결과가 된다.   예를 들어, 빌딩의 온도(y)를 예측하는 모델을 만든다고 가정하자. 개발자는 빌딩의 온도를 결정하는 feature(요소)로 외부 온도(x1), 습도(x2), 사람의 수(x3), 태양 노출 정도(x4)를 결정하였다. 이것은 input data이자 features에 해당한다. 이런 input data x1, x2, x3, x4에 대하여 같은 차원(1×4)을 갖는 model paramters θn (n=1,2,3,4)를 결정한다. 개발자는 x와 y간에 linear한 관계가 있다고 가정하여 linear regression model을 사용할 수 있다.      한편, bias(θ0)를 통해 input data에 대한 모델의 의존성을 줄여줄 필요가 있다. 예를 들어, bias가 없는 경우 model은 항상 (0,0) 지점을 지나게 된다. 이러한 경우를 방지하기 위해 linear  model에서는 항상 bias term을 두어 input data에 대한 의존성을 줄여준다.   Testing      이후 validation data를 이용하여 모델을 예측한다. 학습시 사용되지 않은 새로운 input data x_{n+1}과 위 과정을 통해 결정된 model parameters를 통해 x_{n+1}에 대한 label y_{n+1}을 예측한다.      위 이미지에서 y_{n+1} 위에 “^(hat)” 모양이 있는 것을 확인할 수 있다. 이것은 해당 변수가 실제 real value가 아닌 estimation value(예측값)임을 보여주는 수학적 기호이자 약속이다.    Loss Function   이렇게 예측한 estimation y값은 loss function에 적용된다. Loss Function이란 나의 예측값(estimation value)와 실제값(labels, ground truth)을 비교하여 예측값이 얼마나 정확한지를 판단하는 지표이다. 예를 들어 loss function으로 euclidean distance(유클리디안 거리공식)을 이용할 수 있다. \\(J(θ)=\\frac{1}{n}\\sum_{i=1}^{n}{(\\hat{y_i}-{y_i})^2}\\\\ J(θ): 주어진\\ model\\ parameter\\ θ에서의\\ loss\\\\ \\hat{y_i}: estimation\\ value\\\\ y_i: labels(=ground truth)\\\\ n: number\\ of\\ samples\\)   Optimization   이러한 loss function 결과를 통해 model paramters를 조정하여 더 나은 학습 결과를 도출할 수 있다. 이때 loss function 결과를 통해 model parameters를 조정하는 과정을 optimization이라고 한다. 예를 들어 loss function에 linear least squares 를 적용하여 optimization과정을 수행할 수 있다. \\(\\min_θ{J(θ)}=\\frac{1}{n}\\sum_{i=1}^{n}{(\\hat{y_i}-{y_i})^2}\\) 한편, 주어진 문제가 convex problem인 경우에는 유일한 closed-form solution을 얻을 수 있다.      Convex란?     convex는 한국말로 “볼록한”이라는 뜻이다. ML분야에서 convex problem는 아래로 볼록한 모델(가령 y=x^2)을 떠올리면 된다. 이 경우 minimum value가 유일하므로 ML하기 가장 좋은 경우라고 할 수 있다.    총 정리   위에서 언급한 Training-Testing-Loss Function-Optimization의 과정을 하나의 그림으로 표현하면 다음과 같다.      Nearest Neighbors (최근접 이웃)   Nearest Neighbors는 supervised learning 관련된 알고리즘의 일종으로, 가장 고전적이며 직관적인 알고리즘이다. nearest neighbors 알고리즘은 데이터 간의 거리를 통해 입력 데이터는 이와 가장 가까운 데이터와 같은 label을 갖는다고 판단한다.      nearest neighbor의 경우 학습 과정은 다음과 같다. 먼저 모든 training data와 labels를 memory에 저장한다. 이후 새로운 데이터(validation data)가 들어왔을 때 이와 모든 training data를 비교하여 가장 거리가 짧은 label을 estimation value로 도출한다.      한편, 가장 가까운 점 하나로만 예측하자니 outlier가 있는 경우 예측률이 떨어진다. 따라서 K개의 가까운 점을 통해 결과를 도출하는 nearest neighbor인 K-nearest neighbors을 사용하기도 한다.   Distance Metric   image data는 행과 열을 갖는 metric형태이다. 이러한 metric 형태의 데이터에 대하여 data간의 거리를 판별하는 방법을 distance metric이라고 한다. 두 data간 거리를 구하는 방식은 다양한데, 대표적인 예로 L1과 L2 distance가 있다.   L1 (Manhattan) distance   L1 distance를 수식으로 표현하면 아래와 같다. \\(d_1(I_1,I_2)=\\sum_{p}{|I_1^p-I_2^p|}\\\\ I: image\\ metric\\\\ p: pixel\\\\ d: distance\\)    L1 distance의 입력(I)와 출력(d)간 관계를 그림으로 표현한 것은 아래와 같다.      L2 (Euclidean) distance   \\[d_2(I_1,I_2)=\\sqrt{\\sum_{p}{(I_1^p-I_2^p)^2}}\\]  L2 distance의 입력(I)와 출력(d)간 관계를 그림으로 표현한 것은 아래와 같다.      L1 distance vs. L2 distance   예를 들어, 1-Nearest Neighbors 알고리즘에 대하여 L1, L2 distance를 적용한 경우 결과는 아래와 같다.      사실상 결과에 있어서 큰 차이는 없다.      하지만 일반적인 경우 L2 distance를 더 많이 사용한다.    Hyperparameters   nearest neighbors알고리즘에서의 hyperparameters는 무엇일까?      까먹었을까봐 다시 언급     model parameters는 기계가 학습할 수 있는 parameter를 의미하는 반면 hyperparamters는 사람이 직접 입력해주는 parameter를 의미한다.    nearest neighbors 알고리즘에서 몇 개의 neighbors에 대해 거리를 조사할 지(k), 어떤 distance 공식을 사용할지(d1 or d2?)등이 바로 hyperparameters에 해당한다. 사람이 정해주는 이러한 hyperparameters에 따라 모델의 성능이 크게 달라질 수 있음에 유의해야 한다.   Speed Problem   nearest neighbors는 속도 측면에서의 치명적인 단점으로 인해 image classification 문제에서 잘 사용되지 않는다. 예를 들어 N개의 examples에 대해 nearest neighbors 알고리즘을 사용하는 경우를 생각해보자. nearest neighbors의 training은 단순 memory 저장밖에 없다. 따라서 O(1)의 시간이 소요된다. 반면 predict과정에서는 저장된 N samples에 대하여 일일히 비교해야 하므로 O(N)의 시간이 소요된다. ML에서는 학습시에 걸리는 시간보다는 실제 예측을 하는데에 걸리는 시간을 최대한 줄이는 것이 주요 목적이다. 따라서 이러한 관점에서 봤을 때 nearest neighbors 알고리즘은 실용적이지 않다.   Semantic Gap Problem   Image classification 문제는 computer vision분야의 핵심 과제이다. nearest neighbors 알고리즘이 image classification 문제에 적용되는 경우 semantic gap 문제가 발생할 수 있다.   사람이 볼 수 있는 image의 형태와 computer가 인식할 수 있는 image의 형태에는 차이가 있는데, 이러한 차이를 semantic gap이라고 한다. 예를 들어 아래와 같이 사람이 봤을 때의 고양이 사진은 컴퓨터 입장에서는 0~255사이의 숫자로 표현되는 8-bit unsigned integer로 인식된다.      예를 들어, RGB 이미지 포맷에서 각 pixel의 값은 red, green, blue channel에 대한 밝기 값을 의미하는데, 이러한 밝기값을 feature로 두고 nearest neighbors 알고리즘으로 학습시킨다면 semantic gap으로 인해 다음의 경우에서 제대로된 결과를 도출할 수 없다.           Viewpoint variation       같은 물체를 보더라도 카메라를 살짝 이동한 경우(전체 pixel값의 변화)            Background Clutter              물체가 배경과 비슷한 색을 가진 경우            Illumination              이미지에서 그림자나 역광이 발생하는 경우            Occlusion              물체가 어떤 장애물에 의해 일부가 가려진 경우            Intra-class variation              같은 label(“cat”)을 가졌지만 색상이 다른 경우       위의 경우는 모두 단순 nearest neighbors 알고리즘으로 밝기값을 비교하는 것 만으로는 판별이 불가능하다.   결론   위에서 언급하였듯이, pixel distance를 이용한 k-nearest neighbors은 매우 느린 test time 문제(speed problem)와 pixels에 대한 distance metrics의 효용성 문제(semantic gap problem)로 인해 image classification 문제에서는 사용되지 않는다. 아래 이미지는 여러 이미지에 대해 k-nearest neighbors 알고리즘을 사용한 결과이다.      이처럼 k-nearest neighbors은 전혀 다른 이미지(boxed, shifted, tinted)에 대해 original 과 같은 distance를 갖는다는 치명적인 문제가 있다.   Dimensionality Reduction/Expansion   단순하게 생각했을 때 input data의 차원이 높아질 수록(features의 개수가 많아질수록) 모델의 성능이 더 좋아지지 않을까 생각할 수 있다. 하지만 결과적으로 봤을 때 input data의 차원이 높아질수록 원하는 결과 도출을 위해서 더 많은 데이터가 필요해진다. 이러한 개념은 curse of dimensionality로 설명할 수 있다.   Curse of dimensionality (차원의 저주)   Curse of dimensionality는 데이터 학습을 위한 차원이 증가함에 따라, 해당 차원을 표현할 수 있는 데이터가 요구되어 더 많은 학습 데이터를 필요로하는 현상을 말한다. 예를 들어, 4개의 data points가 있을 때 예측이 가능하다고 가정해보자.      1차원인 경우 4개의 data points만으로 충분히 예측가능하다. 하지만 2차원으로 확장됨에 따라 최소 4^2개의 data points가 필요하게 되고, 차원이 확장됨에 따라 요구되는 data points의 개수가 늘어남을 자연스럽게 연상할 수 있다. 이러한 curse of dimensionality를 그래프로 표현하면 다음과 같다.      x축은 dimensionality(features의 개수)를, y축은 classifier performance(성능)을 나타낸다. 초기에는 차원이 증가할 수록 성능이 향상되는듯 보이지만, 어느 지점을 넘어서게 되면 input data 양의 한계로인해 오히려 성능이 떨어짐을 확인할 수 있다.   이러한 curse of dimensionality를 고려하여 모델을 설계하기 위해 다양한 차원 축소/확장 방법이 제시되었다.   Dimensionality Reduction   학습데이터가 한정되어있을 때 차원을 줄일 목적으로 다양한 방법이 제안되었다.   PCA (Principal Component Analysis)       Principal Component Analysis(PCA)는 분산을 최대한 보존하며 서로 직교하는 새로운 기저를 찾아 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변형하는 기법이다. 이때 분산을 최대한 보존한다는 말의 뜻은, 데이터의 변동성이 가장 큰 방향으로 축을 설정하여 데이터를 표현한다는 의미이다.   LDA (Linear Discriminant Analysis)      Linear Discriminant Analysis(LDA)는 PCA보다 분류에 최적화된 차원축소 기법이다. class가 최대한으로 분류되도록 축을 분리한다는 특징이 있다.   t-SNE (Stochastic Neighbor Embedding)      Stochastic Neighbor Embedding(SNE)은 고차원의 원 공간에 존재하는 데이터 x의 이웃간 거리를 최대한 보존하는 방법이다. 즉, 확률적으로 가장 유사한 축으로 분리한다는 특징이 있다.   Dimensionality Expansion   한편, 입력 데이터가 충분한 경우, 차원 확장을 통해 결과를 더 개선시킬 수 있다.   Linear Classifier   앞서 supervised learning의 일종인 linear regression에 대해서 알아보았다. 이번에는 불연속적인 예측결과를 다루는 classification model 중 가장 대표적인 linear classification에 대해서 알아본다.      까먹었을까봐 다시 언급     supervised learning은  target value가 존재하는 학습 방식으로, input값에 label을 할당한다. ( … ) 예측 결과가 연속적인지 혹은 불연속적인지 여부에 따라 classification model과 regression model로 나눌 수 있다. (…)     Linear classification은 그 중 예측 결과가 불연속적인 classification model에 해당한다.    Linear Classification이란?      주어진 input x에 대하여 target으로 하는 y를 예측하는 supervised learning의 일종이다. 이때 x와 y간에 linear한 관계를 갖는다는 점이 특징이다. linear regression과 다른 점은 출력이 연속적인 value가 아닌 label의 형태(가령 “cat”, “dog”, “ship”)로 나타난다는 점이다.   Linear Classifier   value가 아닌 class를 어떻게 기계적으로 표현할 수 있을까? linear classifier는 각 class별로 score를 두어 가장 높은 score가 estimation value가 되게끔하여 불연속적인 결과값을 도출한다. 예를 들어 아래 이미지는 input image x에 대해 고양이인지, 강아지인지, 배인지를 구분하는 linear classifier를 보여준다.      가령 cat score=1000, dog score=200, ship score=100인 경우 classifier는 결과로 “cat”을 도출한다. 이 경우 학습이 잘 된 모델이라 볼 수 있다. 반면 cat score=100, dog score=800, ship score=300인 경우에는 결과가 “dog”이므로 예측이 틀린 경우라 할 수 있다.   학습 과정   Linear Classification 모델에 대하여 machine이 학습하는 과정을 살펴본다. 이전에 언급했듯, 주어진 dataset에 대하여 train, validation, test용으로 나누었다고 가정한다.   Feed-forward   input image가 가령 2×2의 size를 갖는다고 할 때, 이를 flatten하여(길게 펴서) 4×1의 input data로 만들 수 있다. 결과로 3개의 class에 대한 scores를 도출해야 하므로 weights는 3×4의 사이즈를 갖게 된다. linear 공식을 이용하여 아래와 같이 각 class별 scores를 도출한다. \\(f(X,W)=XW+b\\\\ X: input\\ data\\\\ W: model\\ parameters(weights)\\\\ b: biase\\\\ f: scores\\)   Loss Function   feed-forward의 결과(f, scores)와 target scores를 비교하는 loss function을 이용하여 모델의 성능을 평가한다. 이때 target scores는 정답인 class의 score값이 최대인 scores를 의미한다.   Back Propagation   Loss function의 결과를 바탕으로 모델을 최적화(optimization)하기 위해 back propagation을 수행한다. 이러한 과정으로 얻어진 결과를 weights에 update하여 모델에 반영한다.   총 정리   위에서 언급한 Feed-forward - Loss Function - Back Propagation의 과정을 하나의 그림으로 표현하면 다음과 같다.      Linear Classifier의 한계   Linear classifier는 입력과 출력이 linear관계를 갖는 모델에 한해서만 잘 동작한다. 가령 아래의 경우에 대해서는 linear classifier만으로 판별이 불가능하다.      따라서 Nonlinearity한 관계를 갖는 문제를 해결할 수 있는 모델이 필요했다. nonlinearity는 layer 뒤에 activation function을 추가함에 따라 달성되는데 이와 관련해서는 이후에 좀 더 자세히 살펴보도록 한다.   SVM Classifier   Support Vector Machine  Classifier (SVM classifier) 는 주어진 dataset을 바탕으로, 새로운 data가 어떤 class에 해당하는지를 판단하는 비확률적 binary linear classification model을 만든다. SVM은 linear classification과 더불어 nonlinear classification에서도 사용될 수 있다. 아래의 예시를 통해 SVM classifier에 대해 자세히 알아보자. f(x,w)을 통해 각각의 고양이, 자동차, 개구리 이미지에 대해서 위와 같은 scores를 도출했다고 가정하자.      일반적으로 multiclass에 대한 total loss는 아래와 같이 개별 loss의 평균을 통해 계산할 수 있다. \\(L=\\frac{1}{N}\\sum_i{L_i(f(x_i,W),y_i)}\\\\ L: total\\ loss\\\\ N: number\\ of\\ samples\\\\ L_i: the\\ loss\\ between\\ f(x_i,W)\\ and\\ y_i\\) 이때 SVM Classifier는 아래와 같은 loss function을 활용한다. \\(L_i=\\sum_{j≠y_i}\\max{(0, w_j^Tx_i-w_{y_i}^Tx_i+△)}\\\\\\) 가령, 고양이 이미지에 대한 loss를 구한다고 할 때, 아래와 같이 계산될 수 있다. \\(L_1=\\sum_{j≠y_i}{\\max{(0, w_j^Tx_1-w_{y_1}^Tx_1+△)}}\\\\ =\\max{(0, 5.1-3.2+△)}+\\max{(0, -1.7-3.2+△)}\\\\ =\\max{(0, 1.9+△)}+\\max{(0, -4.9+△)}\\\\\\) 이때 △(delat)=1이라 하면 아래의 결과를 얻는다. \\(L_1=\\max{(0, 1.9+1)}+\\max{(0, -4.9+1)}\\\\ =\\max{(0, 2.9)}+\\max{(0, -3.9)}\\\\ =2.9+0=2.9\\) 따라서 고양이 이미지에 대한 loss는 2.9가 된다. 같은 방식으로 자동차 이미지, 개구리 이미지에 대한 loss를 구하면 각각 0과 12.9가 된다.      따라서 이때의 총 loss L은 (2.9 + 0 + 12.9)/3 = 5.27이다.   Hinge  Loss   SVM loss 공식은 아래와 같다. \\(L_i=\\sum_{j≠y_i}\\max{(0, w_j^Tx_i-w_{y_i}^Tx_i+△)}\\) 이때 delta(△)값에 따라 loss function 수행 결과가 달라진다. 아래의 그림은 delta=1인 경우, delta와 loss간의 상관관계를 잘 보여준다.      delta값은 얼마나 더 엄격한 모델을 만들 것인가와 관련되어 있다. 가령, 고양이 이미지에 대해 cat score=3.2, car score=3.2인 경우 delta=0이면 loss=0이 된다. 하지만 우리의 목적은 고양이 이미지에 대해 확실하게 “cat”이라 분류하는 것이므로 delta값을 두어 cat score와 other scores간에 확실한 차이가 있는 경우에만 loss=0(정답)이라고 판별하는 것이다.   Example Code   def L_i_vectorized(x, y, W):     scores = W.dot(x) # First calculate scores     margins = np.maximum(0, scores - scores[y] + 1) # Then calculate the margins s_{j} - s_{y_i} + 1     margins[y] = 0 # only sum j is not y_i, so when j=y_i, set to zero.     loss_i = np.sum(margins) # sum across all j     return loss_i   Softmax Classifier   scores기반 SVM Classifier은 결과값(score)의 범위에 제한이 없다는 단점이 있었다. 따라서 score가 음수일 수도 있고 아주 큰 값일 수도 있다. 이러한 단점을 개선하여 0과 1사이의 확률값으로 scores를 표현한 것이 바로 softmax classifier이다. 아래의 예시를 통해 자세히 살펴보자.      계산 과정           Unnormalized log-probabilities / logits       위와 같이 f(X,W)에 의해 결과값(scores)가 도출되었을 때 이를 unnormalized log-probabilities라고 한다.            unnormalized probabilities \\(exp(s=f(x_i,W))\\) ​\t\t먼저 scores에 exp()를 취하여 음수값을 갖지 않도록 한다.            Probabilities \\(P(Y=k|X=x_i)=\\frac{\\exp^s{k}}{\\sum_j{\\exp^s{j}}}\\) 이후 unnormalized probabilities를 normalize하여 총 합이 1인 일종의 확률값으로 표현한다.       최종적으로 softmax classifier는 아래의 loss function을 갖는다고 말할 수 있다. \\(L_i = -\\log{P(Y=y_i|X=x_i)}=-\\log{\\frac{\\exp^s{k}}{\\sum_j{\\exp^s{j}}}}\\) 위 예제의 경우 고양이 이미지에 대한 cat, car, frog의 loss를 계산하면 아래와 같다. \\(L_1=-\\log{\\frac{\\exp{3.2}}{\\exp{3.2}+\\exp{5.1}+\\exp{-1.7}}}\\\\ =-\\log{\\frac{24.5}{24.5+164.0+0.18}}\\\\ =-\\log{0.13}=2.04\\) 이때 주의할 점은, 정답(\"Cat\")이 아닌 다른 class에 대한 probabilities는 전혀 고려하지 않는다는 점이다. 이미 softmax의 loss function 자체에 other class가 고려되어있으므로 모든 class에 대한 loss의 평균값을 계산하여 total loss를 구할 필요 없이, 단순히 정답 class에 대한 loss만을 구하면 그것이 total loss가 된다는 뜻이다.   ","categories": ["ai-deepLearning"],
        "tags": ["Deep Learning","AIAS"],
        "url": "/ai-deeplearning/DL01/",
        "teaser": null
      },{
        "title": "[TechBlog-01] Jekyll 블로그 개설하기",
        "excerpt":"    배경   기술블로그란 개발에 대한 내용이 중점인 블로그로, 당근마켓, 카카오, 쿠팡 등 여러 기업에서 최신 기술에 대한 정보를 제공하고, 기술적 역량을 어필하여 더 좋은 개발자를 채용하기 위한 목적에서 운영중이다. 기업뿐만 아니라 여러 개발자들도 학습을 정리하거나 기록하기 위한 목적으로 운영하고 있는데, 나 역시 이러한 목적에서 기술블로그를 시작하고자 한다. 이번 포스팅에서는 몇 가지 블로그 플랫폼에 대해 소개하고, 그 중에서 내가 사용할 GitHub page에 대한 상세한 사용방법을 소개한다.         블로그 플랫폼   기술블로그를 시작할 때 사용할 수 있는 몇가지 블로그 플랫폼은 다음과 같다. 각 블로그 플랫폼이 가진 특징 및 장점과 단점을 비교해본다.   플랫폼 비교   네이버 블로그   네이버 블로그는 개인적으로 너무너무 제약이 많고 그래서 불편하다고 생각한다. 기술블로그용 플랫폼으로는 적합하지 않은 느낌..   장점      개설과 블로그 디자인이 직관적이므로 일반인 사용자도 쉽게 사용할 수 있다.   단점      파워블로그와 같이 상업적 성격을 띠는 블로그가 많아짐에 따라 네이버 검색 순위에서 밀리는 경우가 많아졌다.   커스터마이징에 한계가 있다.   관련 블로그      티몬 개발 블로그 : https://blog.naver.com/tmondev   네이버 블로그 바로가기       티스토리(Tistory)   티스토리를 기술 블로그로 활용하는 분들도 많고, 적절히 커스텀할 수 도 있어서 나쁘지 않다. 다만 에디터보단 마크다운 문법이 더 익숙하다면 비추한다 (적용된 모습을 확인하면서 작업할 수 없기때문에 불편하다). 나 역시 에디터가 불편하다고 느껴서 티스토리를 잠깐 사용하다가 말았다.   장점      개설과 블로그 디자인이 쉽고 네이버블로그보다 디자인이 심플하다.   커스터마이징이 쉬워 여러 API를 적용할 수 있다. HTML/CSS 수정이 가능하다.   비밀글 기능이 있어서 민감한 내용을 포함한 글은 숨길 수 있다.   단점      글쓰기 도구에 한계가 있다. (글 편집이 너무 어렵다😢)   관련 블로그      카카오 엔터프라이즈 기술블로그 : https://tech-kakaoenterprise.tistory.com/   티스토리 바로가기       Medium   최근에 알게 된 블로그 플랫폼이다. 사용해보진 않았지만, 개발블로그로서 필요한 기능은 그래도 갖추고 있는 것 같다. 메디움은 인터페이스가 굉장히 깔끔한데 이게 장점이자 단점이다. 현재 많은 기업들이 이 플랫폼을 사용하는 걸로 봐서 괜찮은 선택지 중 하나라고 생각한다.   장점      Publication 기능이 있어서, 어딘가에 소속되어 글을 쓸 수 있다.   인터페이스가 굉장히 깔끔하다.   단점      코드 하이라이팅 기능이 없다. (GitHub Gist로 해결할 순 있을 것 같다.)   카테고리 기능이 없다.   관련 블로그      Coupang Tech : https://medium.com/coupang-tech   Airbnb Tech : https://medium.com/airbnb-engineering   Netflix Tech : https://netflixtechblog.com/   Google Play : https://medium.com/googleplaydev   당근마켓 : https://medium.com/daangn   Medium 바로가기       GitHub Page   내가 활용하고 있는 블로그 플랫폼이다. 초반에 환경 구축하는 게 조금 까다롭긴 하지만 한 번 익숙해지면 이것만큼 편한 것도 없다고 생각한다. 나는 로컬PC에서 Typo 프로그램으로 글을 작성 및 관리하고, Visual Studio를 이용하여 GitHub에 업로드하고있다. 물론 애초에 블로그를 위한 서비스는 아니기때문에 불편한 점도 꽤 있긴 하다.   장점      커스터마이징이 매우 자유롭다.   단점      일반인이 입문하기 어려우며 프로그래밍 지식이 부족하다면 기능이 매우 제한된다.   이미지를 일일이 GitHub에 push해주어야 한다. (폴더에 불필요한 이미지가 포함되면 일일이 삭제해주어야하므로 약간 불편하다.)   블로그에 커스텀 및 글 업로드 반응이 느리다.   관련 블로그      우아한형제들 기술 블로그 : https://techblog.woowahan.com/   spoqa 기술 블로그 : https://spoqa.github.io/   마켓컬리 기술 블로그 : https://helloworld.kurly.com/   올리브영 기술 블로그 : https://tech.oliveyoung.co.kr/   Github Page 바로가기         GitHub Page   나는 글이 길어져도 글의 전체 구조를 쉽게 파악할 수 있도록 table of contents 기능이 너무 갖고싶었으나 네이버 블로그는 지원하지 않았다. Tistory는 HTML/CSS 편집을 통해 해당 기능을 넣을 수 있었으나, 글편집기는 너무 불편하다고 느껴졌다 ( 코드블록 하이라이트를 하려면 별도로 플러그인을 설치해주어야 한다. 또한 에디터가 사용하기에 너무 불편하다.). 따라서 자유로운 커스터마이징이 가능한 GitHub Page를 이용하여 블로그를 개설하였다.   Fig 1. minimal-mistakes의 table of content 이미지. 오른쪽 리스트와 같이 본문에 붙어서 독자가 읽는 위치를 알려주는 기능 및 바로가기 기능을 한다.   네이버 &lt; 티스토리 &lt; 깃헙페이지 순으로 커스터마이징이 자유롭다. 티스토리도 웬만한 커스텀은 가능하므로 좀 더 직관적인 글쓰기를 하고싶다면 티스토리도 추천한다.      GitHub Pages는 단순히 파일들을 호스팅하여 사이트를 생성해주는 역할만을 한다. 직접 zero부터 시작해서 웹페이지를 생성하여 GitHub Page에 호스팅할 수도 있지만 각종 에러에 시달려야 할 수도 있다. 따라서 나는 Jekyll이라는 정적 사이트 생성기를 이용하여 블로그를 개설할 것이다.    직접 처음부터 끝까지 웹페이지를 만드는 것은 매우 비효율적이다. 웹개발에 뜻이 있다면 말리지는 않겠지만, 웹개발 고수가 아니라면 차라리 Jekyll과 같이 테마를 다운받아 시작해서 전체적인 사이트 구조를 파악하고 하나씩 커스텀해 나가는 방식을 추천한다.      간단한 블로그 개설 튜토리얼   본 튜토리얼은 Windows 개발환경에서 진행되었음을 알린다.   1. 개발 환경세팅           Git 설치       git 공식 사이트에 접속하여 본인 개발환경에 맞게 git을 다운받는다. 필자는 Widnows용 설치파일을 다운로드받았다. 기본 세팅 그대로 Next 버튼을 클릭하여 설치를 완료한다.            Ruby 설치       Ruby 설치 페이지에 접속하여 본인 개발환경에 맞게 Ruby를 다운받는다. 필자는 Windows환경이므로 RubyInstaller를 사용하여 WITH DEVKIT 버전을 다운로드하였다. 기본 세팅 그대로 설치를 완료하면 된다.       끝까지 Next를 클릭하면 아래와 같이 커맨드창이 나오는데, 3을 입력하여 설치를 마치며 된다.       Fig 2. RubyInstaller2 예시 이미지.       설치 완료 후, 설치된 Start Command Prompt with Ruby 프로그램을 실행한 뒤 명령창에 다음과 같이 입력하여 인코딩을 부여해준다. 명령어 오류가 발생하지 않도록 해주는 명령어이다.       chcp 65001           이후 아래 명령어를 통해 미리 jekyll과 bundler를 다운로드받아주었다.       gem install jekyll gem install bundler                Visual Studio Code 설치       Visual Studio Code 홈페이지에서 본인 개발환경에 맞게 VSCode를 다운받는다.             2. Jekyll 환경 구축           GitHub pages 생성       먼저 GitHub에 접속하고 로그인한다. Create New repository를 클릭하여 Repository name값을 본인이름.github.io로 세팅하여 레포지토리를 생성한다.        Fig 3. Repository name 예시 이미지. 나는 기존에 같은 이름의 repository가 존재하므로 이미 존재한다는 경고창이 뜬다.            Git clone       나는 VSCode terminal 환경에서 작업하기위해 아래와 같이 VScode를 실행하고 Git Bash 터미널을 열어주었다.       Fig 4. VScode에서 Git Bash terminal을 여는 방법 이미지.       다음의 명령어를 통해 본인이 생성한 repository를 로컬PC로 clone한다.       git init git clone \"https://github.com/dazory/dazory.github.io\"                Jekyll 설치              Jekyll 홈페이지에 접속하면 여러 정적 사이트 테마를 확인할 수 있다. 필자는 가장 심플한 디자인인 minimal-mistakes을 사용하기로 하였다.                   안정성이 좋은 release 버전으로 다운받기로 한다.            해당 테마를 fork한 뒤, 압축을 풀고 위에서 생성했던 repository에 옮겨준다.       이후 아래와 같이 불필요한 파일을 삭제해준다.              .editorconfig       .gitattributes       .github : github관련 항목을 내부에 배치하는 데에 사용되는 컨벤션 폴더       /docs : demo를 위한 파일이 들어있음       /test : test를 위한 파일이 들어있음. test를 하고싶다면 http://localhost:4000/test/에 접속하면 됨       CHANGELOG.md  : version이 업데이트되면서 수정된 내용이 담긴 마크다운 파일       README.md : 설명서       screenshot-layouts.png : README.md 파일에 이미지를 추가하기 위한 파일일 뿐       screenshot.png : README.md 파일에 이미지를 추가하기 위한 파일일 뿐           3. Customize   블로그 초기 세팅   여기를 참고하여 초기 세팅을 할 수 있다.       디렉토리 설명   주요 디렉토리 구조는 다음과 같다.           _data       데이터를 저장하는 것과 관련된 폴더이다. 기술블로그인 경우, 멤버들의 정보를 저장하기도 하고,  navigation 등을 위해 사용되기도 한다. 여기에 저장된 데이터를 필요로하는 페이지에서 불러와 사용된다. 어떤 데이터를 여러 페이지에서 사용하고싶고, 이후에 수정을 한 곳에서 하고싶은 경우에 활용할 수 있다.       (사용예시)                       /_data/members.yml (출처: 올리브영 기술블로그)                      이런식으로 data를 저장하면, 아래와 같이 다양한 페이지에서 불러와 사용할 수 있다.                                   개발자 소개 페이지                                                   포스팅                                                           /_data/navigation.yml                      위와 같이 저장된 navigation정보는 아래와 같이 메인 페이지에서 불러올 수 있다.                                           _includes       페이지 렌더링이나 여러 보이지 않는 기능들과 관련된 폴더이다. 주로 전반적인 페이지 모두에게 적용되는 코드들을 넣어서 관리한다.       (사용예시)                       /_includes/head.html           head.html은 모든 페이지의 head 태그 부분에 들어가는 코드를 담는다. 아래와 같이 에서 .css파일을 link하여 페이지를 렌더링할 수 있다.                                   /_includes/nav_list_main, /_includes/nav_list                      블로그 왼쪽에 위와 같이 항상 붙어있는 카테고리 항목을 볼 수 있는데, 이 역시 _includes 폴더에 navigation list를 담아서 생성했다.                                 공부하는 식빵맘 블로그를 참고하여 만들었다.                                _layouts       포스팅의 레이아웃을 관리하는 폴더이다. 다양한 레이아웃을 만들어놓고 상황에 따라 적용할 수 있다.       (사용예시)                       _layouts/post-with-comments.html           minimal-mistakes의 기본 layout인 archive.html을 조금 수정하여 댓글 기능이 들어가도록 만든 레이아웃이다.                      아래에서 더 자세히 설명할 거지만, _posts나 _pages에서 이 레이아웃들을 가져와 사용할 수 있다.                                           _pages       사이트의 각 페이지를 관리하는 폴더이다.       (사용예시)                       _pages/about.md                      위와 같은 about 페이지를 markdown언어로 작성하여 페이지를 구성할 수 있다.                                   _pages/404.md                      404 페이지 역시 여기에서 작성할 수 있다.                                _posts       포스팅하는 글을 관리하는 폴더이다. 아래 이미지와 같이 .md로 작성된 다양한 포스트들을 관리하고있다.              (사용예시)                       _posts/2021-08-08-cs-algorithm-algo03.md                      _posts의 .md파일들은 대략적으로 두 부분으로 구성된다. 위 이미지에서 1~20번 라인은 해당 페이지의 속성에 관한 코드이고, 21번 라인 이후부터는 글 내용에 관한 코드이다.           1~20번 라인을 통해 해당 글의 레이아웃, 제목, 날짜 등의 속성을 적용할 수 있다.                                _sass       minimal-mistakes 테마에 관한 .scss 파일들이 들어있다. 나는 무서워서 잘 건드리진 않는다.                    assets       .css나 .js 파일을 관리할 수 있다. main.scss나 _main.js는 건드리지 않는 것이 좋고, 수정이 필요하다면 custom.css나 custom.js라는 파일을 따로 만들어서 _includes/head.html에서 링크걸어 사용하는 것이 좋다.                    files       블로그에서 사용되는 이미지들을 저장하는 폴더로 사용중이다.              포스트별로 폴더를 만들어서 이미지를 저장하고, 포스트에서 불러와 사용한다. Typora의 기능중에 복사한 이미지의 경로를 환경설정할 수 있는데, 이를 이용하여 아래와 같이 typora에서 작성한 글에 관련된 이미지들을 편리하게 관리하고 있다.                  각종 기능 추가하기   공부하는 식빵맘 블로그에 가면 카테고리기능, 스크롤바 커스텀, 댓글기능 등 다양한 기능에 관한 설명을 확인할 수 있다.       Tips   1. markdown 편집   VScode에서 markdown을 편집하는건 너무 힘들다. 사용하기 좋은 몇 가지 markdown 편집기가 있다.           Notion       notion에서 글을 작성하면 기본적으로 markdown 기반으로 작성된다. 이후 [Export]-markdown을 하여 .md파일을 생성할 수 있다. 나는 간혹 레이아웃이 깨지는 경우가 있어서 잘 사용하지 않는다.            Typora       이 글 역시 Typora를 이용하여 편집되었다. notion은 지원하는 여러 기능이 추가로 있어서 export하여 그대로 git에 업로드하면 깨지는 경우가 있었다. 하지만 typora는 지원하는 기능이 정말 markdown으로 한정되어있어 깨질 염려가 없다.       2. 로컬에서 jekyll blog 확인해보기   매번 Github에 호스팅한 뒤 잘 적용이 되었는지를 확인하는 건 매우 번거롭다. 또한 어떤 에러가 발생할지 모르기때문에 반드시 로컬에서 확인을 하고 호스팅해주어야 한다. 이때 사용할 수 있는 명령어는 다음과 같다.   jekyll serve --port 8001   8001말고 다른 port number를 사용해도 괜찮지만, 이미 사용중인 port number는 피하도록한다.   이후 localhost:8001에 접속하여 github pages에 호스팅된 결과를 시뮬레이션해볼 수 있다.           References           개발 환경세팅 : https://murra.tistory.com/160#%EB%AA%A9%EC%B0%A8            Jekyll 블로그 생성 및 git에 업로드 : https://2ssue.github.io/blog/make_jekyll_blog/            블로그 초기환경 설정 : https://syki66.github.io/blog/2020/04/12/minimal-mistakes-theme.html            _post폴더에 글 등록 : https://devinlife.com/howto%20github%20pages/first-post/            notion에서 export markdown : https://swieeft.github.io/2020/03/02/NotionToGithubioPorting.html            ","categories": ["etc-blog"],
        "tags": ["blog"],
        "url": "/etc-blog/TechBlog01/",
        "teaser": null
      },{
        "title": "[DL-02] Lect3. Neural Networks (part1)",
        "excerpt":"  Traditional ML Approaches   이전 시간을 통해 우리는 pixel 단위(밝기)로 학습하면 안되는 이유에 대해서 알아보았다.      까먹었을까봐 다시 언급     사람이 볼 수 있는 image의 형태와 computer가 인식할 수 있는 image의 형태에는 차이가 있는데, 이러한 차이를 semantic gap이라고 한다. 예를 들어 아래와 같이 사람이 봤을 때의 고양이 사진은 컴퓨터 입장에서는 0~255사이의 숫자로 표현되는 8-bit unsigned integer로 인식된다. (…) semantic gap으로 인해 다음의 경우에서 제대로된 결과를 도출할 수 없다.                 Viewpoint variation                  Background Clutter                  Illumination         …            사람과 컴퓨터간의 이미지를 인식하는 방법 차이(semantic gap)로 인해 pixel 단위의 접근 방법은 이미지 인식 분야에 맞지 않았다. 따라서 컴퓨터가 이해할 수 있는 feature vector의 형태로 바꾸는 과정이 필요하다. “Simple is the best”라는 철학 아래에서 봤을 때 feature vector는 복잡한 것 보다 구분이 쉬운 형태가 좋다. 아래 이미지는 feature vector의 좋은 예와 나쁜 예를 보여준다.      왼쪽 이미지의 경우 빨간점과 파란점을 linear classifier만으로 구분하기 어렵다는 한계가 있다. 따라서 이러한 feature vector를 어떠한 방법(f)으로 변형하여 오른쪽과 같이 linear classifier만으로 쉽게 판별 가능한 형태로 만드는 것이 feature extraction에서의 주요 목적이다.   Examples of Image Feature   이미지에서 feature로 뽑을 수 있는 형태에는 어떤 것이 있는지 알아보자.   Color Histogram      color histogram은 이미지에서 각 color가 얼마만큼의 비율을 차지하고 있는가를 보여주는 feature이다. 예를 들어 위 이미지는 “연두색 개구리가 초록색 풀 위에 앉아있는 이미지”이다. 이런 경우 초록계열의 컬러가 대부분이므로 히스토그램을 그렸을 때 초록 부분의 막대가 가장 높다. 이때 주의할 점은 color의 개수만 세면 안된다는 점이다. 입력 이미지는 크기가 제각각일 수 있으므로 color별 개수를 세고 나서 최종적으로 전체 pixel의 개수로 나눠주는 과정이 필수이다.   Histogram of Oriented Gradients (HoG)      histogram of oriented gradients(HoG)는 이미지상에서 밝기값이 급격히 변하는 정도인 gradient를 histogram으로 나타낸 것이다. 예를 들어 위 개구리 이미지의 경우, 잎사귀의 뚜렷한 경계 부분과 개구리의 경계 부분이 gradient로 잘 표시된 것을 확인할 수 있다.   Image Features   전통적인 ML 접근법 관점에서 바라봤을때 feature vectors는 다음과 같이 얻어진다.      먼저 이미지로부터 원하는 features(가령 color histogram, HoG 등)을 뽑아내고, 이를 이어붙인다. 이러한 과정을 concatenation(연쇄)라고 부른다.   Image Features vs. ConvNets   이전에 ML과 DL의 차이에 대해 간략히 소개한 적이 있다. ML에서는 개발자가 feature extraction의 과정을 거친 뒤 컴퓨터에게 학습을 시키는 반면, DL의 경우 feature extraction과정을 model parameters의 영역에 두어 컴퓨터에게 맡긴다. 그렇다면 전통적인 ML관점에서의 image features방식과 DL관점에서의 convolution networks 중에서 어느 방법이 더 좋은 것일까?   완전한 정답은 없다. ML의 image features방법은 데이터가 너무 작거나 한정적인 경우에는 DL보다 오히려 적합할 수 있다. 하지만 요즘은 데이터의 양이 워낙 방대하기때문에 대부분의 경우 DL의 convolution networks가 월등하게 더 좋은 성능을 낸다.   Introduction to Neural Networks (NN)   이번 페이지에서는 neural networks의 기본 개념에 대해 알아보는 시간을 갖는다.   Tensors   1차원 데이터를 우리는 “scalar”라고 부른다. 2차원 데이터는 “vector”혹은 “matrix”라고 부를 수 있다. 그렇다면 3차원 이상의 데이터는 어떻게 부를 수 있을까? 3차원 이상의 데이터는 “tensor”라고 표현한다. 아래 그림은 tensor의 차원에 대해 잘 보여준다.      일반적으로 3차원까지는 인간의 직감으로 충분히 상상할 수 있다. 하지만 차원이 확장됨에따라 직관적인 판단이 어려워진다. 차원에 대해 가장 쉽게 직감할 수 있는 예시는 바로 image이다. image는 3차원으로 표현이 가능하다(너비×높이×채널). 이러한 image를 시간에 따라 표현하면 video가 되는데, 이를 4차원이라 볼 수 있다.   Neural Networks (NN)   이전까지는 컴퓨터가 인간을 절대 이길 수 없다는 생각이 지배적이었다. 컴퓨터는 단순히 정해진 규칙에 따른 연산만을 수행하는 기계였기 때문이다. 하지만 인간의 사고를 모방하는 artificial neural networks(ANNs)의 개념이 등장하면서 컴퓨터의 영역은 단순 연산에서 더욱 확장되었다. artificial neural networks(ANNs)은 인간의 신경전달세포인 neuron의 동작을 모방한 network 구조이다. 줄여서 neural networks(NNs)라고 부르기도 한다.   이전까지는 ML의 가장 단순한 형태인 linear classifier에 대해서 배웠다. linear classifier는 입력과 출력간에 linear관계가 있음을 가정하고 아래의 식을 linear score function으로 둔다. \\(Linear\\ score\\ function:\\ f=Wx\\) 하지만 실제로는 linear한 문제보다 nonlinear한 문제가 더 많다. 따라서 nonlinear 문제를 풀 수 있는 방식으로 nonlinear function의 일종인 activation function이 등장했다. \\(2-layer\\ Neural\\ Network:\\ f=W_2\\max{(0, W_1x)}\\) 위 수식은 2개의 layers를 갖는 neural network의 function을 의미한다. 여기서 max()는 activation function의 일종으로 음수가 나오지 않게 해주는 역할을 하면서 nonlinearity를 부여한다. 이를 더욱 확장하여 3개의 layers를 갖는 NN에 적용하면 아래와 같은 꼴을 갖는다. \\(3-layer\\ Neural\\ Network:\\ f=W_3\\max{(0, W_2\\max{(0, W_1x)})}\\) 이때 3-layer NN에 activation functions이 두 개인 것을 확인할 수 있다. 왜 굳이 activation functions은 추가 layer 당 한 개씩 있어야 하는 걸까? 아래와 같은 형식은 안되는 걸까? \\(f=W_2W_1x\\) 답은 “안된다”이다. 위 수식은 겉 보기엔 2-layer NN처럼 보인다. 하지만 결국 아래의 과정에 의해 1-layer임을 알 수 있다. \\(f=W_2W_1x=W_3x\\ (∵W_3=W_2W_1)\\)   Activation Functions   Activation functions에 대해 자세히 알아보자. 앞서 NN은 인간의 neuron의 동작을 벤치마킹하여 등장한 network 구조임을 배웠다. activation function의 등장 배경을 이해하기에 앞서 neuron에 동작 방식을 먼저 이해해보자.   Neuron   깊게 들어가면 끝이 없으므로 여기서는 간단하게 뉴런의 동작 원리를 이해하는 정도로 넘어가겠다. 인간은 천억개의 뉴런을 가지며, 뉴런 다발을 통해 자극을 각 기관에 전달한다.                  ![Neurone 3D GIF       Gfycat](https://thumbs.gfycat.com/AdvancedSillyDogwoodtwigborer-size_restricted.gif)           이때 하나의 뉴런에서 자극이 일어난다고 가정하자. 뉴런은 자극이 특정 값 이상인 경우 축색돌기를 통해 바깥으로 신경전달물질을 내보내어 다른 뉴런을 또다시 자극한다. NN은 뉴런의 특정값 이상의 자극이 들어올 때 외부로 자극을 보내는 동작 방식에서 착안하였다. 아래 이미지는 NN의 동작 원리를 보여준다.      어떤 이미지가 입력되면(자극), 각 node(뉴런)에서 자극을 받고 자극이 특정값 이상이 되면 다음 node(뉴런)을 다시 자극한다. 이때 특정값 이상이 되면 출력을 내보내는 역할을 하는 함수가 필요한데, 이를 activation function이라고 한다.   Examples   activation function의 종류는 다양하다. 각 activation function별 특징에 대해서 살펴보자.   Sigmoid    \\(σ(x)=\\frac{1}{1+e^{-x}}\\) sigmoid 함수는 출력값이 0~1 사이로 표현되며, 입력값이 특정 범위보다 작으면 0(에 수렴), 크면 1(에 수렴)이 된다는 특징을 갖는다. 이러한 특징으로 인해 sigmoid함수는 다음의 문제를 갖는다.      입력의 절대값이 큰 경우 0또는 1로 출력값이 수렴하여 gradient를 소멸시키는 문제가 발생한다. (backpropagation을 다룰 때 자세히 설명한다.)   원점이 중심이 아니므로 sigmoid함수의 출력의 평균값은 0.5이다. 따라서 각 layer를 통과할 때마다 출력이 입력보다 커질 가능성이 높아지는 bias shift문제가 있다.   이러한 이유로 인해 요즘은 잘 쓰이지 않는 방식이다.   tanh    \\(tanh(x)\\) tanh 함수는 출력값이 -1~1 사이로 표현되며, 입력값이 특정 범위보다 작으면 -1, 크면 1이 된다는 특징을 갖는다. tanh()역시 sigmoid함수와 유사한 문제로 인해 요즘은 잘 쓰이지 않는 함수이다.   ReLU    \\(\\max{(0,x)}\\) ReLU 함수는 출력값이 0 이상으로 표현된다. 입력값이 음수인 경우 0을 출력하며, 음수가 아닌 경우는 입력 그대로를 출력한다. 음수가 아닌 입력에 대해서는 단순히 입력값을 그대로 출력하기 때문에 계산속도가 빠르기 때문에 요즘 가장 많이 사용되는 activation function이다. 하지만 ReLU함수 역시 이러한 특성으로 인해 다음의 문제가 있다.           dead ReLU       학습시, 일부 뉴런이 0만을 출력하여 이후 뉴런에서 전혀 활성화가 되지 않는 문제       이러한 dead ReLU문제를 해결하기 위해 ReLU함수의 다향한 변형 모델이 제시되었다.   Leaky ReLU    \\(\\max(αx,x)\\) Leaky ReLU는 ReLU함수의 변형 함수이다. 입력이 음수인 경우, 0를 출력하던 ReLU 함수와 달리 작은 weigth를 주어 음수 출력을 보다 작게 스케일링하여 출력한다. 이를 통해 음수 입력값에 대해서 항상 0을 출력함으로써 발생했던 dead ReLU문제가 어느정도 해소된다.   Maxout   \\[\\max(w_1^Tx + b_1, w_2^Tx+b_2)\\]  Maxout 역시 ReLU함수의 변형 모델이다.   ELU (Exponential Linear Unit)    \\(\\left\\{ \t\\begin{matrix} \tx\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ x≥0\\\\ \tα(e^x-1)\\ \\ \\ \\ \\ x&lt;0 \t\\end{matrix} \\right.\\) 입력값이 음수인 경우, 출력값의 평균이 0에 가까우므로 bias shift문제를 방지하여 gradient 소실 문제를 줄여준다 (일반적인 경우 α=1로 정의). 또한 음수 입력값에도 출력이 0은 아니므로 ReLU함수의 문제점인 dead ReLU가 발생하지도 않는다.      어떤 activation function을 사용할까?     CS231n 강의에 의하면 sigmoid함수는 사용하지 말고, 일반적인 경우 ReLU를 먼저 사용해 보고 그 다음으로 LeakyReLU나 ELU와 같이 ReLU Family를 사용할 것을 권장한다.    Neural Networks: Architectures   NN의 일반적인 구조는 다음과 같다.      위와 같은 구조를 3-layer Neural Network 혹은 2-hidden-layer Neural Network라고 부른다.   코드 구현   forward-pass of a 3-layer neural network   # forward-pass of a 3-layer neural network: f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid) x = np.random.randn(3,1) # random input vector of three numbers (3x1) h1 = f(np.dot(W1, x)+b1) # calculate first hidden layer activations (4x1) h2 = f(np.dot(W2,h1)+b2) #calculate second hidden layer activations (4x1) out = np.dot(W3, h2)+b3 # output neuron (1x1)   full implementation of training a 2-layer Neural Network   import numpy as np from numpy.random import randn  N, D_in, H, D_out = 64, 1000, 100, 10 x, y = randn(N, D_in), randn(N, D_out) w1, w2 = rand(D_in, H), randn(H, D_out)  for t in range(2000):     h = 1 / (1 + np.exp(-x.dot(w1)))     y_pred = h.dot(w2)     loss = np.square(y_pred - y).sum()     print(t, loss)          grad_y_pred = 2.0 * (y_pred - y)     grad_w2 = h.T.dot(grad_y_pred)     grad_h = grad_y_pred.dot(w2.T)     grad_w1 = x.T.dot(grad_h*h*(1-h))          w1 -= 1e-4 * grad_w1     w2 -= 1e-4 * grad_w2   Gradient Descent   이전 시간에 linear classifier에 대해 알아 보았다. linear classifier의 동작 원리는 아래 그림과 같이 요약할 수 있다.      feed-forward의 출력값(estimation scores)을 실제 target scores와 loss function을 통해 비교하여 모델의 성능을 평가한다. loss를 줄이는 방향으로 back propagationi이라는 과정을 통해 model parameters를 업데이트 하면서 모델을 학습 시킨다. 이때 loss를 줄이는 방향으로 모델을 학습시킬 수 있는 방법인 “back propagation“에 대해서 자세히 알아보자.   How to update the model parameters   Back propagation(optimization)을 하는 알고리즘에는 gradient descent(경사 하강법)가 있다. gradient는 기울기를 의미하는데, 기울기(gradient)가 내려가는 방향으로 model parameters를 update함으로써 optimization이 달성된다.   Strategy #1: Random search   model parameters가 random하게 업데이트되며 학습한다고 가정하자. 이에 대한 코드는 아래와 같다.   # assume X_train is the data where each column is an example (e.g. 3073 x 50,000) # assume Y_train are the labels (e.g. 1D array of 50,000) # assume the function L evaluates the loss function  bestloss = float(\"inf\") # Python assigns the highest possible float value for num in xrange(1000):     W = np.random.randn(10, 3073) * 0.0001 # generate random parameters     loss = L(X_train, Y_train, W) # got the loss over the entire training set     if loss &lt; bestloss: # keep track of the best solution         bestloss = loss         bestW = W     print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)      # prints: # in attempt 0 the loss was 9.401632, best 9.401632 # in attempt 1 the loss was 8.959668, best 8.959668 # in attempt 2 the loss was 9.044034, best 8.959668 # in attempt 3 the loss was 9.278948, best 8.959668 # in attempt 4 the loss was 8.857370, best 8.857370 # in attempt 5 the loss was 8.943151, best 8.857370 # in attempt 6 the loss was 8.605604, best 8.605604 # ... (trunctated: continues for 1000 lines )   위와 같은 학습과정을 거친 뒤 test set에 대하여 모델의 성능을 검증해보았다. 코드는 아래와 같다.   # Assume X_test is [3073 x 10000], Y_test [10000 x 1] scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples # find the index with max score in arch column (the predicted class) Yte_predict = np.argmax(scores, axis = 0) # and calculate accuracy (fraction of predictions that are correct) np.mean(Yte_predict == Yte) # returns 0.1555   결과적으로 random search 학습방법은 약 15.5%의 정확도를 달성하였다. 나쁘지는 않으나 좋지는 않는 성적이다.   Stategy #2: Follow the slope   두번째 전략으로 slope(경사)를 따라 내려가는 방법을 사용해본다. 1차원의 경우 기울기에 대한 수식은 아래와 같다. \\(\\frac{df(x)}{dx} = \\lim_{h→0}{\\frac{f(x+h)-f(x))}{h}}\\) 다중 차원의 경우, gradient는 각 차원에 대한 partial derivatives의 벡터가 될 것이다. 이를 수식으로 표현하면 아래와 같다. \\(x→▽f(x)= \\left( \t\\begin{matrix} \t\t\\frac{∂f(x)}{∂x_1}\\\\ \t\t\\frac{∂f(x)}{∂x_2}\\\\ \t\t...\\\\ \t\t\\frac{∂f(x)}{∂x_n}\\\\ \t\\end{matrix} \\right)\\) (gradient dW를 W에 업데이트하는 과정은 Lect3_55page를 참고)   이렇듯 random하게 model parameters를 학습하는 알고리즘보다는 gradient가 하강하는 방향으로 model parameters를 학습하는 것이 더 좋은 성능을 도출한다.   SGD ( Stochastic Gradient Descent)   위에서 gradient descent 알고리즘에 대해서 살펴 보았다. gradient descent 알고리즘은 loss function을 계산할 때 전체 training data을 사용하여 step을 결정한다. 이러한 방법은 step 방향에 대한 정확도는 높여주지만 한 step을 계산하는 데에 너무 많은 연산이 필요하다는 단점이 있다. 이러한 단점을 개선한 것이 바로 stochastic gradient descent알고리즘이다. stochastic은 “확률적” 이라는 뜻을 가졌는데, 이름에서 알 수 있듯이 model parameters를 업데이트하는 과정에서 “전체” training data가 아닌 “일부” training data만을 사용하여 하나의 step을 업데이트 할 때 보다 덜 정확하지만 빠른 연산 속도를 통해 더 나은 성능을 도출하자는 철학을 가졌다.   용어 정리   SGD에서 새롭게 알게 될 용어들에 대해 간단히 살펴보자.   Epoch   Epoch은 한국말로 “시대”를 뜻한다. AI에서 epoch은 전체 dataset에 대하여 한 번 학습을 완료한 상태(forward &amp; backward pass과정을 거친 상태)를 의미한다.   Batch   batch는 나누어진 dataset을 의미한다. gradient descent에서는 전체 dataset을 한꺼번에 학습시키는데, 이로인해 발생하는 비효율 문제를 해소하고자 나온 알고리즘이 바로 stochastic gradient descent이었다. 여기서 SGD는 전체 dataset을 잘게 쪼개어 batch 단위로 학습시킨다고 볼 수 있다. batch를 mini-batch라고도 부르며, 일반적으로 사이즈는 32, 64, 128개를 주로 사용한다.   Iteration   iteration은 한국말로 “되풀이”라는 뜻이다. 전체 dataset을 쪼개어 batch 단위로 학습을 수행할 때, 1 epoch을 달성하기 위해서는 여러 번의 실행이 필요하다. 이러한 실행 반복을 iteration이라고 한다. 전체 data의 양이 N개라고 할 때 batch size가 n이면, iteration은 N/n이 되는 관계이다.   개념정리   Stochastic Gradient Descent (SGD)에 대한 loss function은 아래와 같다. \\(L(W)=\\frac{1}{N}\\sum_{i=1}^{N}{L_i(x_i,y_i,W)+λR(W)}\\\\ ▽_WL(W)=\\frac{1}{N}\\sum_{i=1}^{N}{▽_WL_i(x_i,y_i,W)+λ▽_WR(W)}\\)   # Vanilla Minibatch Gradient Descent  while True:     data_batch = sample_training_data(data, 256) # sample 256 examples \tweights_grad = eveluate_gradient(loss,fun, data_batch, weights)     weights += - step_size * weights_grad # perform parameter update   Computational Graph &amp; Backpropagation   앞서 optimization을 하는 방법 중 하나로 gradient descent와 같은 방법을 언급했다. 하지만 다양한 꼴의 function에 대해 gradient를 구하기란 쉽지 않다. 지금까지 배운 functions만 나열해도 아래와 같다. \\(s = f(x; W_1,W_2)=W_2\\max{(0,W_1x)} ···· Nonlinear\\ score\\ function\\\\ L_i = \\sum_{j≠y_i}{\\max(0, s_j-s_{y_i}+1)} ···· SVM\\ Loss\\ on\\ predictions\\\\ R(W) = \\sum_k{W_k^2}···Regularization\\\\ L=\\frac{1}{N}\\sum_{i=1}^N{L_i+λR(W_1)+λR(W_2)} ····Total\\ loss:data\\ loss+regularization\\) 가장 좋지 않은 방법은 일일히 손으로 gradient를 계산하는 것이다. 이러한 방법은 너무 많은 matrix 계산을 필요로 할 뿐만 아니라 network에서 일부분을 수정하는 경우 전체 gradient를 다시 계산해야 하는 문제에 직면한다. 또한 너무 복잡한 모델에 대해서는 계산 자체가 쉽지 않다. 이러한 단점을 해소하기 위해 등장한 개념이 바로 computational graphs와 backpropagation이다.   Backpropagation   간단한 예제에 대해 backpropagation을 구해보자.      위 이미지는 model parameters x, y, z를 학습하는 방법에 대한 문제이다. x, y, z의 초기값(연두색)이 각각 -2, 5, -4일 때 출력값은 -12이다. 각 node별로 gradient를 계산함으로써 보다 간편하게 backpropagation할 수 있다. 먼저 출력쪽(f)에서의 gradient를 구하면 아래와 같다. \\(gradient\\ of\\ f : \\frac{∂f}{∂f}=1\\) df/df=1를 항상 만족하므로 마지막 출력쪽에서의 gradient는 언제나 1이다. 이를 이용하여 q와 z에서의 gradient를 구하면 아래와 같다. \\(gradient\\ of\\ q : \\frac{∂f}{∂q}=\\frac{∂f}{∂f}*\\frac{∂f}{∂q}=1*\\frac{∂qz} {∂q}=1*z=-4\\\\ gradient\\ of\\ z : \\frac{∂f}{∂z}=\\frac{∂f}{∂f}*\\frac{∂f}{∂z}=1*\\frac{∂qz}{∂z}=1*q=3\\\\\\) 이를 이용하여 x, y에서의 gradient df/dx, df/dy를 chain rule을 활용하여 아래와 같이 계산할 수 있다. \\(gradient\\ of\\ x : \\frac{∂f}{∂x}=\\frac{∂f}{∂q}*\\frac{∂q}{∂x}=\\frac{∂qz}{∂q}*\\frac{∂(x+y)}{∂x}=z*1=-4\\\\ gradient\\ of\\ y : \\frac{∂f}{∂y}=\\frac{∂f}{∂q}*\\frac{∂q}{∂y}=\\frac{∂qz}{∂q}*\\frac{∂(x+y)}{∂x}=z*1=-4\\\\\\)   Chain Rule   Chain Rule의 개념을 통해 각 node별 gradient 계산 과정을 더 잘 이해할 수 있다. 아래 이미지는 chain rule을 그림으로 표현한 것이다.      먼저 node f 기준으로 봤을 때 출력단에서 되돌아오는 gradient를 upstream gradient라고 한다. 최종 loss L을 f의 출력 z로 미분한 결과에 해당한다. node f를 기준으로 했을 때 입력단으로 되돌아가는 gradient는 downstream gradient라고 부른다. downstream gradient는 최종 loss L을 입력 x나 y로 미분한 결과에 해당한다. downstream gradient를 계산하기 위해서는 local gradient가 필요하다. local gradient는 node f 기준으로 output z에 대한 input x(or y)의 미분 결과이다. downstream gradient는 upstream gradient와 local gradient의 곱으로 표현할 수 있다. 이러한 관계를 chain rule이라고 부른다.   Example   아래와 같은 예시를 통해 좀 더 복잡한 loss function에 대한 backpropagation을 계산해 보자.      위와 같은 loss function f(w,x)가 있을 때 각 node에 대한 local gradient는 다음과 같이 계산된다.      한편, 이러한 loss function에서 현재 model parameters가 아래와 같다고 가정해보자.      이때 loss function 최종 출력 부분에서의 upstream gradient값이 1.00이므로 chain rule에 의하여 아래와 같이 backpropagation을 계산할 수 있다.      이 부분은 위에서 설명했던 chain rule 공식 (downstream gradient = upstream gradient * local gradient)만 잘 활용한다면 쉽게 계산할 수 있다. 한편, 위 loss function의 일부 nodes는 아래와 같이 하나의 function 단위(sigmoid)로 묶일 수 있다.      sigmoid 함수 σ(x)의 local gradient값은 (1-σ(x))σ(x)이다. 이런 부분은 nodes를 하나로 치환해서 바라보는 것이 더 쉽다.   Patterns in gradient flow   자주 사용되는 node에 대하여 local gradients는 다음과 같다.      Backpropagation Implementation   Modularized API를 활용하여 forward &amp; backprop 를 구현한 코드는 아래와 같다 (pseudo)   class ComputationalGraph(object):     # ...     def forward(inputs):         # 1. [pass inputs to input gates ...]         # 2. forward the computational graph:         for gate in self.graph.nodes_topologically_sorted():             gate.forward()         return loss # the final gate in the graph outputs the loss     def backward():         for gate in reversed(self.graph.nodes_topologically_sorted()):             gate.backward() # little piece of backprop (chain rule applied)         return inputs_gradients   Regularization   지금까지 optimization(backpropagation)을 통해 model parameters를 업데이트하는 과정을 살펴 보았다. 그렇다면 loss가 최소가 되는 model parameters는 unique할까? 이전에 봤던  SVM classifier를 예로 들어 생각해보자. SVM classifier의 loss는 아래 그림과 같이 hinge loss의 형태로 도식화 할 수 있었다.      이러한 간단한 예시만 보아도, 다른 모든 class가 정답 scores보다 margin만큼 작다면 언제든지 loss=0이 된다는 것을 확인할 수 있다. 즉, loss를 최소화하는 model parameters는 유일하지 않다. 그렇다면 어떤 model parameters가 가장 좋을까? 이를 설명하기 위해 regularization의 개념을 알 필요가 있다.   개념   우리가 일반적으로 알던 loss function은 아래와 같은 형태이다. \\(L(W)=\\frac{1}{N}\\sum_{i=1}^N{L_i(f(x_i,W),y_i)}=(Data\\ loss)\\) 위 수식은 data loss만을 고려한 loss function이다. data loss란 training data에 대해 model의 성능을 나타내는 지표이다. 이러한 loss function에 regularization 개념을 추가하여 아래와 같이 확장할 수 있다. \\(L(W)=\\frac{1}{N}\\sum_{i=1}^N{L_i(f(x_i,W),y_i)+λR(W)}=(Data\\ loss)+(Regularization)\\) 여기서 λ(regularization strength)는 regularization의 정도를 나타내는 hyperparameter이며 Regularization은 model이 training data에 대하여 너무 잘 맞는 **overfitting**을 방지하기 위해 추가되는 항이다.   Overfitting   간단한 예시를 통해 overfitting의 개념을 알아보자. 아래와 같은 training data에 대하여 모델을 학습시킨다고 가정하자.      여기서 model parameters에 의한 수식은 아래와 같이 두 가지(이상)의 형태 (f1, f2)로 표현된다. f1은 loss=0인 상태이며, f2는 오히려 f1보다 loss가 더 큰 상태이다.      일반적인 경우 머신러닝 분야는 보다 simple한 모델을 선호하는 경향이 있다. 이는 현실계에서 대부분의 현상이 관성(원래 성질을 유지하려는 속성)이라는 속성을 갖기 때문이라 설명할 수 있다. 이러한 관점에서 봤을 때 f1보다 f2가 더 좋은 model parameters를 갖는다. 가령 아래와 같이 data가 추가될 경우를 상상할 때 이러한 관점이 더욱 명확해진다.      즉, model이 training data에 대해 너무 잘 맞으면(overfit, loss=0)  새로운 데이터가 들어왔을 때 오히려 성능이 떨어지는 현상을 overfitting현상이라고 하며, 이러한 현상을 방지하기 위해 loss function에 regularization term을 두어 오히려 training data에 대해 성능을 떨어뜨리는 방식으로 전체 성능을 높힐 수 있다.   Examples   대표적인 regularization function에 대해서 살펴보자.   Simple examples   비교적 간단한 형태의 regularization functions은 L1, L2, Elastic net이 있다. 예를 들어, x, w1, w2가 아래와 같을 때 더 나은 weights를 구해보자. \\(x=[1,1,1,1]\\\\ w_1=[1,0,0,0]\\\\ w_2=[0.25, 0.25, 0.25, 0.25]\\)   L2 regularization   \\[R(W) = \\sum_k\\sum_l{W_{k,l}^2}\\]  위 예시에 대하여 L2 regularization을 적용한 결과는 아래와 같다. \\(R(w_1)=(1-1)^2+(1-0)^2+(1-0)^2+(1-0)^2=3\\\\ R(w_2)=(1-0.25)^2+(1-0.25)^2+(1-0.25)^2+(1-0.25)^2=2.25\\) L2 regularization은 weights가 spread out(퍼진) 형태를 선호한다. 따라서 w2의 model parameters가 더 좋은 parameter임을 알 수 있다.   L1 regularization   \\[R(W)=\\sum_k\\sum_l|W_{k,l}|\\]  위 예시에 대하여 L1 regularization을 적용한 결과는 아래와 같다. \\(R(w_1)=|1-1|+|1-0|+|1-0|+|1-0|=3\\\\ R(w_2)=|1-0.25|+|1-0.25|+|1-0.25|+|1-0.25|=\t3\\) xL1 regularization에 의하면 w1와 w2의 성능은 같다.   Elastic net (L1+L2)   \\[R(W)=\\sum_k\\sum_l{βW_{k,l}^2+|W_{k,l}|}\\]  Elastic net은 L1 regularization과 L2 regularization을 합친 형태이다.   Complex examples   비교적 복잡한 형태의 regularization functions에는 dropout, batch normalization, stochastic depth, fractional pooling등이 있다. 아래의 regularization 방식에 대해서는 추후 더 자세히 다루도록 한다.   Dropout   Batch normalization   Stochastic depth   Fractional pooling   ","categories": ["ai-deepLearning"],
        "tags": ["Deep Learning","AIAS"],
        "url": "/ai-deeplearning/DL02/",
        "teaser": null
      },{
        "title": "[REVIEW-01] Adam: A Method for Stochastic Optimization",
        "excerpt":"  배경   training samples가 너무 많은 경우, 모든 training samples 각각에 대한 loss를 계산한 뒤 평균을 내는 방법은 연산량이 너무 많아 비효율적이다. 이러한 배경에서 stochastic gradient descent(SGD)가 탄생했다. 하지만 SGD는 gradient 기반 optimizer이므로 local minimum에 갇힐 수 있다는 문제점을 가지고 있다.      위 이미지는 local minimum(혹은 saddle point)에 갇히는 경우를 나타낸다. gradient값이 일시적으로 0인 경우, gradient descent가 최적화를 중지한다는 문제를 해결하고자 여러가지 optimizers가 제안되었다.   아래 이미지는 여러가지 optimizers의 장단점을 잘 보여줘서 가져와봤다.      이 중 Adam optimizer는 “무엇을 써야할 지 모른다면 Adam을 써라!” 라는 말이 있을 정도로 대부분의 상황에서 무난하게 잘 적용된다. 이번 포스팅에서는 이 중 Adam optimizer에 관한 논문을 읽고 리뷰해보려고 한다.       리뷰   연구 문제   이 저자가 바라보는 문제는 무엇인가? 무엇을 해결하고 싶었던 걸까?   모수(parameter)란 모집단의 특성을 대표하는 수치이다. 모수는 보통 알려져 있지 않으므로 과학과 공학 분야에서 다루는 대부분의 최적화 문제에서는 모수를 추정하여 문제를 해결할 수밖에 없다. 따라서 최적화 문제에서 모수가 불안정하다는 특성으로 인해 objective functions은 First order method로 다루어야 한다. 이러한 stochastic gradient 기반 최적화를 위해 기존에는 다양한 optimizers가 제시되었다(SGD, AdaGrad, RMSProp 등). 이 논문에서는 기존의 optimizers(AdaGrad, RMSProp)의 장점을 취하고 단점을 보완하는 새로운 stochastic optimization method로 Adam(Adaptive Momen)을 제안한다.   먼저, 기존의 SGD방식은 모든 parameter에 대하여 같은 learning rate를 곱한다는 단점이 있다. 이러한 특징은 ill-condition일 경우 학습효율의 저하로 이어진다. 이러한 단점을 극복하기 위해 과거의 gradient를 참고하는 gradient accumulation variable(이하 GAV)을 도입한 AdaGrad 방식이 제안되었다. AdaGrad 방식은 변화량이 큰 parameter에 대해 GAV로 나누어 step size를 작게 변화시킨다. 따라서 sparse gradients에 대해 잘 작동한다. 하지만 iteration이 증가함에 따라 GAV의 값이 증가하여 step size가 너무 작아진다는 한계가 있다. RMSProp 방식은 exponential moving average개념을 도입함으로써 이러한 한계를 보완하였다. Adam은 이러한 RMSProp 방식에 momentum을 조합하고 bias-correction 개념을 추가하여 확장한 방법이다.        방법  문제를 해결하기 위해서 어떤 방법을 이용했나? 방법을 만들 때 참고한 이전 방법이 있었나?   Adam은 RMSProp와 momentum을 조합한 뒤 bias-correction 개념을 추가한 방식이다. 다음 [참고1]은 RMSProp 알고리즘을 나타낸다.      momentum은 현재 parameter의 업데이트에 이전 gradient의 값을 반영한다. Adam은 RMSProp방식과 momentum을 조합하였다. [참고1]의 4번 과정에 의하면 r값을 이전 값들을 축적한 r와 g⊙g에 대한 exponential moving average의 값으로 업데이트 한다. 이는 Adam에서 2nd moment estimate이라고 해석할 수 있다. 즉, Adam에서는 exponential decay rates β_1,β_2와 moment variable m_t, g_t (각각 1st, 2nd moment variable)가 있다고 할 때, [참고1]의 4번 과정이 아래와 같이 수정된다.      Adam algorithm에서는 초기에([참고1]의 1번 과정에서) moment variable m_t, g_t를 각각 0으로 초기화해준다. 이는 학습 초기 가중치가 0으로 편향되는 문제를 야기할 수 있다. 따라서 아래와 같이 bias-correction 개념을 적용한다.      한편 AdaGrad의 parameter 업데이트 공식은 아래와 같다. (ε: global learning rate, g: gradient, δ: small constant. r: gradient accumulation variable)      Adam은 parameter를 업데이트 하는 방법으로 AdaGrad의 업데이트 공식을 이용했다. Adam의 parameter 업데이트 공식은 아래와 같다. (α: step size, (m_t ) ̂: biased 1st moment estimate, ε: small constant(약 10^(-8)), (v_t ) ̂: biased 2nd moment estimate)      위의 설명을 참고하여 [참고1]의 RMSProp 알고리즘에 반영하면 아래와 같은 Adam algorithm을 도출할 수 있다.      분석   이 방법이 좋다는 것을 어떻게 증명하였나?   논문에서는 도입한 1st, 2nd moment estimation을 통해 최적의 parameter를 구하는 과정을 SNR의 개념으로 쉽게 설명한다. (m_t ) ̂와 (v_t ) ̂는 확률적 기대값이 각각 gradient의 1차, 2차 moment이다. 이는 gradient에 대한 모평균, 모분산을 의미한다. 이러한 논리 과정을 통해 논문에서는 (m_t ) ̂/√((v_t ) ̂ )을 signal-to-noise로 해석하여 [참고2]의 5번 과정에 대한 직관적인 해석을 돕는다.   Adam은 현재 parameter값 근처에 trust region을 생성한다. 이를 통해 step size에 대한 적절한 scale을 미리 알 수 있어서 Adam 방식을 신뢰할 수 있게 된다.  논문에서는 (1-β_1 )&gt;√(1-β_2 )인 경우와 그렇지 않은 경우, 두 가지 경우에 대해 effective step size의 magnitude값(|∆_t|)을 살펴봄으로써 수식적으로 증명한다.   Adam 방법이 실제 상황에서 잘 적용되는지를 증명하기 위해 저자는 실험적인 방식을 이용한다. 널리 사용되는 machine learning models에 대해 Adam optimizer를 적용한 결과를 다른 stochastic optimizers를 적용한 결과와 비교한 결과, Adam이 다른 방법에 비해 좋은 성과를 보였다. 또한 VAE(Variational Auto-Encoder)을 서로 다른 β_1, β_2에 대해 학습시킨 결과를 통해 β_2의 값이 커질수록(1에 가까워질 수록) bias-correction 개념의 도입으로 인한 loss값 감소의 효과가 커짐을 증명하였다.       References           Adam: A Method for Stochastic Optimization : https://arxiv.org/abs/1412.6980            자습해도 모르겠던 딥러닝, 머리속에 인스톨 시켜드립니다. : https://www.slideshare.net/yongho/ss-79607172      ","categories": ["ai-deepLearning"],
        "tags": ["Deep Learning","optimizer","review"],
        "url": "/ai-deeplearning/REVIEW01-adam/",
        "teaser": null
      },{
        "title": "[CVPR2021-01] CVPR2021 트렌드 및 수상 논문 살펴보기",
        "excerpt":"  CVPR(IEEE Conference on Computer Vision and Pattern Recognition)은 컴퓨터 비전과 패턴 인식 분야에서 세계적으로 최고 수준의(Top-tier) 학회이다. 이번 포스팅에서는 2021년도 CVPR 학회의 트렌드와 수상한 몇 가지 논문을 Abstract 위주로 살펴본다.   트렌드      이번 CVPR에 가장 많은 논문을 제출한 국내 기관은 총 20편 기록을 세운 연세대다. 다음으로 서울대가 17편, KAIST가 10편 논문을 발표했다.     기업 중에서는 네이버가 총 8개로 최다 논문 채택 수를 기록했다. 네이버랩스 유럽에서 발표한 논문 중 하나는 심층발표 세션에 채택됐다. 카카오는 이번 CVPR에서 2개 논문을 발표했는데, 이 중 하나인 HOTR 논문은 심층발표 세션에서 공개됐다.    CVPR 채택 논문의 제목 키워드를 추출하여 트렌드를 분석한 결과(출처: 이호성 코그넥스 리서치 엔지니어)는 다음과 같다.   주요 키워드   CVPR 2021 채택 논문의 주요 키워드를 시각화한 결과는 다음과 같다.      이를 CVPR 2020 채택 논문과 비교한 결과는 다음과 같다.              Image, detection, 3d, object, video, segmentation 등은 이전 년도와 마찬가지로 주요 키워드를 차지했다.            unsupervised, self-supervised, semi-supervised는 이전과 비교했을 때 대략 1.5배 빈도수가 증가했다.                       unsupervised : 52 -&gt; 77                        self-supervised: 33 -&gt; 50                        semi-supervised: 25 -&gt; 35                       CVPR’21 PAPER AWARDS   Best Student Paper Honorable Mentions           Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling  Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, Jingjing Liu (link)              배경                       일반적인 video-and-language 학습 방법에 의하면, vision models의 dense video features과 language models의 text features로부터 뉴런 모델을 학습시킨다.                        (문제점) 이러한 feature extractors는 일반적으로 target domains와 다른 tasks에 대해 독립적으로 학습되므로, 이러한 고정된 features는 downstream tasks에 대해 차선책으로 사용된다.           또한 dense video features의 높은 연산 과부하로인해 feature extractors를 기존의 접근 방식에 직접적으로 연결하여 finetuning하기 어렵다.                        (해결책) 이러한 딜레마를 해결하기위해, 포괄적인 프레임워크인 CLIPBERT을 제안한다.                   해결: CLIPBERT              video-and-language tasks를 학습하기 위한 합리적인 학습 방법이다.       각 학습 단계에서, 비디오로부터 sparse sampling된 하나 혹은 아주 짧은 비디오 클립만을 사용한다.           결론                       6 datasets에 대해 test-to-video 검색 및 video 질의응답에 대한 실험에 의하면, CLIPBERT는 기존의 방식(전체 비디오를 활용)보다 더 높은 정확도를 달성하였다.           즉, 전체 비디오로부터 offline features를 추출하는 방식(기존)에 비교했을 때, 단지 몇 개의 드문드문하게 샘플링된 클릭만 사용하는 것(CLIPBERT)이 “less is more, 덜한 것이 더한 것이다”를 증명하는 셈이다.                        3초짜리 GIF 영상부터 180초짜리 유튜브 영상까지, 서로다른 도메인과 길이를 갖는 데이터셋에 대해 실험한 결과는 이러한 접근 방식이 일반화가능하다는 것을 증명한다.                        요약      video-and-language 학습방법은 video로부터 추출한 특징과 text로부터 추출한 특징을 cross-modal modeling하여 결과를 도출한다. 기존에는 전체 비디오에 대해 특징을 추출하여 사용했으나, 연산 과부하로인해 정밀튜닝이 어렵다는 문제가 있다. 이러한 문제를 해결하기위해 비디오를 짧은 클립 영상으로 나눈 뒤 사용하는 CLIPBERT 모델을 제안하였다.                    Binary TTC: A Temporal Geofence for Autonomous Navigation Abhishek Badki, Orazio Gallo, Jan Kautz, Pradeep Sen (link)       배경                       Time-to-contact(TTC)은 경로 계획(path planning)을 위한 강력한 도구이다.                      Time-to-contact(TTC)란?  “물체가 관찰자의 평면과 충돌하는 시간”이다.  e.g., 빛(물체)이 사물에 반사되어 우리 눈(관찰자의 평면)에 충돌하는데, 충돌에 걸리는 시간을 TTC라고 한다.                    TTC의 장점                      물체의 깊이(depth), 속도(velocity), 그리고 가속도(acceleration) 보다 더 많은 정보를 제공한다.           보정되지 않은 단안 카메라(a monocular, uncalibrated camera)만 있으면 된다.                   (문제점) TTC의 단점                      각 픽셀에 대한 TTC의 회귀(regressing)가 항상 바로 오는 것은 아니다.           대부분의 기존 방식은 장면에 대한 추정을 과하게 단순화한다.                           해결: Binary TTC              일련의 더 단순한 이진 분류를 통해 TTC를 추정하여 TTC의 기존 문제점을 해결한다.           결론                       관찰자가 특정시간 내에 장애물과 충돌하는지 여부는 때로 정확한 픽셀당 TTC를 아는 것보다 더 중요하며, 이는 짧은 지연시간으로 측정할 수 있다.           이러한 시나리오에서, Binary TTC 방법은 기존의 방법보다 25배 이상 빠른 6.4ms geofence를 제공한다.           또한 계산하기 위한 예산이 충분한 경우, 연속값을 포함하는 임의의 미세 양자화(fine quantization)를 이용하여 픽셀당 TTC를 추정할 수 있다.                        충분히 높은 프레임 속도로 TTC정보(binary 혹은 coarsely quantized)를 제공하는 최초의 방법이라 생각된다.                        요약      TCC는 다른 방법에 비해 더 많은 정보를 제공한다는 점과 단지 단안 카메라만 있으면 된다는 점에서 장점을 갖는다. 하지만 TTC는 regression이 언제나 직접적이지 않으며 추정을 과하게 단순화한다는 단점이 있다. 이러한 단점을 해결하기위해 이진 분류로 TTC를 추정하는 방법을 제안한다.                    Real-Time High-Resolution Background Matting Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian Curless, Steve Seitz, Ira Kemelmacher-Shlizerman (link)              배경              background matting 기술에서 실시간으로 고해상도 이미지를 처리하며, 가닥 수준의 머리카락 세부 사항을 유지하면서 고품질의 알파 매트(alpha matte)를 계산하는 것은 어렵다.           해결: 실시간, 고해상도 배경 교체 기술                       최신 GPU에서, 30fps(4K 해상도) 혹은 60fps(HD)로 동작한다.                        background matting를 기반으로 동작한다.                      background matting란? 배경의 추가 프레임이 캡쳐되어 알파 매트(alpha matte)와 전경 레이어(foreground layer)에 대한 정보를 제공하는 데에 사용된다.                                            베이스 네트워크는 저해상도 결과를 계산하며, 이는 두 번째 네트워크에 의해 선택적 패치에 대해 고해상도로 작동한다.                        2개의 대규모 비디오 및 이미지 매트 데이터셋인 VideoMatte240K와 PhotoMatte 13K/85를 소개한다.                   결론              이전의 최첨단 배경 매트 기술과 비교했을 때, 더 높은 품질의 결과를 산출함과 동시에 속도와 해상도 측면에서 모두 극적인 향상을 가져왔다.                요약      기존의 배경 매팅 기술에서 실시간으로 고해상도 이미지를 처리하면서 세부사항을 유지하는 알파 매트를 계산하는 것이 매우 어려웠다. 첫 번째 네트워크에서 대략적으로 결과를 계산하고 두 번째 네트워크에서 선택적 패치에 대해 세부적으로 결과를 계산함으로써, background matting을 기반으로 한 실시간 고해상도 배경 교체 기술을 달성했다.           Best Student Paper           Task Programming: Learning Data Efficient Behavior Representations Jennifer J. Sun, Ann Kennedy, Eric Zhan, David J. Anderson, Yisong Yue, Pietro Perona (link)       배경              전문 도메인 지식은 보통 심층적인 분석을 위해 도메인 전문가로부터 training sets에 정확하게 주석을 달아야하지만, 부담스럽고 시간이 많이 소요되는 일이다.       이러한 문제는 video tracking data에서, 행위자의 관심있는 움직임이나 행동이 감지되는 자동화된 행동 분석에서 두드러지게 발생한다.           해결: TREBA              annotation을 위한 수고를 덜기 위해 제안된, multi-task self-supervised learning 기반으로 하는 행동 분석을 위한, annotation-sample 효율적인 궤적 임베딩을 학습하는 방법이다.       도메인 전문가에 의해 구조화된 지식을 명시적으로 인코딩하는 프로그램을 사용하는 “task programming”이라는 프로세스를 통해, 도메인 전문가가 효율적으로 설계할 수 있습니다.       data annotation time 대신, 적은 수의 프로그래밍된 작업에 시간을 사용함으로써, 전체 도메인 전문가의 노력을 줄일 수 있다.           결론                       행동을 식별하기 위해 전문 도메인 지식이 사용되는 신경과학의 데이터를 통해 이러한 trade-off를 평가한다.                        생쥐와 초파리, 두 개의 도메인에 걸쳐 세 가지 데이터셋에 대한 실험 결과를 제시한다.           TREBA의 임베딩을 사용하여, 최첨단 기능에 비교했을 때, 정확도를 손상시키지 않으며 annotation 부담을 최대 10배까지 줄였다.           결과적으로, 작업 프로그래밍(task programming)과 자체 감독(self-supervision) 방식이 도메인 전문가의 annotation 부담을 줄여주는 효과적인 방법임을 시사한다.                        요약      전문 도메인 지식은 도메인 전문가에 의해 주석되어야 하는데, 이는 매우 비용 소모적이라는 문제가 있다. multi-task self-supervised learning 기반으로 동작하는 TREBA 방법에서, task programming이라는 프로세스를 통해 도메인 전문가에 의해 구조화된 지식을 명시적으로 인코딩함으로써 이러한 문제를 해결한다.           Best Paper Honorable Mentions           Exploring Simple Siamese Representation Learning Xinlei Chen, Kaiming He (link)       배경                       Siamese networks는 unsupervised visual representation learning을 위한 다양한 최근 모델에서 일반적인 구조가 되었다.           이러한 구조는, 솔루션 붕괴를 막기 위한 특정 조건에 따라, 하나의 이미지의 두 가지 augmentations 사이의 유사성을 극대화한다.                   해결                       단순한 Siamese networks가 다음의 것들을 사용하지 않아도 의미있는 표현을 학습할 수 있다는 놀라운 경험적 결과를 보고한다.           (1) 음수 샘플 쌍 (negative sample pairs) (2) 대규모 배치 (large batches) (3) 운동량 인코더 (momentum encoders)                   결론                       실험은, loss와 structure에 대한 collapsing 솔루션이 존재하지만, collapsing을 막기 위해서는 stop-gradient 연산의 역할이 필수적임을 보여준다.           논문은, stop-gradient의 의미에 대한 가설을 제공하고 이를 확인하는 개념 증명(proof-of-concept) 실험을 추가로 보여준다.                        논문의 “SimSiam” 방법은 ImageNet과 downstream tasks에서 경쟁력있는 결과를 달성한다.                        이러한 간단한 기준이 unsupervised representation learning에 대한 Siamese 아키텍처의 역할에 대해 다시 한 번 생각해볼 수 있는 동기가 되기를 바란다.                        요약      unsupervised representation learning에서는 일반적으로 Siamese 네트워크 구조가 사용된다. 하지만 논문은 Siamese 네트워크에서 음수 샘플 쌍, 대규모 배치, 운동량 인코더는 필수적인 조건이 아님을 실험을 통해 밝혔다. 또한 붕괴를 막기 위해서는 stop-gradient 연산이 필수적임을 밝히며 이에 대한 가설을 증명했다.                   Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos Yasamin Jafarian, Hyun Soo Park (link)       배경              옷입은 인간의 기하학 학습에서 주요 과제는, ground truth data (e.g., 3D scanned models)의 제한된 가용성에 있다. 이는 실제 이미지에 적용할 때 3D human reconstruction의 성능 저하를 초래한다.           해결                                  이러한 문제를 해결하기 위해 새로운 데이터 리소스를 활용하였다.           다양한 외모, 옷 스타일, 퍼포먼스, 정체성을 아우르는 여러 개의 소셜 미디어 댄스 비디오                   각 비디오는 3D ground truth geometry가 부족한 한 사람의 몸과 옷의 역동적인 움직임을 묘사한다.                        이러한 비디오를 사용하기 위해, 지역변환(local transformtation)을 사용하는 새로운 방법을 제시한다. 이러한 방법은, 이미지에서 예측된 사람의 local geometry를 다른 시간의 다른 이미지의 것으로 변환한다.                        이러한 변환을 통해 예측된 geometry는 다른 이미지로부터 변형된 geometry에 의해 self-supervised 될 수 있다.               추가로, 기하학적 일관성을 극대화하여 로컬 텍스처, 주름 및 음영에 대해 매우 민감하게 반응하는 표면 법선(surface normals)와 함께, 깊이를 공동으로 학습한다.           결론              이러한 방법은 end-to-end 학습이 가능하므로, 입력 실제 이미지에 대해 충실한 정밀 geometry를 예측하는, 충실도 높은 깊이 추정이 가능하다.       이러한 방법은, 실제 이미지와 렌더링된 이미지 모두에서 최첨단 human depth 추정과 human shape 복구 접근 방식을 능가한다.                요약      옷을 입은 인간의 기하학 학습에서는 ground truth 데이터가 제한된 가용성을 갖는다는 한계가 있다. 이를 해결하기 위해 여러 개의 소셜 미디어 댄스 비디오를 활용하는 방안을 제시한다. 이러한 방법은 최첨단 human depth 추정과 human shape 복구 접근 방식보다 좋은 성능을 달성했다.           Best Paper           GIRAFFE: Representing Scenes As Compositional Generative Neural Feature Fields Michael Niemeyer, Andreas Geiger (link)       배경                       Deep generative models은 고해상도에서 사실적인 이미지 합성(photorealistic image synthesis)를 가능하게 해준다. 하지만, 너무 많은 응용 프로그램에 대해 충분하지 않기 때문에, content creation 또한 제어할 수 있어야 한다.                        최근 몇 가지 연구에서 데이터 변동의 근본적인 요인을 푸는 방법을 연구하고 있으나, 대부분은 2차원에서 동작하므로 우리의 세계가 3차원이라는 사실을 무시한다.           또한 장면의 구성적인 특성을 고려한 연구는 거의 존재하지 않는다.                   해결              주요 가설은 “합성 3D 장면 표현을 생성 모델에 통합하면, 더 제어 가능한 이미지 합성으로 이어질 것”이다.       장면을 합성 생성 신경 기능 필드(compositional generative neural feature fields)로 표현함으로써, 추가적인 supervision 없이 구조화되지 않은 이미지 컬렉션으로부터 학습하는 동안, 하나 또는 다중 객체를 배경 뿐만 아니라 개별 객체의 모양 및 외형으로부터 분리할 수 있다.       이러한 장면 표현을 neural rendering pipeline과 결함하여, 더 빠르고 사실적인 이미지 합성 모델을 생성한다.           결론              실험에 의해, 이러한 모델이 개별 객체를 풀 수 있고, 장면에서의 변환 및 회전뿐만 아니라 카메라 포즈를 변경할 수도 있음을 보였다.                요약      Deep generative models은 고해상도에서 사실적인 이미지 합성을 가능하게 해주지만, content 생성 제어에 대한 연구는 많이 이루어지지 않았다. 논문은, 합성 3D 장면 표현을 생성 모델에 통합함으로써 이를 해결하였다. 실험을 통해 이러한 모델이 개별 객체를 풀고, 장면에서의 변환과 회전뿐만 아니라 카메라의 포즈도 변경할 수 있음을 보였다.             아직 모르는게 많다보니, 보다 다양한 주제의 논문을 읽어보고 싶다. 원래는 포스팅 하나에 전부 다루고싶었지만 분량상 나누어 다룰 예정이다. 다음 포스팅에서 내가 관심있는 주제에 관한 CVPR2021 논문을 이어서 다루도록 한다.       References      CVPR 2021: http://cvpr2021.thecvf.com/   brunch/kakao-it: https://brunch.co.kr/@kakao-it/297   CVPR-2021-Paper-Satistics : https://github.com/hoya012/CVPR-2021-Paper-Statistics?fbclid=IwAR0MGG3x-9bDU8YjVp-UGHcJDAXUspNwJ3Iy-o17oi7UFbTSFGcKS_OqbaQ   52CV/CVPR-2021-Papers: https://github.com/52CV/CVPR-2021-Papers   amusi/CVPR2021-Papers-with-Code: https://github.com/amusi/CVPR2021-Papers-with-Code       ","categories": ["ai-computerVision"],
        "tags": ["Computer Vision","CVPR","Papers","Review"],
        "url": "/ai-computervision/CVPR202101/",
        "teaser": null
      },{
        "title": "[CVPR2021-02] CVPR2021-SLAM/AR/Robotics 논문 훑기",
        "excerpt":"  CVPR(IEEE Conference on Computer Vision and Pattern Recognition)은 컴퓨터 비전과 패턴 인식 분야에서 세계적으로 최고 수준의(Top-tier) 학회이다. 이번 포스팅에서는 2021년도 CVPR 학회에서 SLAM/AR/Robotics 주제에 관한 몇 가지 논문들을 Abstract 위주로 살펴본다.       관심 논문   내가 관심있는 주제에 관련된 몇 가지 논문을 abstract 위주로 살펴본다.   👾 SLAM/AR/Robotics    \t차례              Tangent Space Backpropagation for 3D Transformation Groups         Visual Room Rearrangement         GATSBI: Generative Agent-centric Spatio-temporal Object Interaction         DexYCB: A Benchmark for Capturing Hand Grasping of Objects         ContactOpt: Optimizing Contact to Improve Grasps         ManipulaTHOR: A Framework for Visual Object Manipulation        Tangent Space Backpropagation for 3D Transformation Groups  Zachary Teed, Jia Deng    (link)   배경      3D 변환(3D transformation) 그룹은 3D vision과 로봇공학에서 널리 사용되지만, vector spaces를 형성하지 않고 대신 smooth manifolds에 놓인다.   Euclidean 공간에 3D변환을 포함하는 표준 역전파(backpropagation) 접근 방법은, 수치적인 어려움을 겪고 있다.   해결      3D 변환의 그룹 구조를 활용하고, manifolds의 접선 공간(tangent spaces)에서 역전파를 수행하는 새로운 라이브러리를 소개한다.   3D 변환 그룹 SO, SE, 그리고 Sim을 포함하는 computation graphs에 대해 역전파를 수행하는 문제를 해결한다.   결론      이러한 접근 방법은 수치적으로 더 안정적이며, 구현이 더 쉽고, 다양한 작업 세트에 유용하다는 것을 보였다.   plug-and-play Pytorch 라이브러리는 여기에서 사용 가능하다.           요약     \t3D 변환 그룹은 3D vision과 로봇 공학에서 널리 사용되지만 vector spaces를 형성하지 않고 manifolds 공간에 놓인다. 따라서 Euclidean 공간에서 backpropagation을 수행해야 하는 어려움이 있는데, 이러한 문제를 3D 변환 그룹의 구조를 활용하고 manifolds의 접선 공간에서 backpropagation을 수행하는 라이브러리로 해결하였다. 수치적으로 더 안정적이며, 구현이 쉽고, 다양한 작업 세트에 유용함을 증명하였다.          Visual Room Rearrangement  Luca Weihs, Matt Deitke, Aniruddha Kembhavi, Roozbeh Mottaghi    (link)      배경      최근 Embodied AI 분야에서 상당한 진전이 있었다. 동시에, 많은 연구원들이 embodied agents가 완전히 보이지 않는 환경 내에서 탐색하고(navigate) 상호작용하도록 하는 모델과 알고리즘을 개발하였다.   해결: RoomR           재배열(Rearrangement) 작업을 위한 새로운 데이터 세트와 baseline 모델을 제안한다. 그중 특히 Room Rearrangement 작업에 초점을 맞춘다.       : agent가 방을 탐색하고 objects의 초기 구성을 기록하는 것으로 시작한다. 이후 agent를 제거하고 방 안의 일부 objects의 포즈와 상태(e.g., open/closed)를 변경한다. agent는 방안의 모든 objects의 초기 구성을 복원해야 한다.            RoomR이라 불리는 제안된 데이터 세트는 120개의 장면에서 72개의 서로 다른 object 타입을 포함하는 6,000개의 고유한 재배열 설정이 포함되어있다.       결론      실험을 통해 navigation과 object interaction을 포함하는 이러한 interactive task 문제를 해결했음을 밝혔다. 이는, embodied tasks에 대한 최신 최첨단 기술의 기능을 넘어서는 결과이다. 또한 여전히 이러한 유형의 작업에서 완벽한 성능을 달성하는 것과는 거리가 멀다.   코드와 데이터 세트는 여기에서 사용할 수 있다.           요약     \tRoom Rearrangement 작업에 초점을 맞춘 새로운 데이터 세트와 baseline 모델을 제안한다. 실험을 통해 navigation과 object interaction을 포함하는 interactive 작업에서 최첨단 기술의 능력을 넘어섬을 밝혔다.          GATSBI: Generative Agent-centric Spatio-temporal Object Interaction  Cheol-Hui Min, Jinseok Bae, Junho Lee, Young Min Kim    (link)   배경      비전 기반 의사결정 시나리오에서, agent는 다중 entities가 서로 상호작용하는 복잡한 고차원 observations에 직면한다.   agent는 필수 구성 요소를 식별하고 시간 범위를 따라 일관되게 전파하는 visual observation에 대한 좋은 scene representation이 필요하다.   해결: GATSBI      GATSBI는 일련의 raw observations을 agents의 행동의 시공간적 맥락을 완전히 포착하는 구조화된 잠재 표현으로 변환할 수 있는 생성모델이다.   GATSBI는 활성 agent, 정적 배경, 그리고 수동 오브젝트를 분리하기 위해 unsupervised object-centric scene representation learning을 활용한다. 이후, 분해된 entities간의 인과 관계(causal relationships)를 반영하는 상호작용을 모델링하고, 물리적으로 그럴듯한 미래의 상태를 예측한다.   결론      제안된 모델은 서로 다른 종류의 로봇과 오브젝트가 서로 동적으로 상호작용하는 다양한 환경에 대해 일반화된다.   GATSBI는 최첨단 기술에 비해 장면 분해(scene decomposition)와 비디오 예측(video prediction)에서 월등한 성능을 달성하였다.           요약     \t비전 기반 의사 결정 시나리오에서, agent는 복잡한 고차원 관찰에 직면한다. 이때 agent는 필수 구성 요소를 식별하고, 시간이 지나도 일관되게 전파되는 시각적 관찰에 대한 좋은 장면 표현을 필요로한다. 일련의 raw observations을 시공간적 맥락을 포함하는 구조화된 잠재 표현으로 변환할 수 있는 생성 모델인 GATSBI를 통해 이러한 문제를 해결한다. GATSBI는 최첨단 기술에 비해 장면 분해와 비디오 예측에서 월등한 성능을 달성하였다.          DexYCB: A Benchmark for Capturing Hand Grasping of Objects  Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj S. Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, Jan Kautz, Dieter Fox    (link)      배경      3D 객체 포즈 추정과 3D 손 포즈 추정은 비전에서 풀리지 않은 두가지 주요한 문제이다. 전통적으로 이러한 문제는 개별적으로 다루어졌지만, 많은 중요한 응용 문제에서 두 가지 기능을 동시에 사용할 필요가 있다. 예를 들어, 로봇 공학에서, 물체의 손 조작을 신뢰성있게 모션캡쳐하는 것이 시범(human demonstration)으로부터 학습하는 것과 유창하고 안전한 인간-로봇 상호작용하는 것 모두에 중요하다.   해결: DexYCB      DexYCB는 물체를 손으로 잡는 것을 캡쳐하기 위한 새로운 데이터세트이다.   결론           우선, 교차 데이터 세트 평가(cross-dataset evaluation)를 통해 DexYCB를 관련된 항목과 비교한다.       이후, 세 가지 관련 tasks에 대한 최첨단 접근 방식의 철저한 벤치마크를 제시한다: 2차원 객체 및 키포인트 감지, 6차원 객체 포즈 추정, 그리고 3차원 손 포즈 추정       마지막으로, 새로운 로봇공학 관련 tasks를 평가한다: human-to-robot object handover에서 안전한 로봇의 grasps를 생성       그 결과, 데이터 세트가 중요한 측면에서 발전을 주도할 것이라 예상한다.            데이터 세트와 코드는 여기에서 사용할 수 있다.               요약     \t3차원 객체 포즈 추정과 3차원 손 포즈 추정은 비전 문제에서 매우 중요한 두 가지 문제이다. 이전까지 개별적으로는 많이 다뤄져 왔으나, 많은 응용분야에서는 두 기능을 동시에 사용할 필요가 있다. 논문은 물체를 손으로 잡는 것을 캡쳐하는 새로운 데이터 세트 DexYCB를 제안하여 이러한 문제를 해결한다. 실험 결과는 로봇 공학 관련된 tasks에서 발전을 주도할 것을 시사한다.          ContactOpt: Optimizing Contact to Improve Grasps  Patrick Grady, Chengcheng Tang, Christopher D. Twigg, Minh Vo, Samarth Brahmbhatt, Charles C. Kemp    (link)   배경      손과 물체간의 물리적인 접촉은 human grasps에서 중요한 역할을 한다.   해결: ContactOpt           hand mesh와 object mesh가 주어졌을 때, 접촉 데이터의 ground truth에 대해 학습된 딥 모델이 메시 표면에 걸쳐 바람직한 접촉을 추론한다.       이후, ContactOpt는 미분 가능한 contact model을 사용하여 원하는 접촉을 달성하기 위해 손 포즈를 효율적으로 최적화한다.       특히, 이러한 contact model은 메시 상호 침투(mesh interpenetration)를 야기하여 손의 변형 가능한 연조직(soft tissue)을 근사화한다.       결론      물체와의 접촉을 예상하기 위해 손 포즈를 최적화하는 방식을 통해, 이미지 기반 방법을 통해 추론된 손 포즈를 개선할 수 있음을 보였다.   이러한 방법이 ground truth contact과 더 잘 일치하는 grasps 결과를 도출하며, 더 낮은 kinematic error를 갖고, 인간 참여자가 더 선호하는 방법이라 평가된다.           요약     \t손과 물체의 물리적 접촉은 human grasps에서 중요한 역할을 한다. hand mesh와 object mesh가 주어졌을 때, ground truth를 통해 딥 모델이 mesh 표면에 걸쳐 바람직한 contact 결과를 추론하도록 한다. 이후 ContactOpt를 이용하여 손 포즈를 효율적으로 최적화함으로써 손 포즈 추론 결과를 개선할 수 있음을 보였다. 이러한 방법은 더 좋은 일치율을 보이며, 더 낮은 kinematic error를 가지고, 인간 참여자가 더 선호하는 방법이라는 점에서 장점을 갖는다.          ManipulaTHOR: A Framework for Visual Object Manipulation  Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi    (link)   배경      Embodied AI 도메인은 특히 환경 내에서 agents를 탐색하는 데(navigating)에서 상당한 발전이 있었다.   이러한 초기 성공은 agent가 그들의 환경 내의 물체와 적극적으로 상호작용해야하는 작업을 커뮤니티가 처리할 수 있도록 building blocks을 마련하였다.   물체 조작(object manipulation)은 로봇 공학 커뮤니티 내에서 확립된 연구 영역으로, 조작기 동작(manipulator motion), 잡기(grasping), 그리고 긴 수평선 계획(long-horizon planning)을 포함한 여러 문제를 제기한다. 특히 이러한 문제는 시각적으로 풍부하고 복잡한 장면, 모바일 에이전트를 이용한 조작(탁상조작(tabletop manipulation)과 반대되는 말), 그리고 보이지 않는 환경 및 물체에 대한 일반화와 관련되어 자주 간과되는 실제 설정과 연관되어 있다.   해결      물리적 기반의 시각적으로 풍부한 AI2-THOR 프레임워크를 기반으로 구축된 object manipulation을 위한 프레임워크를 제안하고, ArmPointNav로 알려진 Embodied AI 커뮤니티에 새로운 도전을 제시한다.   이러한 작업은 유명한 포인트 탐색 작업(point navigation task)을 물체 조작(object manipulation)으로 확장하며, 3차원 장애물 회피, 폐색(occlusion)이 있는 상태에서 개체 조작, 그리고 장기적인 계획이 필요한 다중 객체 조작을 포함한 새로운 과제를 제공한다.   결론      PointNav 챌린지에서 성공한 인기있는 학습 패러다임은, 가능성을 보여주었지만 개선의 여지가 많다.           요약     \t손과 물체의 물리적 접촉은 human grasps에서 중요한 역할을 한다. hand mesh와 object mesh가 주어졌을 때, ground truth를 통해 딥 모델이 mesh 표면에 걸쳐 바람직한 contact 결과를 추론하도록 한다. 이후 ContactOpt를 이용하여 손 포즈를 효율적으로 최적화함으로써 손 포즈 추론 결과를 개선할 수 있음을 보였다. 이러한 방법은 더 좋은 일치율을 보이며, 더 낮은 kinematic error를 가지고, 인간 참여자가 더 선호하는 방법이라는 점에서 장점을 갖는다.          👻 AR    \t차례              Stay Positive: Non-Negative Image Synthesis for Augmented Reality \t\tHDR Environment Map Estimation for Real-Time Augmented Reality     \tNeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering using RGB Cameras        Stay Positive: Non-Negative Image Synthesis for Augmented Reality  Katie Luo, Guandao Yang, Wenqi Xian, Harald Haraldsson, Bharath Hariharan, Serge Belongie    (link)   배경           광학 투시(optical see-through) 및 프로젝터 증강 현실과 같은 애플리케이션에서, 이미지를 생성하는 것은 기존의 이미지에만 빛을 추가할 수 있는 비음성(non-negative) 이미지 생성 문제를 해결하는 것과 같다.       그러나 대부분의 이미지 생성 기법은, 각 픽셀에 임의의 색상을 할당할 수 있다고 가정하기 때문에 이러한 문제 설정에 적합하지 않다.            사실, 기존 방법의 순진한 적용은 빛을 추가함으로써 더 어두운 픽셀을 생성할 수 없기 때문에 MNIST digits와 같이 단순한 도메인에서도 실패한다.       해결           인간의 시각 시스템은 밝기와 대비의 특정 공간 구성을 포함한 착시 현상에 속을 수 있다. 이러한 동작을 활용하여 무시할 수 있는 인공물(artifacts)로 고품질 이미지를 생성할 수 있다는 점이 주요 인사이트이다.       예를 들어, 주변의 픽셀을 밝게 만들어 더 어두운 패치의 환상을 만들 수 있다.            semantic과 non-negativity constraints 모두를 충족하는 이미지를 생성하기 위한 신박한 최적화 절차를 통해 문제를 해결할 수 있다.       결론      이러한 접근 방식은 기존의 최첨단 방식을 통합할 수 있으며, image-to-image translation과 style transfer를 포함한 다양한 tasks에서 훌륭한 성능을 보여준다.           요약     \t증강현실과 같은 어플리케이션에서 이미지를 생성하기 위해서는 기존의 이미지에 빛만을 추가할 수 있다. 이러한 제약은 빛 추가를 통해 더 어두운 픽셀을 생성할 수 없다는 문제로 이어진다. 주변의 픽셀을 밝게 만들어 더 어둡게 보이도록 하는것과 같이 인간의 시각 시스템에 착시를 불러일으키는 최적화 절차를 통해 이러한 문제를 해결할 수 있다. 이러한 접근 방식은 기존의 방식을 통합할 수 있으며, 다양한 tasks에서 좋은 성능을 보였다.          HDR Environment Map Estimation for Real-Time Augmented Reality  Gowri Somanath, Daniel Kurz    (link)      배경      다양한 기하학적 물체 렌더링을 지원하기 위해서는 환경맵이 반드시 high dynamic range이어야 하며, 장면에서 물체와 특징을 표현하기 위해 충분히 고해상도 이미지를 가져야 한다.   해결      실시간으로 좁은 시야의 LDR 카메라 영상으로부터 HDR 환경 맵을 추정하는 방법을 제안한다.   이를 통해 증강 현실을 사용하여 실제 물리적 환경으로 렌더링된 거울(mirror)부터 확산(diffuse)까지, 모든 재료 마감의 가상 물체에 지각적으로 훌륭한 반사와 음영이 가능하다.   이러한 방식은 효율적인 CNN 아키텍처인 EnvMapNet을 기반으로 하며, 생성된 이미지에 대한 ProjectionLoss와 적대적인 교육에 대한 ClusterLoss라는 두 가지 신박한 losses와 함께 end-to-end로 학습된다.   결론      최첨단 방식과의 정성적·정량적 비교를 통해, 이러한 알고리즘이 추정된 광원의 방향 오차를 50% 이상 감소시키고, 3.7배 더 낮은 Frechet Inception Distance(FID)를 달성함이 입증되었다.   또한, iPhone XS 환경에서 9ms 미만으로 이러한 NN 모델을 실행시킬 수 있으며, 이전까지 실제 환경에서 볼 수 없었던 시각적으로 일관된 가상 객체를 실시간으로 렌더링할 수 있는 모바일 어플리케이션을 선보였다.           요약     \t다양한 기하학적 물체를 렌더링하기 위해서는 HDR이어야 하며 충분히 고해상도 이미지를 가져야 한다. EnvMapNet을 기반으로 하며 ProjectionLoss와 ClusterLoss라는 두 가지 신박한 losses를 통해 좁은 시야의 LDR영상으로부터 HDR 환경 맵을 추정하는 방법을 통해 이러한 문제를 해결한다. 이러한 방식은 추정된 광원의 방향에 대한 오차를 50%이상 감소시켰으며, 3.7배 더 낮은 Frechet Inception Distance를 달성하였다.          NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering using RGB Cameras  Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, Kaiwen Guo, Minye Wu, Lan Xu    (link)      배경      human activities의 4차원 재구성 및 렌더링은 몰입형 VR/AR 경험을 위해 매우 중요하다.   최근 발전은 여전히 sparse multi-view RGB 카메라의 입력 이미지에 존재하는 세부 수준으로 정밀한 형상(geometry)과 텍스처 결과를 복구하지 못한다.   해결: NeuralHumanFVV      NeuralHumanFVV는 임의의 새로운 관점에서 human activities에 대한 고품질 기하학과 사실적인 텍스처 모두를 생성하기 위한, real-time neural human performance 캡쳐 및 렌더링 시스템이다.   실시간 내재된 기하학 추론을 위한 계층적 샘플링 전략뿐만 아니라, 새로운 관점에서 고해상도(e.g., 1k) 및 사실적인 텍스처 결과를 생성하는 새로운 신경 혼합 방식(neural blending scheme)를 제안한다.   또한, neural normal blending을 채택하여, 기하학적 디테일을 향상시키고 이러한 신경 기하학 및 텍스처 렌더링을 다중 작업 학습 프레임워크로 공식화하였다.   결론      광범위한 실험을 통해, 이러한 접근 방식이 도전적인 human performances를 위한 고품질 기하학 및 사실적인 자유로운 시점 구성을 달성할 수 있음을 보여주었다.           요약     \thuman activities의 4차원 재구성 및 렌더링은 몰입형 AR/VR 경험에 매우 중요하다. 하지만 여전히 sparse multi-view RGB 카메라를 통해서는 정밀한 geometry와 텍스처 결과를 복구할 수 없다. 임의의 새로운 관점에서 human activities에 대한 고품질 기하학과 사실적인 텍스처 모두를 생성할 수 있는 NeuralHumanFVV를 통해 이러한 문제를 해결하였다.            다음 포스팅에서 3D vision 주제에 관한 논문들을 다루도록 한다.       References      CVPR 2021: http://cvpr2021.thecvf.com/   brunch/kakao-it: https://brunch.co.kr/@kakao-it/297   CVPR-2021-Paper-Satistics : https://github.com/hoya012/CVPR-2021-Paper-Statistics?fbclid=IwAR0MGG3x-9bDU8YjVp-UGHcJDAXUspNwJ3Iy-o17oi7UFbTSFGcKS_OqbaQ   52CV/CVPR-2021-Papers: https://github.com/52CV/CVPR-2021-Papers   amusi/CVPR2021-Papers-with-Code: https://github.com/amusi/CVPR2021-Papers-with-Code       ","categories": ["ai-computerVision"],
        "tags": ["Computer Vision","CVPR","Papers","Review"],
        "url": "/ai-computervision/CVPR202102/",
        "teaser": null
      },{
        "title": "[CVPR2021-03] CVPR2021-3D Vision 논문 훑기",
        "excerpt":"  CVPR(IEEE Conference on Computer Vision and Pattern Recognition)은 컴퓨터 비전과 패턴 인식 분야에서 세계적으로 최고 수준의(Top-tier) 학회이다. 이번 포스팅에서는 2021년도 CVPR 학회에서 3D Vision 주제에 관한 몇 가지 논문들을 Abstract 위주로 살펴본다.       관심 논문   내가 관심있는 주제에 관련된 몇 가지 논문을 abstract 위주로 살펴본다.   3D(3차원 비전)    \t차례              A Deep Emulator for Secondary Motion of 3D Characters \t\tNeural Deformation Graphs for Globally-consistent Non-rigid Reconstruction     \tDeep Implicit Templates for 3D Shape Representation                 SMPLicit: Topology-aware Generative Model for Clothed People         Picasso: A CUDA-based Library for Deep Learning over 3D Meshes         Semi-supervised Synthesis of High-Resolution Editable Textures for 3D Humans         RGB-D Local Implicit Function for Depth Completion of Transparent Objects         Deep Two-View Structure-from-Motion Revisited         Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence         S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling         Plan2Scene: Converting Floorplans to 3D Scenes         Shelf-Supervised Mesh Prediction in the Wild         Unsupervised Learning of 3D Object Categories from Videos in the Wild        A Deep Emulator for Secondary Motion of 3D Characters  Mianlun Zheng, Yi Zhou, Duygu Ceylan, Jernej Barbič (link)      배경      3차원 캐릭터를 애니메이션화하기 위한 빠르고 가벼운 방법은 컴퓨터 게임과 같은 다양한 애플리케이션에 적합하다.   해결           생생한 2차 모션 효과로 3D 캐릭터의 skinning 기반 애니메이션을 향상시키는 학습 기반 접근 방식을 제안한다.            인접 정점(vertices) 사이의 내부 힘을 암시적으로 인코딩하는 캐릭터 시뮬레이션 메쉬의 각 로컬 패치를 인코딩하는 신경망을 설계하였다.       이 네트워크는 캐릭터 역학의 상미분 방정식(ordinary differential equations)을 emulate하여, 현재의 가속도, 속도, 그리고 위치로부터 새로운 정점(vertex)의 위치를 예측한다.       로컬 방식이므로, 이러한 네트워크는 mesh topology에 독립적이며, 테스트시 임의의 모양을 갖는 3D character meshes로 일반화된다.            정점(vertex)당 제약 조건과 강성(stiffness)과 같은 재료 속성을 통해, mesh의 서로 다른 부분에서 역학을 쉽게 조정할 수 있다.       결론      이러한 방식을 다양한 캐릭터 메쉬와 복잡한 모션 시퀀스에 대해 평가하였다.   실제(ground-truth) 물리적 기반 시뮬레이션에 비해 30배 더 효율적일 수 있으며, 빠른 근사치를 제공하는 대체 솔루션보다 성능이 뛰어남을 보였다.        요약  컴퓨터 게임과 같은 애플리케이션에서는 빠르고 가벼운 3D 캐릭터 애니메이션 기법이 요구된다. 이에, 캐릭터 역학의 상미분 방정식을 에뮬레이트하여, 현재의 가속도, 속도, 그리고 위치 정보로부터 새로운 정점의 위치를 예측하는 로컬 방식의 네트워크를 제안한다. 이를 통해 정점당 제약 조건과 강성과 같은 재료 속성을 통해 역학을 쉽게 조정할 수 있다.       Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction  Aljaž Božič, Pablo Palafox, Michael Zollhöfer, Justus Thies, Angela Dai, Matthias Nießner (link)      배경      단단하지 않은 변형 표면을 캡처하는 것은 종종 매우 역동적인 실제 환경을 재구성하고 이해하는 데 필수적이다.   정적 3D 씬(scene) 재구성에 인상적인 발전이 이루어졌지만, 동적 추적 및 재구성은 여전히 매우 어려운 과제이다.   해결           전역적으로 일관된(globally-consistent) 변형 추적(deformation tracking) 및 비강체(non-rigid) 객체의 3D 재구성을 위한 Neural Deformation Graphs를 제안한다.       특히 심층 신경망(deep neural network)을 통해 변형 그래프를 암시적으로 모델링한다.            신경 변형 그래프(neural deformation graph)는 어떤 객체의 특정 구조에 의존하지 않으므로, 일반 비강성 변형 추정에 적용할 수 있다.       움직이지 않는 물체(non-rigidly moving)의 주어진 깊이 카메라 관찰 시퀀스에서 이러한 신경 그래프를 전역적으로 최적화한다.       명시적 관점에서의 일관성과 프레임간 그래프 및 표면 일관성 제약을 기반으로, 기본 네트워크가 자체 감독(self-supervised) 방식으로 학습된다.            암시적으로 변형 가능한 다중 MLP 모양 표현을 사용하여, 객체의 기하학에 대해 추가로 최적화한다.       결론      순차적 입력 데이터를 가정하지 않으므로, 빠른 동작 또는 일시적으로 연결이 끊긴 녹음에 대해 강력한 추적을 가능하게 해준다.   실험을 통해, Neural Deformation Graphs가 64% 개선된 재구성과, 62% 개선된 변형 추적 성능임을 증명했다. 이는, 질적 및 양적으로 최첨단 비강체 재구성 접근 방식을 능가함을 보여주는 결과이다.        요약  단단하지 않은 변형 표면을 캡처하는 것은 역동적인 실제 환경을 재구성하는 데에 필수적이지만, 여전히 어려운 과제이다. Neural Deformation Graphs는 명시적 관점에서 일관성과 프레임간 그래프 및 표면 일관성 제약을 기반으로 self-supervised 방식으로 학습되는 기본 네트워크를 포함한다. 순차적인 입력 데이터를 가정하지 않으므로 빠른 동작을 가능하게 한다.       Deep Implicit Templates for 3D Shape Representation  Zerong Zheng, Tao Yu, Qionghai Dai, Yebin Liu (link)      배경           일종의 3D 모양 표현인 Deep implicit functions (DIFs)은 컴팩트함과 강력한 표현력덕분에 3D vision 커뮤니티에서 점점 더 대중화되고 있다.       그러나 폴리곤 메쉬 기반 템플릿(polygon mesh-based templates)과 달리, DIF로 표현되는 모양 전반에 걸쳐 dense correspondences 혹은 다른 의미론적 관계(semantic relationships)를 추론하는 것은 여전히 도전 과제로 남아있으며, 이는 텍스처 전송, 모양 분석 등에 적용하는 것을 제한한다.       해결           이러한 한계를 극복하고 DIF를 더 해석하기 쉽게 만들기 위해, deep implicit representations에서 명시적 대응 추론을 지원하는 Deep Implicit Templates라는 새로운 3D 모양 표현 방법을 제안한다.            핵심 아이디어는, DIF를 template implicit function의 조건부 변형(conditional deformations)으로 공식화하는 것이다.       이를 위해, 조건부 공간 변환(conditional spatial transformation)을 다중 아핀 변환(multiple affine transformations)으로 분해하고, 일반화 능력을 보장하는 공간 왜곡 LSTM(Spatial Warping LSTM)을 제안한다.            training loss는 unsupervised 방식에서 정확한 대응과 그럴듯한 템플릿을 학습하면서, 높은 재구성 정확도를 달성하기 위해 신중하게 설계되었다.       결론      실험을 통해, 이러한 방식이 모양 컬렉션에 대한 공통적인 implicit template을 학습할 수 있을 뿐만 아니라, 어떠한 supervision 없이도 모든 모양에 대해 동시에 조밀한 대응을 설정할 수 있음을 밝혔다.        요약  3D vision 분야에서 Deep implicit funtions는 컴팩트함과 강력한 표현력으로 인해 대중화되고있다. 하지만 DIF로 표현되는 모양 전반에 걸쳐 조밀한 대응(dense correspondences)과 의미론적 관계를 추론하는 것은 여전히 도전 과제이다. 이러한 한계를 극복하고자 DIF를 조건부 변형으로 공식화하는 Deep Implicit Templates라는 새로운 3D 모양 표현 방법을 제안하여 이러한 문제를 해결하였다.       SMPLicit: Topology-aware Generative Model for Clothed People  Enric Corona, Albert Pumarola, Guillem Alenyà, Gerard Pons-Moll, Francesc Moreno-Noguer    (link)      배경      다양한 신체 형태와 포즈로 의복 스타일과 변형을 제어할 수 있는 차별화된 저차원 생성 모델 구축을 통해, 옷을 입은 사람의 디지털 애니메이션, 3D 콘텐츠 제작 및 가상 시승과 같은 여러 가지 흥미로운 애플리케이션에 문을 열 수 있다.   해결: SMPLicit           신체 포즈, 모양, 그리고 옷의 기하학을 공동으로 표현하는 신박한 생성 모델인 SMPLicit을 제안한다.            각 유형의 의류에 대해 특정 모델을 학습해야하는 기존의 학습 기반 방식과 달리, SMPLicit은 의류의 사이즈나 타이트함/루즈함과 같은 다른 특성들을 제어하면서 다양한 의류 topologies(e.g., 민소매옷부터 후드티, 오픈 재킷까지)를 통일된 방식으로 표현할 수 있다.            이러한 모델을 티셔츠, 후드티, 재킷, 반바지, 바지, 스커트, 신발, 심지어 헤어를 포함한 다양한 의복에 적용할 수 있음을 보였다.            SMPLicit의 표현 유연성(flexibility)은 SMPL 인체 매개변수와 의미론적으로 해석가능하며 의류 속성으로 정렬된 학습 가능한 잠재 공간으로 조건화된 암시적 모델(implicit model)을 기반으로 한다.       제안된 모델은 완전히 미분가능하므로, 더 큰 end-to-end 학습 가능한 시스템에 적용할 수 있다.       결론           실험을 통해, SMPLicit이 3D 스캔을 피팅하고, 옷 입음 사람의 이미지에서 3D 재구성에 쉽게 사용할 수 있음을 보여준다.       두 경우 모두, 복답한 의복의 형상을 검색하고, 여러 겹의 의복이 있는 상황을 처리하고, 손쉬운 의상 편집을 위한 도구를 제공함으로써 최첨단 기술을 뛰어넘을 수 있다.            이러한 방향의 추가 연구를 위해서, [여기](http://www.iri.upc.edu/people/ecorona/smplicit/에 있는 코드와 모델을 공개적으로 사용할 수 있다.               요약     \t신체 포즈, 모양, 그리고 옷의 기하학을 표현하는 신박한 생성 모델인 SMPLicit을 통해 다양한 애플리케이션에 문을 열 수 있을 것이라 기대된다. 기존에는 각 유형의 의류에 대해 특정 모델을 학습해야했지만, SMPLicit은 다양한 의류 topologies를 통일된 방식으로 표현할 수 있다.          Picasso: A CUDA-based Library for Deep Learning over 3D Meshes  Huan Lei, Naveed Akhtar, Ajmal Mian    (link)   배경           계층 신경 아키텍처(Hierarchical neural architectures)는 빠른 mesh decimation의 필요성을 나타내는 다중 스케일 추출에서 효과적이라 입증되었다.       그러나 기존의 방법은 다중 해상도 메쉬를 얻기 위해 CPU 기반 구현에 의존한다.       해결: Picasso           복잡한 실제 3D 메쉬에 대한 딥러닝을 위한 새로운 모듈로 구성된 CUDA 기반 라이브러리인 Picasso를 소개한다. 이처럼 GPU 가속 mesh decimation을 설계하여, 네트워크 해상도 감소를 효율적으로 즉각 촉진할 수 있다.            Pooling 및 unpooling 모듈은 decimation 동안 수집된 정점 클러스터(vertex clusters)로 정의된다.            Picasso는, 메쉬에 대한 특징 학습에 대해, 소위 facet2vertex, vertex2facet, 그리고 facet2facet convolution이라 불리는 세 가지 유형의 새로운 컨볼루션을 포함한다.       따라서 mesh를 기존의 방식처럼 모서리(edges)가 있는 공간 그래프가 아니라 꼭짓점(vertices)과 면(facets)으로 구성된 기하학적 구조로 취급한다.            Picasso는 메쉬 샘플링 (정점 밀도)에 대한 robustness를 위해 필터에 fuzzy mechanism을 통합한다.       Gaussian mixtures를 활용하여 facet2vertex convolution에 대한 fuzzy 계수를 정의하고, 무게 중심 보간(varycentric interpolation)을 활용하여 나머지 두 컨볼루션에 대한 계수를 정의한다.       결론      이 release에서는, S3DIS에 대한 경쟁력있는 세분화 결과와 함께 제안된 모듈의 효율성을 보여주었다.   해당 라이브러리는 여기에서 공개하였다.           요약     \t계층 신경 아키텍처는 다중 스케일 추출에서 효과적이라 입증되었다. 하지만 기존의 방법은 CPU 기반으로 구현되었다. 이에, 복잡한 실제 3D mesh에 대한 딥러닝을 위한 새로운 모듈로 구성된 Picasso라는 CUDA GPU기반 라이브러리를 제안하였다.          Semi-supervised Synthesis of High-Resolution Editable Textures for 3D Humans  Bindita Chaudhuri, Nikolaos Sarafianos, Linda Shapiro, Tony Tung    (link)      배경      최근 AR/VR 기기 및 가상 커뮤니케이션의 사용이 증가함에 따라 3D Human Avatar 제작이 인기를 얻고 있다.   인체는 모양을 모델링한 3D 표면 메시와 3D 표면에 매핑된 모양을 인코딩한 텍스처 맵(UV 공간의 이미지)으로 표현되며, 이때 몰입적인 경험을 위해서는 아바타의 사실적인 질감이 매우 중요하다.   해결           semi-supervised 설정에서 3D human meshes에 대한 다양한 고충실도(high fidelity) 텍스처 맵을 생성하는 새로운 접근 방식이다.            텍스처맵에서 의미 영역(semantic regions)의 레이아웃을 정의하는 분할 마스크(segmentation mask)가 주어졌을 때, 제안된 네트워크는 렌더링을 목적으로 사용되는 다양한 스타일을 갖는 고해상도 텍스처를 생성한다.       이를 달성하기 위해, 각 영역의 스타일 확률 분포를 개별적으로 학습하여 지역별 분포로부터 샘플링하여 생성된 텍스처의 스타일을 제어할 수 있는 Region-adaptive Adverserial Variational AutoEncoder(ReAVAE)를 제안한다.            또한, single-view RGB 입력으로부터 가져온 데이터로 학습 세트를 보강(augment)할 수 있는 데이터 생성 기술을 제안한다.       이러한 학습 전략은 참조 이미지 스타일을 다양한 영역에 대한 임의의 스타일과 혼합할 수 있도록 하며, 이는 가상 체험 AR/VR 어플리케이션에 유용한 속성이다.       결론      실험을 통해, 이러한 방식이 이전의 작업과 비교했을 때 더 나은 텍스처맵을 합성함과 동시에 독립적인 레이아웃과 스타일 제어 능력을 가능하게함을 보였다.           요약     \t레이아웃을 정의하는 분할 마스크가 주어졌을 때, 다양한 스타일을 갖는 고해상도 텍스처를 생성하는 접근 방식을 제안한다. 각 스타일 확률 분포를 개별적으로 학습하여, 생성된 텍스처의 스타일을 제어할 수 있는 Region-adaptive Adverserial Variational AutoEncoder(ReAVAE)를 통해 달성할 수 있다. 또한 signle-view RGB 입력으로부터 학습 세트를 보강할 수 있는 데이터 생성 기술을 제안한다. 이를 통해 더 나은 텍스처맵을 합성할 수 있으며, 독립적인 레이아웃과 스타일 제어 능력을 보여주었다.          RGB-D Local Implicit Function for Depth Completion of Transparent Objects  Luyang Zhu, Arsalan Mousavian, Yu Xiang, Hammad Mazhar, Jozef van Eenbergen, Shoubhik Debnath, Dieter Fox    (link)      배경      로봇공학에서 대부분의 인지(perception) 방법은 RGB-D 카메라에 의한 깊이 정보를 필요로 한다. 그러나, 표준 3D 센서는 빛의 굴절(refraction)와 흡수(absorption) 문제로 인해 투명한 물체에 대한 깊이 정보를 취득하지 못한다.   해결           접근 방식의 핵심은 제안된 방식이 보이지 않는 물체를 일반화하고 빠른 추론 속도를 달성하도록 하는 ray-voxel pairs 기반 local implicit neural representation이다. 이러한 표현법에 기반하여, 노이지한 RGB-D 입력이 주어졌을 때, 누락된 깊이 정보를 완성할 수 있는 신박한 프레임워크를 제안한다.       또한, self-correcting refinement model을 사용하여 반복적으로 깊이 추정을 더욱 개선할 수 있다.            전체 파이프라인을 학습하기 위해, 투명한 객체를 갖는 대규모 합성 데이터 세트를 구축하였다.       결론      실험을 통해, 이러한 방법이 합성 데이터와 실제 데이터 모두에서 최신 최첨단 기법을 능가하는 성능을 보임을 밝혔다. 또한, 이전의 가장 뛰어난 방법인 ClearGrasp와 비교했을 때 추론 속도를 20배 향상시킴을 보였다.   코드와 데이터세트는 여기에서 확인할 수 있다.           요약     \t로봇 공학에서 대부분의 인지 기법은 깊이 정보를 필요로하지만, RGB-D 카메라의 한계로 인해 투명한 물체에 대한 깊이 정보는 취득하지 못한다. 이에 ray-voxel pairs 기반 local implicit neural representation을 통해 누락된 깊이 정보를 완성할 수 있는 프레임워크를 제안한다. 실험을 통해 해당 방법이 합성 데이터와 실제 데이터 모두에서 뛰어남을 증명하였다.          Deep Two-View Structure-from-Motion Revisited  Jianyuan Wang, Yiran Zhong, Yuchao Dai, Stan Birchfield, Kaihao Zhang, Nikolai Smolyanskiy, Hongdong Li    (link)      배경      Two-view structure-from-motion(SfM)은 3D 재구성 및 시각적 SLAM의 초석이다.   기존의 딥러닝 기반 방식은 이러한 문제를 두 개의 연속 프레임으로 부터 절대적인 포즈 스케일을 복구하거나, 단일 이미지로부터 깊이 맵을 예측하는 방식으로 해결했다. 하지만 두 방법 모두 잘못된 문제이다.   해결      고전적인 파이프라인의 좋은 자세(well-posedness)를 활용하여 깊은 두 시점 SfM의 문제를 재검토할 것을 제안한다.   제안된 방법은 다음으로 구성된다.            두 프레임 사이의 조밀한 대응을 예측하는 광류(optical flow) 추정 네트워크       2D 광류 대응으로부터 상대적인 카메라 포즈를 계산하는 정규화된 포즈 추정 모듈       탐색 공간을 줄이고, 조밀한 대응을 정제하고, 상대적인 깊이맵을 추정하기위해 에피폴라 기하학(epipolar geometry)을 활용하는 스케일 불변 깊이 추정 네트워크           결론      광범위한 실험을 통해, 제안된 방법이 상대적 포즈와 깊이 추정 모두에 대해 KITTI depth, KITTI VO, MVS, Scenes11, 그리고 SUN3D 데이터세트에 대한 명확한 마진으로 모든 최첨단 두 시점 SfM 방법을 능가함을 보였다.           요약     \tSfM은 3D 재구성과 visual SLAM의 초석이다.그러나  기존의 딥러닝 기반 방법은 SfM 문제를 잘못해결했으며, 이에 고전적인 파이프라인 well-posedness를 활용하여 SfM 문제를 재검토할 것을 제안한다. 실험을 통해 상대적 포즈와 깊이 추정 모두에서 최첨단 방법을 능가했음을 보였다.          Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence  Yu Deng, Jiaolong Yang, Xin Tong    (link)      배경      동일한 클래스의 3D 객체는 몇 가지 공통 모양 특징과 의미적 대응성을 공유한다. 따라서 형상 이해(shape understanding), 재구성(reconstruction), 조작(manipulation), 이미지 합성(image synthesis)과 같은 3D 및 2D 영역에서 다양한 다운스트림 작업에 유용한 변형 형상 모델(deformable shape model)을 구축하는 데 사용될 수 있다.   해결: Deformed Implicit Field(DIF)      카테고리의 3D 모양을 모델링하고 모양간의 dense correspondences를 생성하기 위한 새로운 Deformed Implicit Field(DIF) 표현법을 제안한다.   DIF를 이용하여, 3D 모양을 각 모양 인스턴스에 대한 3D 변형 필드 및 수정 필드와 함께 카테고리 전체에서 공유되는 template implicit field로 표현할 수 있다. 모양 일치는 변형 필드를 사용하여 쉽게 설정할 수 있다.   DIF-Net이라 불리는 제안된 신경망은, 어떠한 대응이나 부분 라벨도 없이 카테고리에 속한 3D 객체에 대한 모양 잠재 공간과 이러한 필드를 공동으로 학습한다. 학습된 DIF-Net은 또한 모양 구조 불일치를 반영하는 신뢰할 수 있는 대응 불확실성 측정을 제공할 수 있다.   결론      실험을 통해, DIF-Net이 충실도가 높은 3D 모양을 생성할 뿐만 아니라 다양한 모양에 걸쳐 고품질의 조밀한 대응 관계를 구축함을 보였다.   이러한 방법이 이전 방법으로는 달성할 수 없었던 텍스처 전송(texture transfer) 및 모양 편집(shape editing)과 같은 여러 응용 프로그램을 시연함을 보였다.           요약     \t동일한 클래스의 3D 객체는 변형 형상 모델(deformable shape model)을 구축하는 데에 사용될 수 있다. 카테고리에서 3D 모양을 모델링하고 모양간 dense correspondences를 생성하기 위한 새로운 Deformed Implicit Field(DIF) 표현법을 통해, 카테고리 전체에서 공유되는 3D 모양을 생성할 뿐만 아니라 다양한 모양에 걸쳐 고품질의 dense correspondences를 구축할 수 있다.          S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling   Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, Raquel Urtasun (link)      배경      인간을 구성하고 애니메이션에 적용하는 것은 VR 혹은 시뮬레이션에서 로봇공학 테스트와 같이 다양한 응용 분야에서 가상 세계를 구축하는 데에 중요한 구성 요소이다. 다양한 모양, 자세, 그리고 옷을 가진 인간의 변형이 기하급수적으로 많으므로, 실제 데이터로부터 대규모로 인간을 자동으로 재구성하고 애니메이션할 수 있는 방법을 개발하는 것은 중요하다.   해결           보행자의 모양, 자세, 그리고 스키닝 가중치(skinning weights)를 데이터로부터 직접적으로 학습된 neural implicit functions로 표현하는 방법을 제안한다.       이러한 표현 방법은 human paremetric body model에 명시적으로 맞추지(fit) 않으면서 다양한 보행자의 모양과 자세를 처리할 수 있으므로, 더 넓은 범위의 인간 기하학과 토폴로지를 처리할 수 있다.       결론      다양한 데이터 세트에 대해 이러한 접근 방식이 효율적임을 보였다.   이러한 재구성이 기존의 최첨단 방법을 능가함을 보였다.   재애니메이션(re-animation) 실험을 통해, 단일 RGB 이미지 (및/또는 선택적인 LiDAR sweep)을 입력으로부터 3D 인간 애니메이션을 대규모로 생성할 수 있음을 보였다.        요약      단일 RGB 이미지 (및/또는 optional LiDAR sweep) 입력으로 부터 보행자의 모양, 자세, 그리고 skinning weights을 직접적으로 학습하여 neural implicit function으로 표현하는 방법을 제안하였다.       Plan2Scene: Converting Floorplans to 3D Scenes  Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa, Angel X. Chang, Manolis Savva (link, project)      해결: Plan2Scene      Plan2Scene은 평면도와 주택 관련 사진 세트를 질감이 있는 3D mesh 모델로 변환하는 작업이다.   제안된 시스템은 1) 평면도 이미지를 3D mesh 모델로 들어올리며(lift), 2) 입력 사진을 기반으로 표면 질감을 합성하고, 3) 그래프 신경망 아키텍처를 사용하여 관찰되지 않은 표면에 대한 텍스처를 추론한다.   시스템을 훈련하고 평가하기 위해 실내 표면 텍스처 데이터 세트를 만들고, 수정된 표면 작물(surface crops) 및 추가적인 주석을 통해 이전 작업의 평면도 및 사진 데이터 세트를 보강(augment)한다.   결론      제안된 접근 방식은 거주지를 일부가 덮인 정렬되지 않은 드문드문한 사진 세트로부터 바닥, 벽, 그리고 천장과 같은 지배적인 표면에 대해 타일링 가능한 텍스처를 생성하는 문제를 처리한다.   정성적·정량적 평가를 통해, 이러한 시스템이 사실적인 3D 인테리어 모델을 생성하고, 텍스처 품질 메트릭 세트에 대한 baseline 접근 방식을 능가함을 보였다.        요약      평면도 사진과 거주지의 일부 사진세트로부터 바닥, 벽, 그리고 천장과 같은 타일링 가능한 텍스처를 생성하는 시스템을 제안한다. 이를 통해 사실적인 3D 인테리어 모델을 생성할 수 있으며, 텍스처 품질 메트릭 세트에 대한 baseline 접근 방식을 능가할 수 있다.       Shelf-Supervised Mesh Prediction in the Wild  Yufei Ye, Shubham Tulsiani, Abhinav Gupta (link, project)      배경      대부분의 야생에서의 컴퓨터 비전 시스템은 여전히 2D 의미 인식(분류/탐지)(2D semantic recognition)을 수행한다.   최근 2D 인식의 발전은 지도 학습(supervised learning)에서 비롯되었지만, 2D semantic tasks와 달리 3D understanding에 대해 감독(supervision)을 받는 것은 여전히 확장 가능하지 않다.   최근 2D 접근방식의 감독을 갖는 3D 접근 방식을 구축하려는 시도가 있었으나, 초기 접근법은 다중 뷰 감독을 사용하는 것에 초점을 맞추었다. 그러나 동일한 개체/장면에 대한 다중 뷰를 얻는 것은 쉽지 않다는 문제가 있다.   해결           단일 이미지로부터 객체의 3D 모양과 자세을 추론하는 것을 돕고, off-the-shelf 인식 시스템(i.e. ‘shelf-supervised’)의 분할 출력만으로 감독되는 구조화되지 않은 이미지 컬렉션으로부터 학습할 수 있는 학습기반 접근방식을 제안한다.            먼저, 카메라 포즈와 함께 표준 프레임에서 체적 표현(volumetric representation)을 추론한다. 모양과 마스크 모두와 기하학적으로 일치하는 표현을 시행하고, 합성된 새로운 뷰가 이미지 컬렉션과 구별되지 않도록 한다. 거친 체적 예측(the coarse volumetric predictions)은 메쉬 기반 표현으로 변환되어, 예측된 카메라 프레임에서 더욱 세분화된다.       이러한 두 단계를 통해 이미지 컬렉션의 모양-포즈 인수분해와 인스턴스별 재구성을 보다 세부적으로 수행할 수 있다.       결론      합성 데이터 세트와 실제 데이터 세트 모두에서 방법을 조사하고, 기존 작업보다 훨씬 더 많은 클래스인 야생에서의 50가지 범주에서 확장성을 보였다.        요약      기존의 2D 접근 방식의 감독을 갖는 3D 접근 방식은 다중 뷰 감독을 사용하는 것에 초점을 맞추었지만, 동일한 개체/장면에 대해 다중 뷰를 갖는 것은 쉽지 않다. 단일 이미지로부터 3D 모양과 자세를 추론하는 것을 돕고, off-the-shelf 인식 시스템의 분할 출력만으로 감독되는 학습 기반 3D 접근 방식을 통해 이러한 문제에 접근할 수 있다. 실험 결과, 기존 작업보다 훨씬 더 많은 클래스에 대해 확장성을 보였다.       Unsupervised Learning of 3D Object Categories from Videos in the Wild  Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman Shapovalov, Tobias Ritschel, Andrea Vedaldi, David Novotny (link)      배경      최근 합성 데이터를 사용하거나 키포인트와 같은 2D 기본 요소가 사용가능하다고 가정하여 유사한 결과를 얻었다.   해결      논문은 수동적인 주석 없이 도전적인 실제 데이터로 학습하는 방식에 주목했다. 따라서 대규모 객체 인스턴스 컬렉션의 다중 뷰로부터 모델을 학습하는 것에 중점을 두었다.   주어진 카테고리의 객체에 대한 적은 수의 이미지가 주어졌을 때, 그것을 3D로 재구성하는 심층 네트워크를 학습하는 것을 목표로 한다.   모델들의 이러한 클래스를 훈련하고 벤치마킹하는 데에 적합한 객체 중심 비디오의 새로운 대규모 데이터 세트를 제공한다.   마지막으로 WCR(warp-conditional ray embedding)이라는 새로운 신경망 설계를 제안한다. WCR은 객체의 표면과 질감의 상세한 implicit representation을 얻는 동안 재구성을 크게 개선하고, 학습 프로세스를 부트스트랩(bootstrap)한 초기 SfM 재구성 노이즈를 보상한다.   결론      격리된 객체를 재구성하는 데에 잘 작동하는 메쉬, 복셀, 또는 암시적인 표면(implicit surfaces)을 활용하는 기존의 기술은 제안된 대규모 데이터 세트에 적합하지 않다.   기존 벤치마크와 새로운 데이터 세트에 대한 여러 심층 단안 재구성 기준선(deep monocular reconstruction baselines)에 대한 성능 향상을 보였다.        요약      최근의 3D reconstruction 문제는 합성 데이터를 사용하거나, 키포인트와 같은 2D 기본 요소가 사용가능하다 가정하여 문제를 해결하였다. 본 논문은 대규모 객체 인스턴스 컬렉션의 다중 뷰가 주어졌을 때, 수동적인 주석 없이 실제 데이터로 학습하는 도전적인 방법에 주목하였다. 주어진 카테고리에 대해 적은 수의 이미지가 주어졌을 때 이를 3D로 재구성하는 심층 네트워크로 학습하여 이를 달성한다.         ​         References      CVPR 2021: http://cvpr2021.thecvf.com/   brunch/kakao-it: https://brunch.co.kr/@kakao-it/297   CVPR-2021-Paper-Satistics : https://github.com/hoya012/CVPR-2021-Paper-Statistics?fbclid=IwAR0MGG3x-9bDU8YjVp-UGHcJDAXUspNwJ3Iy-o17oi7UFbTSFGcKS_OqbaQ   52CV/CVPR-2021-Papers: https://github.com/52CV/CVPR-2021-Papers   amusi/CVPR2021-Papers-with-Code: https://github.com/amusi/CVPR2021-Papers-with-Code       ","categories": ["ai-computerVision"],
        "tags": ["Computer Vision","CVPR","Papers","Review","3D Vision"],
        "url": "/ai-computervision/CVPR202103/",
        "teaser": null
      },{
        "title": "[Environment] Anaconda 가상환경 생성 및 VSCode 연동 방법",
        "excerpt":"2021.07.25 최초 작성        Anaconda 설정   아나콘다는 여기에서 다운로드받을 수 있다. (전부 Next 클릭)   세팅   아나콘다 버전 확인   conda --version       아나콘다 업데이트   conda update conda   가상환경 생성   가상환경 생성           문법:       conda create --name 가상환경명 설치할패키지                예시: project01라는 가상환경에 python 3.5 버전을 설치하는 경우       conda create --name project01 python=3.5            나는 보통 가상환경명을 프로젝트명으로 한다. 그래야 프로젝트 단위로 관리하기 쉽기 때문이다.           가상환경 활성화 및 비활성화   가상환경 리스트 확인   conda info --envs       가상환경 활성화           문법:       activate 가상환경명                예시: project01라는 가상환경을 활성화하는 경우       activate project01               가상환경 비활성화           문법:       deactivate 가상환경명                예시: project01라는 가상환경을 비활성화하는 경우       deactivate project01                 VScode에 Anaconda 연동   1. VScode 설치   VSCode 홈페이지에서 설치       2. Extensions   Extensions에서 Python과 Code Runner를 설치한 뒤, VScode 재실행       3. Interpreter 선택   ctrl+shift+p 누른 뒤, Python: Select Interpreter 선택   이후 원하는 아나콘다 가상환경 선택         기타 명령어   Python   패키지 버전 변경           예시: python을 3.5가 아닌 3.7.0버전으로 변경하고 싶은 경우       conda install pyton=3.7.0           라이브러리 설치           문법:       pip install 설치할라이브러리                예시:       matplotlib을 pip 패키지로 설치하려는 경우       pip install matplotlib           numpy를 pip 패키지로 설치하려는 경우       pip install numpy                     ","categories": ["etc-environment"],
        "tags": ["environment","anaconda","vscode"],
        "url": "/etc-environment/anaconda/",
        "teaser": null
      },{
        "title": "[ALGO-01] Lect1. 자료구조/알고리즘의 정의",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/              👉 자료구조, Data Structure   자료구조는 말 그대로 “자료의 구조”를 의미한다. 자료의 구조를 표현하기 위해서는 일반적으로 다음의 3가지 요소가 필요하다.      자료의 값 (Data values)   자료들 간의 관계 (Relationships among data values)   자료에 적용될 수 있는 연산 (Operations that can be applied to the data)   Fundamental data structures      데이터의 구조는 크게 기본 데이터 유형(Primitive)과 기본이 아닌 데이터 유형(Non-primitive)으로 나눌 수 있다.   1. 기본 데이터 유형(Primitive)   프로그래밍 언어에 의해 미리 정의된 데이터 타입을 의미한다. 변수 값의 크기와 유형이 미리 정의되어있다. 가령 C에서는 integer, float, character와 같은 primitive data가 존재한다.   2. 기본이 아닌 데이터 유형(Non-primitive)   프로그래밍 언어에 의해 정의되지 않고 프로그래머에 의해 생성된 데이터 타입이다. 데이터를 저장하는 메모리 위치를 참조하기 때문에 “참조 변수(reference varaibels)” 또는 객체 참조(object references)”라고도 불린다. Non-primitive data는 다음과 같이 세 가지로 분류할 수 있다.   2.1. 선형 데이터 구조, Linear   선형 데이터 구조는 데이터 요소가 순차 또는 선형으로 배열된 구조를 의미한다. 컴퓨터 메모리가 선형 방식으로 배열되어 있기 때문에 구현하기 쉽다는 특징이 있다. array, stack, queue, linked list는 선형 구조를 갖는다.           Stack vs. Queue                 stack과 queue 모두 차례로 쌓인 구조이지만, stack은 들어온 역 순으로 데이터가 읽히고 queue는 들어온 순서대로 데이터가 읽힌다는 점에 차이가 있다.         2.2. 비선형 데이터 구조, Non-linear   비선형 데이터 구조는 순차적 또는 선형으로 배열되지 않은 데이터 구조를 의미한다. 선형 데이터 구조에 비해 구현하기 쉽지 않지만, 컴퓨터 메모리를 더 효율적으로 사용할 수 있는 장점을 갖는다. 예로는 tree와 graph가 있다.           Tree vs. Graph                 tree는 두 정점 사이에 하나의 path만 존재하는 반면, graph는 둘 이상의 path가 허용된다. graph는 tree를 일반화한 것이라고 볼 수 있다.          2.3. 파일 데이터 구조, Files   파일 구조는 서로 관련있는 필드로 구성된 레코드 집합인 파일에 대한 자료 구조로, 보조 기억 장치에 데이터가 실제로 기록되는 형태이다. 예로 sequential file, index file, direct file이 있다.       Graph traversal algorithms   선형 데이터 구조에서 데이터 요소는 단일 실행에서만 순회할 수 있는 반면, 비선형 데이터 구조에서 데이터 요소는 다중 실행을 통해 순회가 가능하다. graph나 tree와 같은 비선형 데이터를 순회하는 대표적인 알고리즘에는 DFS와 BFS가 있다.      🗣  뒤에서 더욱 자세히 다루도록 한다.          👉 추상화, Abstraction   컴퓨터 과학에서 추상화(abstraction)는 복잡한 자료, 모듈, 시스템 등으로부터 핵심적인 개념 또는 기능을 간추려 내는 것을 말한다. 가령 경복궁을 찾아간다고 할 때, 아래의 왼쪽 그림(위성사진)보다는 오른쪽 그림(약도)이 더 보기 쉽다.      이처럼 필수 정보만 제공하고 세부 사항을 숨기는 프로세스를 추상화라고 하며, 컴퓨팅 사고(computational thinking)에서 중요한 능력 중 하나이다.         👉 알고리즘, Algorithm   알고리즘은 연산의 시퀀스이다. 적절한 알고리즘은 다음의 두 조건을 만족해야 한다.      모든 가능한 input instance*에 대해서 정답을 도출해야 한다.   반드시 알고리즘은 종료되어야 한다.        *인스턴스(instance)란?   인스턴스는 알고리즘의 입력이 되는 데이터로, 필요한 정보를 모두 포함한다.   다양한 알고리즘 중에 더 좋은 알고리즘을 판별하기 위해서는 다양한 요소에 대해 분석해보아야 한다. 대표적으로 점근적 표기법(Asymptotic notation)을 이용하여 알고리즘의 복잡도(complexity)를 분석할 수 있다. 복잡도는 크게 시간 복잡도(time complexity)와 공간 복잡도(space complexity)로 나눌 수 있다.   Fundamental Algorithms   여기서는 대략적으로 어떤 알고리즘이 있는지만 살펴보고, 각 알고리즘에 대한 자세한 내용은 후에 다루도록 한다.      정렬 알고리즘(Sorting Algorithm): 원소들을 일정한 순서대로 열거하는 알고리즘   최단 경로 알고리즘(Shortest Path Algorithm): graph에서 한 노드에서 다른 노드로 가는 가장 빠른 길을 찾는 알고리즘   최소 신장 트리 알고리즘(Minimum Spanning Tree Algorithm): graph를 tree로 변환하는 알고리즘   위상 정렬 알고리즘(Topological Sort Algorithm): 방향을 가진 유향 그래프의 꼭짓점들(vertex)을 변의 방향을 거스르지 않도록 나열하는 알고리즘   동적 계획법 알고리즘(Dynamic Programming Algorithm): 복잡한 문제를 간단한 sub-problem으로 나누어 푸는 알고리즘         👉 실습 환경   앞으로의 실습은 C++ 프로그래밍 언어를 이용하여 진행된다. Dev-c++(경량 프로그램) 혹은 vscode 코드 편집기를 사용하여 실습을 진행한다.           👉References      위키백과-추상화: https://ko.wikipedia.org/wiki/%EC%B6%94%EC%83%81%ED%99%94(%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B3%BC%ED%95%99)   edureka-Know All About the Various Data Types in Java: https://www.edureka.co/blog/data-types-in-java/   Geeksforgeeks-Difference between Linear and Non-linear Data Structures: https://www.geeksforgeeks.org/difference-between-linear-and-non-linear-data-structures/  ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo01/",
        "teaser": null
      },{
        "title": "[ALGO-02] Lect2. 기본적인 선형 자료구조의 종류, 구동 원리 및 활용법 이해",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/              선수지식   메모리 접근 오퍼레이터   용어           Pointer  어떤 variable은 크게 Address, Value, Name으로 표현된다고 할 때, pointer를 이용하여 value가 아닌 address로 값을 받아올 수 있다.            &amp;(Ampersand) Operator  &amp; 연산자는 reference 연산자라고도 불리며, 변수의 주소값을 반환한다.            *(Asterisk) Operator  *연산자는 dereference 연산자라고도 불리며, 포인터 앞에서 포인터가 가르키는 값에 접근한다.           예제1   변수 n의 주소를 변수 pn에 초기화   int n=3; int *pn = &amp;n;          예제2   char c = 'A'; char *pc = &amp;c; printf(\"c=%c, *pc=%c \\n\", c, *pc);  *pc = 'C'; printf(\"c=%c, *pc=%c \\n\", c, *pc);   출력결과:   c=A, *pc=A c=C, *pc=C        Function Call   함수를 호출할 때 argument를 넣어주는 방식에 따라 크게 두 가지로 나뉜다.   1. Call by value           값 자체     를 넣어준다.       원래 변수의 값을 수정할 수 없다.   2. Call by reference           포인터/주소값     으로 함수를 호출한다.       원래 변수의 값을 수정할 수 있다.       예제   void swap1(int x, int y); void swap2(int *px, int *py);  int main(int argc, char **argv){     int a=5, b=7;     printf(\"a=%d, b=%d\\n\", a, b);      swap1(a,b);     printf(\"swap1: a=%d, b=%d\\n\", a, b);      swap2(&amp;a, &amp;b);     printf(\"swap2: a=%d, b=%d\\n\", a, b);          return 0; }  void swap1(int x, int y){     int tmp = x;     x = y;     y = tmp; } void swap2(int *px, int *py){     int tmp = *px;     *px = *py;     *py = tmp; }   출력결과:   a=5, b=7 swap1: a=5, b=7 swap2: a=7, b=5         배열과 리스트      선형 자료 구조 중 대표적인 array와 list에 대해 알아본다.   Array   array는 어떤 자료의 값들을 모아둔 자료 구조이다.   1. 기본개념   구분자   array index를 통해 구분되며, 프로그래밍 언어에 따라 인덱스는 0부터 시작할 수도, 1부터 시작할 수도 있다. C/C++, JAVA 프로그래밍 언어에서는 array index가 0부터 시작한다.   가령 아래와 같이 size가 10인 int형 array score를 선언할 때 array index는 0~9이다.          저장   프로그래밍 언어를 이용하여 array를 선언하면, physical memory인 RAM에 연속적인 공간을 할당받는다.       선언   array를 선언하는 방법은 크게 두 가지가 있다.           정적 생성      Type d[10];                동적 생성      Type *d = new Type[size];           동적으로 생성한 array를 다 사용한 경우, 할당을 삭제해야 한다: delete [] d;           접근   d[5] = 2;   인덱스를 이용하여 접근할 수 있다.       2. Array 생성 (in C++)   1차원 array   int arr[10]; // 혹은  int *arr = new int[10];          2차원 array   int a[3][4]; // 혹은  int **a = new int *[3]; for(int i=0; i&lt;3; i++)     a[i] = new int[4];          3. Array in Memory   int score[3] = {52, 17, 61};   위와 같이 score array를 생성했을 때, physical memory에는 아래 이미지와 같이 값이 쓰여진다.          4. Insertion and Deletion   Insertion   array 요소의 중간에 어떤 값을 삽입하고싶을 때, 이후 값을 모두 1칸씩 뒤로 밀고 생긴 빈자리에 값을 넣어주어야 한다.   int main(int argc, char **argv){     int arr[5] = {3, 10, 7, 6,};     printIntArray(arr, 5);          // 3(arr[0])과 10(arr[1]) 사이에 5를 삽입     for(int i=5; i&gt;1; i--)         arr[i] = arr[i-1];     arr[1] = 5;      // array 결과를 출력     printIntArray(arr, 5);      return 0; }   출력결과:   3 10 7 6 0  3 5 10 7 6       Deletion   마찬가지로 어떤 요소값을 삭제할 때도 이후 값을 모두 1칸씩 앞으로 밀어주어야 한다.   int main(int argc, char **argv){     printf(\"Hello World!\\n\");      int arr[5] = {3, 5, 10, 7, 6};     printIntArray(arr, 5);          // 3(arr[0])를 삭제     for(int i=0; i&lt;5; i++)         arr[i] = arr[i+1];      // array 결과를 출력     printIntArray(arr, 5);      return 0; }   출력결과:   3 5 10 7 6  5 10 7 6 0       List   insertion 혹은 deletion을 수행할 때 해당 인덱스를 기점으로 뒤의 값을 모두 밀어주어야하는 array의 단점을 해결하기위해 제안된 자료구조가 바로 linked list이다. physical memory상에서 순서대로 작성되지 않으므로 삽입과 삭제가 한 번의 동작으로 가능하다.   1. 기본개념   typedef int Data; typedef struct _Node {     Data item;     struct _Node * next;     } Node;  typedef struct {     Node *head;     int len; } LinkedList;             스택과 큐   stack과 queue 는 빈번히 사용되는 자료구조이다. C++에서는 STL(standard template library) 소프트웨어 패키지를 이용하여 pair, vector, list, queue, stack과 같은 다양한 종류의 container를 제공한다.   Stack      삽입과 삭제가 LIFO(last-in first-out) 순서로 진행된다.   용어      Top: stack의 최상단 지점   Push: top 위치에 아이템을 삽입   Pop: top에 위치한 아이템을 제거       응용   괄호매칭 문제 에서 stack이 사용된다. 열린 괄호가 나타날 때 stack에 열린괄호를 push하고, 닫힌 괄호가 나타날 때 열린 괄호를 pop한다. 최종적으로 stack이 비어있다면 괄호매칭이 잘 된 것이라 판단한다.        Queue      삽입과 삭제가 FIFO(first-in first-out) 순서대로 진행된다.   용어      Front (head): queue의 앞 부분 (deletion이 발생)   Rear (back, tail): queue의 뒷 부분 (insertion이 발생)   Enqueue: rear에서 아이템이 삽입됨   Dequeue: front에서 아이템이 삭제됨       Linear Queue vs. Circular Queue      linear queue는 enqueue/dequeue를 반복하다보면 rear가 주어진 array의 마지막을 가리키게 된다. 따라서 front의 앞 부분에 분명 공간이 남아있음에도 사용할 수 없는 공간 낭비 문제가 발생한다.   이를 해결하기 위해 linear queue에서 할당된 array의 양끝을 이어붙인 circular queue 구조가 제안되었다. circular queue는 rear가 array의 마지막을 간 경우, 다시 0 index로 돌아오는 형태로 되어있다.        References      kmooc-[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/      ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo02/",
        "teaser": null
      },{
        "title": "[ALGO-03] Lect3. 비선형 자료구조",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/                 이번 시간에는 대표적인 비선형 자료구조인 트리와 그래프에 대해 알아본다.       트리 자료구조   트리는 노드들의 집합체로, 계층적인 관계를 나타내는 자료구조이다. 디렉토리 구조는 대표적인 tree구조이다..          특징      정보는 nodes에 저장된다.   첫 번째 노드는 root라고 불린다 (위 그림에서 A에 해당)   각 노드는 자손 노드(children)들을 가질 수 있다.   root를 제외한 각 노드는 하나의 부모 노드(parent)를 가진다.       용어           Degree: 해당 노드가 갖는 자식 노드의 개수              Leaf 노드: degree = 0인 노드       Internal 노드: degree != 0인 노드                Sibling: 같은 부모 노드를 공유하는 자매 노드            Unordered tree: 자식 노드의 순서가 무시될 수 있는 트리 구조  Ordered tree: 자식 노드간에 순서가 존재하는 트리 구조            Path: 노드들의 시퀀스 $(a_0, a_1, …, a_n)$​​​​ (이때, $a_{k+1}$: $a_k$​​​​의 자식노드)                       예시: path(B,E,G)의 길이는 2이다.                                   depth: 루트노드부터 해당 노드까지의 경로의 길이                       예시: depth(B)=1, depth(E)=2, depth(F)=3                                   height: 트리에 존재하는 가장 큰 depth값              예시:   ⒜ 루트 노드만 존재하는 경우 height=0  ⒝ 아무 노드도 존재하지 않는 경우 heigth=-1                ancestor: a는 b의 ancestor (노드 a에서 노드 b로 가는 path가 존재할 때)  descendant: b는 a의 descendant (노드 a에서 노드 b로 가는 path가 존재할 때)  strictly descendant: 자기 자신이 자기 자신의 자손이나 부모가 되는 것을 금지시키는 조건 ※ 일반적인 경우, 자기 자신은 자기 자신의 ancestor이자 descendant이다. ※           표현      각 노드가 자식 노드를 참조하고 있는 구조로 표현된다.         그래프 자료구조   그래프는 데이터 사이의 인접한 정보를 저장하는 자료구조이다.      ■ 응용      SNS상에서 친구 관계   회로 사이의 component간의 연결성   선수과목 정보 표현       용어           vertex: 정점(node), $V$라 표현            Objects: 저장하고자 하는 객체. 유한개의 nodes(혹은 vertices)의 집합            edge: vertex 간의 연결, $E$라 표현            Relationship: 유한개의 edges(혹은 arcs, links)의 집합            degree: 이웃 vertex의 개수                       예시: degree($v_1$)=3                                   neighbor: 인접한(adjacent) vertex의 집합            sub-graph: original graph에서 일부 vertex와 edge를 sampling하여 얻을 수 있는 그래프            path: vertex간의 연결: $(v_0, v_1, …, v_k)$              trivial path: length=0인 path       simple path: 경로상에, 처음과 마지막을 제외하고 중복이 없는 경우  simple cycle: 처음과 마지막 vertex가 일치하는 simple path                    예시  case1) A-B-C : simple path  case2) A-B-C-A : simple path &amp;&amp; simple cycle  case3) A-B-C-B-A : simple path아님                                connectedness(연결성): graph의 vertex끼리 어떤 path로든 연결되어있는지 여부                       예시: 아래 그래프에서 흰색 영역에 존재하는 path가 끊어지면 conntedness 성질이 사라진다.                                  유형   그래프의 속성에 따라 다양한 종류의 그래프가 존재한다.      Undirected graph: edge에 방향이 없는 무향 그래프   directed graph: edge에 방향성이 존재하는 유향 그래프   Weighted graph: 가중치가 있는 그래프   Tree: unique path를 갖는 conncted graph   forest: tree들의 모음       1. Undirected Graphs      vertices의 모음으로 표현되는 방향이 없는 그래프       ■ 특징   $V={v_1, v_2, …, v_n}$일 때           vertices의 개수 $│V│=n$            vertices를 연결하는 edges E = ${v_i, v_j}$​   ※ 순서가 없는 쌍 ※           ■ 예시   연결성을 표현하기 위해 adjacency matrix나 adjacency list를 사용할 수 있다.         9개의 vertex가 존재: $V={v_1, v_2, …, v_9}$​​, $│V│=9$​​   5개의 edge가 존재: $E={{v_1, v_2}, {v_3,v_5}, {v_4, v_8}, {v_4, v_9}, {v_6, v_9}}$, $│E│=5$       ■ 최대 edge개수   자기 자신으로 가는 edge는 없다고 가정하면, undirected graph에서 최대한 많은 edge의 개수는 다음과 같다.   \\(|E|≤ _VC_2 = \\binom{|V|}{2} = \\frac{|V|(|V|-1|)}{2} = O(|V|^2)\\)     2. Directed Graphs      vertices의 모음으로 표현되는 방향이 있는 그래프       ■ 용어           in_degree: 방향이 해당 node로 향하는 edge의 개수  out_degree: 해당 node에서 나오는 방향인 edge의 개수                       예시: in_degree($v_1$)=1, out_degree($v_1$)=2                                   source: in_degree=0인 node  sink: out_degree=0인 node            strongly connected: 방향성을 고려했을 때 모든 pair간에 경로가 존재하는 경우  weakly connected: 방향성을 무시할 때 모든 pair간에 경로가 존재하는 경우            directed acyclic graph (DAG, 유향 비순환 그래프): 순환하지 않는 유향 그래프       예시:                  3. Weighted Graphs      edge가 연결성뿐만 아니라 가중치도 표현하는 그래프       ■ 응용   vertex간의 거리나 에너지 소모등을 표현하는 경우에 주로 사용되며, 이러한 그래프를 통해 shortest path 문제를 해결할 수 있다.       ■ path length   단순히 연결된 path의 개수로 표현되는 un-weighted graphs와 달리, weighted graph는 path의 가중치의 합으로 length가 표현된다.   예시: $\\text{length of }(v_1, v_3, v_5, v_4)=5.1+1.3+1.1=7.5$          4. Tree   graph가 다음의 조건을 만족하는 경우, tree라고 부를 수 있다.      cycle이 없음   연결 그래프       ■ 특징           unique path를 갖는다. ⇒ 따라서 $│E│=│V│-1$       root를 제외한 모든 node가 하나의 parent와 연결된 path를 가지며, parent가 누구냐에 따라 구조가 달라진다고 생각하면 |E|=|V|-1의 결과를 쉽게 얻을 수 있다.           하나의 edge를 추가하면 cycle이 생긴다.            하나의 edge를 제거하면 disconnected graph가 되며, 두 개의 tree로 분리된다.           5. Forest      하나 이상의 tree로 이루어진 집합을 forest라고 한다.       ■ 특징           cycle이 없다 ⇒ 따라서 $│E│&lt;│V│$        forest에서 root를 하나만 남도록 연결하면(edge를 추가하면) 하나의 tree가 된다. 즉, │E│=│V│-1인 tree의 edge 개수보다 forest의 edge개수가 더 적으니 언제나 │E│&lt;│V│           tree의 개수 = $│V│-│E│$       |V|에서 |E|를 빼면 root의 개수가 되므로 tree의 개수는 |V|-|E|           edge를 제거하면 tree가 하나 더 생성된다.           표현   그래프를 표현하는 대표적인 방식은 다음과 같다.   1. Binary-relation list   edge를 나열하여 graph를 표현한다.   ■ 예시      ${(1,2), (71,4), (3,5), (4,2), (4,5), (5,2), (5,3), (6,9), (7,9), (8,4)}$       ■ 특징      메모리 사용량 = $│E│$​   어떤 두 vertex간에 edge가 존재하는지 확인하기 위한 계산량 $≤│E│$​​​​​   어떤 vertex의 neighbor를 구하기 위한 계산량 $≤│E│$​​       2. Adjacency Matrix   vertex $v_j$​​에서 $v_k$​​로 가는 edge가 존재할 때 $(v_j, v_k)$​​의 값을 True로 지정한다.       ■ 특징      메모리 사용량 = $│V│^2$   어떤 두 vertex 간에 edge가 존재하는지 확인하기 위한 계산량 = 1   어떤 vertex의 neighbor를 구하기 위한 계산량 = $O(│V│)$     ※ computational overhead ※      weighted graph인 경우, true/false 대신 weight 값으로 표현       3. Adjacency List   일반적인 알고리즘에서 가장 많이 사용되는 그래프 표현법으로, 각 노드를 기준으로 자기 자신과 연결된 vertex를 list형태로 표현한다.   ■ 예시              vertex     adjacent to      v1v2, v4   v2    v3v5   v4v2, v5   v5v2, v3   v6v9   v7v9   v8v4, v5   v9        ■ 특징      메모리 사용량 = $│V│ &lt; │E│$​       References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91   ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo03/",
        "teaser": null
      },{
        "title": "[CS-01] Lect1. 컴퓨터구조 개요",
        "excerpt":"     강의 정보              강의명: 컴퓨터구조          교수: 상명대학교 전자공학과 박병수 교수님, 상명대학교 시스템반도체공학과 홍대길 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SMUCk+CK.SMUC03k+2017_T6/course/              1. Computer System 구조   전반적 구조      구성요소 설명           CPU, Central Processing Unit, 중앙 처리 장치  :산술/논리연산 및 제어를 수행하는 부품            Main Memory  :주로 RAM(Random Access Memory)이 이에 해당한다. ※ 일반적으로 ROM은 main memory에 포함되지 않음 ※            SSD, Solid State Drive, Secondary Storage Device, 보조 저장 장치  :e.g.,  하드디스크, CD ROM, ... ※ SSD는 I/O의 일부로 분류하기도 함 ※ ※ Main Memory와 SSD 모두 데이터를 저장하는 기능을 가졌으나 쓰임이 다르다.              I/O, Input/Output Device  :e.g., Input device: 키보드, 마우스   e.g., Ouput device: 모니터, 프린터       System Bus  :위와 같은 부품들을 연결한다.       CPU내부   CPU의 내부는 크게 세 가지 sub-block으로 구성되어 있다.           ALU, Arithmetic Logic Unit, 산술 논리 장치            Register Set  :데이터를 임시로 저장하는 register의 집합            Control Unit  :제어 신호를 전송       이때 위 세 가지 sub-block을 연결하는 경로를 Internal Bus라고 한다.  ※ system bus와 다르다 ※          ALU, 산술 논리 장치   ALU는 산술 연산을 수행하는 AU와 논리 연산을 수행하는 LU로 구성되어 있다.      AU, Arithmatic Unit: +, -, *, %   LU, Logic Unit: OR, AND, NOT, EX-OR       Register Set   크게 일반목적 Register와 특수목적 Register로 구성된다.           일반 목적의 Register: 임시로 데이터를 저장하는 장치                       Shift Register: Bit의 자리를 옮김                        Status Register: 현재 상태를 저장                        특수목적 Register: 특수한 목적을 갖는 저장 장치                       ACC, Accumulator: 처리할 데이터가 누적되는 저장 장치 (연산의 최종 결과가 저장됨)                        SP, Stack Pointer: 개념 난해. 나중에 알려드림                        PC, Program Counter: 실행할 프로그램이 저장되어있는 메모리 주소가 저장됨                        MAR, Memroy Adrress Register: Address bus로 전송될 주소값이 저장됨                        MBR, Memory Buffer Register: Data bus로부터 받은 데이터가 저장됨                        IR, Instruction Register: MBR에 저장된 데이터가 명령인 경우, IR에 해당 명령이 저장됨                       Control Unit           ID, Instruction Decoder  :Register Set의 `IR(Instruction Register)`에 저장된 데이터가 Control Unit의 ID(Instruction Decoder)로 전달됨.   명령어로 구성된 프로그램을 기계가 이해할 수 있도록 디코딩하여 번역하는 역할            CAR, Control Address Register  :ID에서 디코딩의 결과로 출력된 'μ-명령어의 주소'가 저장됨            CM, Control Memory (μ-instruction)  :&lt;p&gt;'μ-명령어로 이루어진 μ-프로그램'이 저장된 기억장치&lt;/p&gt;            CBR, Control Buffer Register  :CM으로부터 읽은 'μ-명령어'를 저장 및 control bus로 전달            μ-instruction      사람이 짠 프로그램을 컴파일하면 기계어가 출력되는데, ID에 의해 번역된 주소가 CM을 거쳐 μ-instruction(마이크로 명령어)가 된다.            S/W      S/W는 아래와 같이 크게 System S/W, Application S/W, F/W로 구성된다.           System S/W: e.g.,window, linux, ...     Application S/W: e.g., Adobe Photoshop, ...      F/W: H/W를 구동하기 위한 μ-instruction으로 구성됨            System Bus   System bus는 컴퓨터 시스템의 주요 구성 요소를 연결하여 신호를 전달한다. 아래와 같이 크게 세 가지 bus로 구성된다.           Address Bus       CPU→memory(단방향)으로 주소값을 전달 (Unidirectional)        \tAddress Bus에서 width의 의미  \tAddress Bus의 폭은 최대 Memory Capacity를 결정한다. 가령 Address Bus가 2Bits로 구성된 경우, 2²개의 주소를 만들 수 있다(00, 01, 10, 11). 13Bits로 구성된 Address Bus는 2¹³=8K개의 주소를 만들 수 있다.                    Data Bus       데이터값을 전달 (Bidirectional)              CPU→memory : 연산 결과를 메모리에 저장하는 경우       memory → CPU : 메모리의 데이터를 CPU로 가져오는 경우                Data Bus에서 width의 의미      가령 Data Bus의 Width가 32Bits인 경우, 한 번에 32Bits만큼씩 데이터 전송이 가능하다.            Control Bus       명령어 시퀀스를 전달 (Bidirectional)              CPU→memory : READ/WRITE와 같은 명령어 시퀀스를 전달하는 경우       memory→CPU : 주변 장치가 CPU에게 interrupt를 걸며 제어 신호를 전달하는 경우               Main Memory, SSD, I/O의 구성      Main Memory   Main memory는 아래와 같이 세 가지로 구성된다.           Addressing Mode, 주소 지정 방식              명령어에서 피연산자(Operand)의 주소가 지정되는 방식을 지정한다. 이에 따라 여러가지 형태로 데이터를 가져오게 된다.                RAM, Random Access Memory (READ/WRITE)                       주 메모리(또는 기본 메모리)라고도 불린다.                        전원이 꺼지면 데이터가 손실되는 휘발성 메모리이다.                        m-instruction(machine instruction)들이 저장되어 있다.                        ROM, Read Only Memory (READ)                       필수 프로그램과 같은 시스템 운영에 필수적인 정보를 저장한다.                        항상 데이터를 유지하는 비휘발성 메모리이다.                        m-instruction(machine instruction)들이 저장되어 있다.                       Cache   Main memory에 저장된 데이터 중 빠른 시일 내에 CPU가 읽을만한 데이터를 저장한다. CPU는 데이터를 읽기 위해 Main memory에 접근하기 전에, Cache를 먼저 거친다.   특징      반도체를 이용하여 만들기 때문에 RAM/ROM에 비해 빠르다.&gt;       SSD, Solid State Drive   SSD는 고형 상태의 보조 기억 장치로, Disk, CD-ROM이 이에 해당한다.   main memory는 system bus에 직접 연결된 반면, SSD는 SSD controller를 거쳐 system bus에 연결된다.       SSD가 System bus와 통신하는 방법   SSD의 데이터를 SSD Controller에 임시로 가져온 뒤, system bus에 싣는다.       Main Memory vs. SSD                  비교       Main Memory       SSD                       Access 방식       표준이 있어 CPU가 register를 통해 직접 접근       CPU가 별도의 Controller를 통해 접근                 Device 종류       RAM, ROM, Cache       Magnetic Disk, Optical Disk, RAID                 특징       ● 빠른 속도 ● 비싼 가격 ● 용량대비 면적이 큼 ● 휘발성       ● 느린 속도 ● 저렴한 가격 ● 용량대비 면적이 작음 ● 비휘발성               I/O, Input/Output Device   Input/Output device는 SSD와 마찬가지로 System bus와의 통신을 담당하는 I/O Controller가 존재한다.       I/O vs. SSD   공통점      CPU입장에서 I/O와 SSD는 차이가 없다.   둘 다 Device Controller를 통해 System bus에 접근할 수 있다.   Controller 내부에 존재하는 Status Register와 Data Register에 별도의 Address가 할당되어 있다.   차이점           I/O(keyboard, printer..)는 bytes(8Bits) 단위     로 전송하지만      SSD는 block(512/ 1024/ 4096 Bytes) 단위     로 전송한다.       ∴ SSD는 Controller 내에 한 block 이상을 임시 저장할 수 있는 Data Buffer가 존재한다.             2. 컴퓨터 구성품의 연결   CPU와 컴퓨터 구성품의 통신 과정   CPU와 컴퓨터 구성품이 통신하는 방법에 대해서 알아본다. 이러한 연결 방식은 크게 아래와 같이 네 가지로 구분할 수 있다.      Memory read   memory write   I/O read   I/O write   Memory Read   Main Memory로부터 데이터를 읽는 과정은 다음과 같다.      CPU에서 main memory에 있는 데이터를 읽어오는 과정           CPU → Main Memory                       CPU → Address Bus → Cache → (Main Memory)           메모리의 어느 주소를 읽을지에 대한 주소 정보(Address)가 전달된다.  ※ Cache에 원하는 데이터가 있는 경우 Main Memory까지 신호가 전달되지 않음 ※                        CPU → Control Bus → Cache → (Main Memory)           메모리로부터 데이터를 읽을 것이라는 READ 신호가 전달된다.                        CPU ← Main Memory                       주소를 읽은 결과에 해당하는 데이터         가 Data Bus를 통해 CPU로 전달된다.                       신호선 타이밍도          Memory Write      CPU에서 Main Memory로 데이터를 작성하는 과정      CPU → Main Memory            CPU → Address Bus → Main Memory : Address가 전달       CPU → Data Bus → Main Memory : 연산할 Data가 전달       CPU → Control Bus → Main Memory : WRITE enable 신호가 전달               신호선 타이밍도          I/O Read      I/O Device Controller   I/O Device Controller 내부에는 다음의 두 가지 Register가 존재한다.           Status Register, 상태 레지스터       CPU에게 데이터를 잘 수신했음을 확인시켜주기 위한 Register이다.  새로운 데이터가 업데이트되면 status가 0에서 1로 업데이트된다.            Data Register, 데이터 레지스터       임시로 데이터가 저장되는 Register이다.           I/O Device로부터 I/O Device Controller로 데이터를 읽어오는 과정              CPU → I/O Device Controller       Address와 READ enable 신호를 받는다.            I/O Device Controller ←→ I/O Device       I/O Device로부터 데이터를 읽어와 Data Register에 데이터를 저장한다. 이후 Status Register의 값을 0에서 1로 업데이트한다.            CPU ← I/O Device Controller       Status Register의 값을 읽어서, In_RDY Bit의 값이 1인지를 판단한다.  In_RDY Bit는 Status Register에 존재한다.            CPU ← I/O Device Controller       4.1. In_RDY Bit == 1인 경우       Controller가 데이터를 잘 읽어온 경우이다. Data Register의 값을 읽어 Data Bus를 통해 CPU에게 전달한다.       4.2. In_RDY Bit != 1인 경우       Controller가 데이터를 아직 읽어오지 못한 경우이다.      다시 3의 과정으로 돌아간다.                     I/O Write   유사한 방식으로 I/O Device로부터 데이터를 읽을 수 있다.   결정 방식   Bus Arbitration   System Bus는 한 순간에 하나의 Address나 Data만 전송시킬 수 있다. 이때 동시에 System Bus를 사용해야 하는 경우, 우선권을 주는 방식에 대해서 다룬다. 이러한 방식을 Bus Arbitration이라고 하며 아래와 같이 크게 3가지 방식으로 나눌 수 있다.              PA, Parallel Arbitration       Bus Priority를 병렬로 부여하는 방식            Serial Arbitration       Bus Priority를 직렬로 부여하는 방식            Polling       요구한 디바이스에게 우선적으로 Bus Priority를 부여하는 방식 (선착순)           I/O Device 접속 방식   I/O Device를 설계할 때, 주로 기존의 프로세서를 이용하여 원하는 동작을 수행하는 어플리케이션을 만든다. 이때 외부의 데이터 입출력을 어떤 방식으로 CPU화할 것인지(인터페이스)를 결정해야 한다. 이러한 개념은 I/O Device 접속 방식에 해당한다. 아래와 같이 크게 세 가지로 분류할 수 있다.         Programmed I/O a.k.a. Polling            주기적으로 데이터가 입력되었는지를 확인하는 방식       마이크로프로세서를 이용하여 코딩하는 경우 많이 사용됨           Interrupt Driven I/O, 현재 대부분의 방식에서 사용됨            데이터가 왔을때 신호를 받아 데이터가 온 것을 알아차리는 방식           I/O with DMA, Direct Memory Access            데이터가 왔을때 처리하는 장치가 별도로 존재하는 방식           위 세가지 방식에도 하위 방식이 여러가지 존재하는데, 추후에 자세히 다루도록 한다.   컴퓨터의 전체 구조      3. Program과 Data의 처리, 반도체 개요   read와 write가 이루어지는 과정에 대해서 다룬다.   1차시. 컴퓨터의 구성품   2차시. 구성품간의 연결   3차시. 어떻게 오가게하고 처리할 것인지   READ/WRITE 데이터를 처리하는 과정              High-Level Language Program, (C/C++, Python 등등..)       e.g., Z=X+Y            컴파일                       Assembly Program                                   Assembly Instruction               목적지가 앞에, 출발지가 뒤에 나옴               e.g.,                              LOAD A, X: A(accumulator) register로 X memory에 저장된 값을 LOAD(가져가라).               ADD A, Y : A register로 Y에 저장된 값을 더해라.               STOR Z, A: Z 메모리에 A의 값을 저장해라.                                                        Machine Program                      기계만 이해할 수 있는 내용.           이게 메모리에 저장되며, 메모리에 있는 machine program이 한 줄씩 주소에 따라서 CPU로 가서, CPU에서 어떤 코딩과정을 거쳐, 제어신호로 바뀜.           e.g.                            LOAD: 001               X: 00101 = 5번지.   거기 읽었더니 00011011이 써있음               A는 도마야. 지정할필요없어. 당연히 A에 저장하겄지. 그래서 따로 binary 없어               ADD: 100               Y: 00110 = 6번지.   거기 읽었더니 11010111 써있음               STOR: 010               Z: 00111 = 7번지.   거기에 저장을 하는겨                                                   반도체 부품의 발전      Vaccum Tube, 진공관 (1세대)            다이오드의 역할 ~~ 스위치. 끄면 0, 키면 1           TR, Transistor (2세대)            Vacuum Tube가 발전된 형태.       컴퓨터로 구성하는 모든 반도체의 기본 단위           IC (3세대: SSI, Small Scale Integration)            transistor를 기판 안에 집적시켜, 수백만개의 transistor를 하나의 칩 안에 집적시키게 됨           MSI, Medium Scale Integration            단위면적당 transistor수 증가: 집적이 더 많이 됨.           LSI, Large Scale Integration   VLSI, Very Large Scale Integration            현재 이 정도.           ULSI,            아직 세대는 아님           Optical/Neural Computer            앞으로 나올 것들.       신경망을 이용한 컴퓨터           AI            인공지능 형태의 반도체           IC 제조 과정   Silicon: IC 만드는 대표적 재료 (실리콘 혹은 저메니움,갈마스나이드 여러 종류가 있음)   실리콘 이용하여 wafer를 만듦 ~~ 웨하스의 얇은 판떼기      정사각형 하나가 칩 하나. 짜투리는 버림.   여기에 Integrate(직접)함.   이후 쪼갬. 다이아몬드 칼로 하나를 쪼개서 이 패키지 안에 넣음.=&gt; PAckaging      Pin도 부착   PCB에 연결: 녹색 판에 저항, 케페시터 등 부착하면 보드를 얻게됨.   컴퓨터 시스템의 분류   가장 작은           Embedded Computer (두 번째로 많이 사용). 일에 종사하면 자판기 등에서 이런걸로 됨.       시스템 내에 내장되어 있다는 의미에서 내장 컴퓨터            PC, Personal Computer (가장 많이 사용)       최근 모든 기능은 PC로 집약되는 느낌.            WS, Work Station, Super-Mini Computer       크고 성능이 파워풀한 것.       최근 쓰임새가 줄어듬. 왜냐면 PC에서 구현 가능            Main-Frame Computer       작은 방안을 채울 정도로 큰 규모를 갖는 컴퓨터            Super Computer, Cluster Computer (Alpha-Go)       최근 경향: 하나의 머신을 만들지 않고 cluster computer라해서 네트워크를 연결된 여러 대의 PC를 컴퓨터 파워 모아서 비슷하게 만들어주는 그런걸로 됨.       alpha-go도 Cluster computer. 수천개의 pc가 네트워크로 연결되어 바둑을 둚           References      kmooc-컴퓨터구조: http://www.kmooc.kr/courses/course-v1:SMUCk+CK.SMUC03k+2017_T6/course/      ","categories": ["computerScience-computerStructure"],
        "tags": ["computer science","computer structure","CS"],
        "url": "/computerscience-computerstructure/cs-computerStructure-kmooc01/",
        "teaser": null
      },{
        "title": "[ALGO-04] Lect4. 그래프 탐색 알고리즘: DFS, BFS",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/                 이번 시간에는 주어진 그래프에서 노드를 순회(traversal)하는 방법에 대해 배운다.       1. 그래프 탐색: DFS와 BFS   Graph Traversal(Search), 그래프 순회   그래프 순회란, 그래프의 각 vertex 하나씩 방문하는 것을 말한다. 대표적인 그래프 순회 방식은 아래와 같이 크게 두 가지로 나눌 수 있다.           Breadth-First Search, BFS: 너비 우선 순회                   Depth-First Search, DFS: 깊이 우선 순회              BFS와 DFS은 유사한 알고리즘을 공유하지만 BFS는 queue을, DFS는 stack를 사용하여 알고리즘이 구현된다는 점에서 차이가 있다.       대략적인 알고리즘 요약      하나의 vertex를 선택한 뒤, visit이라 표시하고 queue(혹은 stack)에 삽입한다.   queue(혹은 stack)이 빌 때까지 아래 동작을 반복한다.            queue(혹은 stack)에서 하나의 vertex를 꺼낸다.       꺼낸 vertex와 인접한 vertex 중, visit이라 표시되지 않은 vertex를 queue(혹은 stack)에 삽입한다.           이때, queue(혹은 stack)이 비었는데 not visited 상태인 vertex가 존재한다면, 해당 그래프는 unconnnected graph라고 판별할 수 있다.      BFS 알고리즘   아래 예시를 통해 BFS 알고리즘을 살펴본다.      최종 순회 순서: A-B-C-E-D-F-G-H-I      BFS 알고리즘은 Queue를 이용하여 visited vertices를 push &amp; pop한다. (그래프를 tree구조로 해석했을 때) FIFO 성질에 의해 같은 부모를 공유하는 sibling 순서대로 순회함을 알 수 있다. 이러한 논리에 따라 BFS 알고리즘을 구현할 수 있다.       DFS 알고리즘   BFS 알고리즘이 queue를 사용하여 FIFO 순서로 그래프를 순회하였다면, DFS 알고리즘은 stack을 사용하여 LIFO 순서로 그래프를 순회한다.   아래 예시를 통해 DFS 알고리즘을 살펴보자.      최종 순회 순서: A-B-D-C-F-E-G-H-I      DFS 알고리즘은 Stack를 이용하여 visited vertices를 push &amp; pop한다. (그래프를 tree구조로 해석했을 때)  LIFO 성질에 의해 자식 vertex를 향해 깊어지는 방향으로 순회함을 알 수 있다. 이러한 논리에 따라 DFS 알고리즘을 구현할 수 있다.       BFS와 DFS   특징   BFS와 DFS는 인접 vertex를 삽입하는 순서에 따라 최종 순회 순서가 달라진다. 즉, ordering은 unique하지 않다는 특징이 있다.       응용   BFS 혹은 DFS 알고리즘을 이용하여 그래프로부터 connected component를 구할 수 있다. \"box가 비었음에도 불구하고 visited 마크되지 않은 vertex가 존재한다면, 해당 그래프는 unconnected graph라는 특성\"을 이용하면 된다.         2. 그래프 탐색의 STL 활용 구현      위와 같은 그래프를 BFS와 DFS 알고리즘을 이용하여 정렬하는 코드를 작성해보자.       BFS 및 DFS 구현         실행 결과   $ ./main.exe graph.txt  [Graph/Init] start # of vertices=13, # of edges = 12 ... edges[A] : B ... edges[A] : H ... edges[B] : C ... edges[B] : E ... edges[C] : D ... edges[E] : F ... edges[E] : G ... edges[H] : I ... edges[H] : M ... edges[I] : J ... edges[I] : K ... edges[I] : L [Graph/Init] end  [BFS] start --- The result of Breadth-First Searching --- A B H C E I M D F G J K L [BFS] end  [DFS] start --- The result of Depth-First Searching --- A B C D E F G H I J K L M [DFS] end         References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91   ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo04/",
        "teaser": null
      },{
        "title": "[ALGO-05] Lect5. 함수의 점근적 분석",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/              이번 시간에는 주어진 함수를 점근적으로 분석하는 방법에 대해 배운다.   함수의 점근적 분석법   함수의 점근적 분석(Asymptotic Analysis)과 이를 통한 함수들 사이의 점근적 대소 관계에 대해 학습한다.   배경      알고리즘을 직접 구현하여 비교하는 방식은 비용소모적이다.   점근적 분석법은 수학적으로 분석하여, 두 알고리즘의 성능을 비교하기 위한 분석법이다.       ■ Motivation: Linear Search vs. Binary Search      Linear Search : Array에서 특정 값을 찾고자 할 때, 앞에서부터 순차적으로 테스트하는 방법   Binary Search : 정렬된 Array에서 특정 값을 찾고자 할 때, 중간값과 찾고자하는 값을 비교하여 범위를 좁혀가며 테스트하는 방법      위 그래프는 Array의 사이즈에 대한 Searching Algorithm의 연산량 그래프이다. Array Size가 증가함에 따라 Linear Search 알고리즘에 비해 Binary Search 알고리즘의 연산량이 더 적은 것을 알 수 있다.       특징   Quadratic Growth      $f(n)=n^2$​, $g(n)=n^2-3n+2$​일 때, $N$​을 $(0,3)$​의 범위에서 관찰하면 $f(n)$​과 $g(n)$​ 그래프의 차이가 없지만,  $N$을 $(0,100)$​의 범위에서 관찰하면 두 그래프가 거의 비슷해진다.   따라서 Asymptotic Analysis에서는 N이 무한히 커졌을 때 두 알고리즘의 차이가 얼마나 심한지가 주요 관심사이자 핵심이다.       Counting Instructions    \tA보다 B 알고리즘이 더 느릴 때, 더 좋은 컴퓨터를 사용함으로써 B가 A보다 빨라질수도, 혹은 그렇지 않을 수도 있다.   ■ 두 알고리즘 모두 Leading Term이 $n^k$​인 경우 (Polynomial)   \\[f(n)=a_kn^k + a_{k-1}n^{k-1} + ...\\\\ g(n)=b_kn^k + b_{k-1}n^{k-1} + ...\\]  $M=\\frac{a_k}{b_k}+1$일 때, n이 충분히 큰 경우 다음이 언제나 성립한다.   \\[f(n)&lt;Mg(n)\\]  $f(n)$​​ 알고리즘을 구동시키는 컴퓨터보다 $M$​​배 빠른 컴퓨터로 $g(n)$​​알고리즘을 구동시킨다면, $g(n)$​​알고리즘이 더 빠르게 동작한다.       ■ 두 알고리즘의 승수가 다른 경우   \\[f(n)=a·n^2\\\\ g(n)=b·n·\\log{(n)}\\]  g(n)은 절대 f(n)보다 빨라질 수 없다.       Weak Ordering   ■ Equivalent   아래와 같이 f(n)과 g(n)의 비율이 상수로 수렴한다면, f와 g는 동일한 복잡도를 갖는다: $f \\sim g$​​.   \\[\\lim_{n→∞}{\\frac{f(n)}{g(n)}}=c \\ \\ \\ \\ \\text{where, 0&lt;c&lt;∞}\\]      ■ 대소관계   아래와 같이 f(n)과 g(n)의 비율이 0에 수렴한다면, g의 복잡도가 더 크다: $f &lt; g$​​​​​.   \\[\\lim_{n→∞}{\\frac{f(n)}{g(n)}}=0\\]          심볼 기반 함수의 점근적 바운드   함수의 점근적 바운드를 표현하는 5가지 심볼에 대해 이해한다.   Notation   함수의 증가 양상을 다른 함수와의 비교로 표현하는 점근 표기법(Asymptotic Notation)은 대표적으로 다섯 가지 표기법이 있다.      Θ-Notation (Theta Notation, 대문자 세타 표기법)   Big-O Notation (Big O Notation, 대문자 O 표기법)   Big-Ω Notation (Big Omega Notation, 대문자 오메가 표기법)   o Notation (O Notation, 소문자 o 표기법)   ω Notation (Omega Notation, 소문자 오메가 표기법)   Θ-Notation      ■ 정의   \\[f(n)=Θ(g(n))\\\\ \\text{$0≤c_1g(n)≤f(n)≤c_2g(n)$, for all $n≥n_0$}\\]  위 조건을 만족하는 $c_1, c_2$가 존재할 때, 다음과 같이 표현할 수 있다.      $f(n)$: $g(n)$과 같은 증가 속도를 갖는다.   $f(n)$: $Θ(g(n))$에 속한다.   $g(n)$: $f(n)$에 대한 Aymptotically Tight Bound       ■ 예시   \\[f(n)=Θ(g(n))\\text{: a polynomial of degree k}\\\\ ⇔ f(n)=Θ(n^k)\\]          $\\frac{1}{2}n^2 - 3n = Θ(n^2)$       ∵ $\\frac{1}{14}n^2 ≤ \\frac{1}{2}n^2 - 3n ≤ n^2$, for all $n≥7$            $an^2 + bn +c = Θ(n^2), a&gt;0$​​​       ∵ $\\frac{a}{4}n^2 ≤ an^2 + bn +c ≤ \\frac{7a}{4}n^2$, for all $n≥2\\max{(\\frac{│b│}{a}, \\sqrt{\\frac{│c│}{a}})}$           Big-O Notation      ■ 정의   \\[f(n)=O(g(n))\\\\ \\text{$0≤f(n)≤cg(n)$, for all $n≥n_0$}\\]  위 조건을 만족하는 양의 실수 $c, n_0$​​가 존재할 때, 다음과 같이 표현할 수 있다.      $g(n)$: $f(n)$​​에 대한 Aymptotically Upper Bound       ■ 특징      $f(n)=Θ(g(n)) ⇒ f(n)=O(g(n))$​   $O$: worst-case running time을 bound하기에 적합하다.       ■ 예시      $ 2n^2 + 8n - 2 = \\text{$O(n^2)$, $O(n^3)$, $O(n^4)$, …}$​​​​       Big-Ω Notation      ■ 정의   \\[f(n)=Ω(g(n))\\\\ \\text{$0≤cg(n)≤f(n)$, for all $n≥n_0$}\\]  위 조건을 만족하는 양의 실수 $c, n_0$​​가 존재할 때, 다음과 같이 표현할 수 있다.      $g(n)$: $f(n)$​​에 대한 Aymptotically Lower Bound       ■ 특징      $f(n)=Θ(g(n)) ⇒ f(n)=Ω(g(n))$​​   $Ω$: best-case running time을 bound하기에 적합하다.       o-Notation      ■정의   \\[f(n)=o(g(n))\\\\ \\text{$0≤f(n)≤cg(n)$, for all $n≥n_0$ and all constant $c&gt;0$}\\]  위 조건을 만족하는 양의 실수 $c, n_0$​​가 존재할 때, 다음과 같이 표현할 수 있다.           $g(n)$: $f(n)$​​​​​에 대한 Upper Bound이면서 Asymptotically Tight하지 않다.       ※ Asymptotically Tight하다는 것은, 하한과 상한 경계가 모두 존재함을 의미한다. ※           ■ 특징      Big-O Notation은 Aysmptotically Tight할 수도, 안 할 수도 있지만     Little-o Notation은 언제나 Aysmptotically Tight하지 않다.          ■ 예시      $2n=O(n^2), o(n^2)$​​​​​   $2n^2=O(n^2)$  but $2n^2 ≠o(n^2)$       ω-Notation      ■ 정의   \\[f(n)=ω(g(n))\\\\ \\text{$0≤f(n)≤cg(n)$, for all $n≥n_0$ and all constant $c&gt;0$}\\]  위 조건을 만족하는 양의 실수 $c, n_0$​​가 존재할 때, 다음과 같이 표현할 수 있다.      $g(n)$: $f(n)$에 대한 Lower Bound이면서 Asymptotically Tight하지 않다.       ■ 특징      Ω-Notation은 Aysmptotically Tight할 수도, 안 할 수도 있지만     ω-Notation은 언제나 Aysmptotically Tight하지 않다.      ■ 예시      $2n^2=Ω(n), ω(n)$​​​   $2n^2=Ω(n)$​​​  but $2n^2 ≠ω(n)$​​​       정리         일반적으로는 tight boundary를 갖는 Θ-Notation과 worst-case를 가정하는 Big-O Notation을 주로 사용한다.       Notations Defined with Limit      \\[\\begin{cases} f(n)=o(g(n)) : \\lim_{n→∞}{\\frac{f(n)}{g(n)}} = 0\\\\ f(n)=O(g(n)) : \\lim_{n→∞}{\\frac{f(n)}{g(n)}} &lt; ∞\\\\ f(n)=Θ(g(n)) : 0 &lt; \\lim_{n→∞}{\\frac{f(n)}{g(n)}} &lt; ∞\\\\ f(n)=Ω(g(n)) : 0 &lt; \\lim_{n→∞}{\\frac{f(n)}{g(n)}}\\\\ f(n)=ω(g(n)) : \\lim_{n→∞}{\\frac{f(n)}{g(n)}} = ∞\\\\ \\end{cases}\\]      ■ 특징           Transitivity   \\[\\text{$f(n)=Θ(g(n))$ and $g(n)=Θ(h(n))$}\\\\ \\text{⇔ $f(n)=Θ(h(n))$ for Θ, O, o, Ω, ω}\\]           Reflexivity   \\[\\text{$f(n)=Θ(f(n))$ for Θ, O, Ω}\\]           Symmetry   \\[\\text{$f(n)=Θ(g(n))$ ⇔ $g(n)=Θ(f(n))$}\\]           Transpose Symmetry   \\[\\text{$f(n)=Θ(g(n))$ ⇔ $g(n)=Ω(h(n))$}\\\\ \\text{$f(n)=o(g(n))$ ⇔ $g(n)=ω(h(n))$}\\]          Common Classes   일반적으로 자주 사용되는 몇몇 Notation은 다음과 같이 이름붙여진다.      $Θ(1)$ : constant   $Θ(\\log(n))$​​ : logarithmic   $Θ(n)$​​ : linear   $Θ(n\\log(n))$​ : n log n   $Θ(n^2)$​​ : quadratic   $Θ(n^3)$​​ : cubic   $Θ(2^n, e^n, 4^n, …)$​​ : exponential       ■ Weak Ordering      \\[Θ(1) &lt; Θ(\\log(n)) &lt; Θ(n) &lt; Θ(n\\log(n)) &lt; Θ(n^2) &lt; Θ(n^3) &lt; Θ(4^n)\\]      응용           n개의 floating point value들이 있는 list를 합하는 경우:       n번의 operation이 필요하므로 $Θ(n)$ algorithm            run-time이 $O(n^d)$인 경우, polynomial time complexity를 갖는다고 말할 수 있다.       ※ 일반적으로, polynomial time complexity를 가지면 Efficient(효율적)이다. ※           polynomial-time을 갖지 않는다고 알려진 (혹은 모르는) algorithms을 Intractable하다고 한다.       e.g., : Traveling salesman problem(TSP)는 아직 polynomial-tim을 갖는 알고리즘이 알려지지 않았다. 지금까지 알려진 best algorithm은 $Θ(n^2 2^n)$이다.          References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91        ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo05/",
        "teaser": null
      },{
        "title": "[ALGO-06] Lect6. 코드블록 단위의 복잡도 분석",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/              이번 시간에는 알고리즘에서 블록 단위의 시간/공간 복잡도를 분석하는 방법에 대해 배운다.   코드 블록 단위의 복잡도 분석   Instruction 의 개념을 배우고, Code Sequence가 있을 때 전체 Complexity를 표현하는 방법에 대해 배운다.       Motivation   알고리즘을 분석하는 목적은 코드 블록 단위로 다양한 파라미터에 대한 asymptotic run time 혹은 asymptotic memory requirements를 결정하기 위함이다.       ■ Asymptotic Behavior of Algorithms   알고리즘의 Asymptotic Behavior이란, scale(n)에 따라 알고리즘이 어떻게 동작하는지에 관한 것이다.       ● 예시: 알고리즘 A, B에 대한 complexitiy가 각각 $f_A(n)=Θ(n^2)$, $f_B(n)=Θ(n\\log_2n))$인 경우           $n=2k$일 때   \\[f_A(n) = (2k)^2 = 4k^2\\\\ f_B(n) = (2k)\\log_2(2k) = 2k(\\log_2(k)+\\log_2(2)) = 2k\\log(k)+2k\\]           $n=10k$일 때   \\[f_A(n) = (10k)^2 = 100k^2\\\\ f_B(n) = (10k)\\log_2(10k) = 10k(\\log_2(k)+\\log_2(10)) = 10k\\log(k)+33.2k\\]      위 예시를 통해 n이 5배가 될 때, $f_A$는 25배로 증가하고 $f_B$는 약 5배로 증가함을 알 수 있다.       Machine Instructions        프로그램이 어떤 Instruction으로 구성되었는지를 알면 프로세서에 따라 어느정도의 시간이 소요되는지를 계산할 수 있다.    프로세서(CPU,GPU)는 한정된 숫자의 연산(e.g., 덧셈, 뺄셈, 곱셈, 나눗셈)만을 수행할 수 있다. 이러한 연산을 Instruction이라고 하며, Instruction set은 프로세서에 따라 다르다. 따라서 프로그램을 컴파일 할 때는 꼭 타겟(e.g., ARM, x86)을 지정해주어야 machine이 이해할 수 있는 언어로 번역되어 동작가능해진다.       Constant 수행시간을 갖는 연산   아래의 연산들은 Machine Instruction에 매핑되어있기 때문에 fixed number of cycle에 수행될 수 있다. 즉, Constant 수행시간$Θ(1)$)을 갖는다.           Retieving/Storing variables from memory : 메모리로부터 변수 검색 및 저장            Variable assignment (=) : 메모리에 특정값 할당            Integer Operations (+, -, *, /, %, ++, –) : 덧셈/뺄셈/곱셈/나눗셈 등의 산술 연산            Logical Operations (&amp;&amp;, ││, !): AND/OR와 같은 논리 연산            Bitwise Operations (&amp;, │, ^, ~)            Relational Operations (==, !=, &lt;, &lt;=, =&gt;, &gt;) : 값의 비교            Memory Allocation and Deallocation (new, delete) : 메모리 할당           ■ 예시1. Swap Algorithm   int tmp = a; a = b; b = tmp;   위 알고리즘은 3줄의 코드로 구성되어있지만 각각이 Θ(1)의 수행시간을 가지므로, 알고리즘이 Θ(1) 시간을 갖는다고 말할 수 있다.       ■ 예시2. AVL 트리의 노드 재정렬   Tree_node *lrl = left-&gt;right-&gt;left; Tree_node *lrl = left-&gt;right-&gt;right; parent = left-&gt;right; parent-&gt;left = left; parent-&gt;right = this; left-&gt;right = lrl; left = lrr;   위 알고리즘 역시 모든 코드가 Θ(1)의 수행시간을 가지므로 Θ(1)의 시간을 갖는 알고리즘이라고 말할 수 있다.       시간복잡도의 Dominant Term   가령 어떤 알고리즘이 Θ(1)의 시간복잡도를 갖는 코드와 Θ(n)의 시간복잡도를 갖는 코드로 구성되어있을 때, 이 알고리즘은 Θ(1+n)=Θ(n)의 복잡도를 갖는다고 말한다. 즉, 다항식으로 구성된 시간복잡도는 dominant term으로 구성된 시간복잡도라고 말할 수 있다.       ■ 예시: Θ(n) 시간 복잡도를 갖는 알고리즘   int *Increase_Capacity(int *_array, int _n, int _delta) { \tint *array_old = _array; \t_array = new int [_n + _delta];          for(int i=0; i&lt;_n; i++) \t\t_array[i] = array_old[i];         \tdelete[] array_old;     return _array; }   위 알고리즘은 위의 두 줄과 아래의 두 줄은 모두 Θ(1)의 시간복잡도를 갖지만, 3~4번째 줄(for문)이 machine instruction을 _n회 반복하므로 Θ(n)의 시간복잡도를 갖는다. 따라서 $Θ(1+n+1)=Θ(n)$의 시간복잡도를 갖는 알고리즘이라고 말할 수 있다.   이전에 배웠던 Weak Ordering을 이용하면 코드 시퀀스의 복잡도를 dominant term만으로 이용하여 설명할 수 있다.       알고리즘 복잡도 분석   프로그램 언어에서 사용하는 Control Statement(e.g., if, for)에 대해 Complexity를 계산하는 방법에 대해 배운다.       ■ Control Statement의 종류      Conditional Statements : if, switch   Condition-controlled Loops : for, while, do-while   Count-controlled Loops : for i from 1 to 10 do … end do;   Collection-controlled Loops : foreach (int i in array) {…}       Conditional Statements   Conditional Statement의 대표적인 예로는 `if`, `switch`가 있다.          Conditional Statement의 runtime은 다음과 같이 구성된다.           condition (test)에 대한 runtime            실행될 body에 대한 runtim           일반적인 condition(e.g., n&gt;5)에 대한 runtime은 Θ(1)이지만, condition에서 function call을 하는 경우는 constant runtime을 가지지 않을 수도 있다.       ■ 예시1. Factorial Algorithm   int Factorial(int _n) { \tif( _n == 0)         return 1;     else         return _n * Factorial( _n -1 ); }   _n=0인 조건을 제외하면 언제나 recursive하게 함수를 호출한다는 것을 명확하게 알 수 있다. 따라서 complexity를 비교적 쉽게 계산할 수 있다.       ■ 예시2. Find Max Algorithm   template &lt;typename Type&gt; Type Find_Max(Type *_array, int _n) {     Type maxVal = _array[0];     for(int i=1; i&lt;_n; i++)     {         if(_array[i] &gt; maxVal)             maxVal = _array[i];  // ·······ⓐ     }     return maxVal; }   위 코드에서 ⓐ 코드가 몇 번 실행되는지는 데이터 _array에 따라 달라진다.           _array가 작은 것부터 큰 것까지 순서대로 정렬된 경우       for문을 돌 때마다 매번 최대값이 갱신되어 ⓐ가 언제나 실행된다.            _array가 큰 것부터 작은 것까지 순서대로 정렬된 경우       _array[0]이 최대값이므로 ⓐ는 한 번도 실행되지 않는다.           예시2와 같이 데이터의 분포에 따라 코드의 시행횟수가 달라지는 경우가 있기 때문에 이러한 변수까지 고려하여 알고리즘의 complexity를 계산하기는 어렵다. 따라서 이러한 경우는 최악의 경우(모든 데이터에 대해서 한 번씩 수행될 때)와 평균적인 경우(일반적인 데이터가 주어졌을때, 혹은 랜덤 분포를 갖는 데이터일 때)에 대해서 논할 수 있다.       Switch   switch문에서 case는 변수가 아닌 상수에 대해서만 프로그래밍이 가능하다. 따라서 미리 정의된 machine instruction에 의해, 바로 해당 값의 binary로 점프할 수 있도록 최적화 되어있기때문에 if-else문에 비해 switch문이 조금 더 빠르다.       Conditional-controlled Loops   Conditional-controlled Loop(반복문)의 대표적인 예로는 for, while, do-while가 있다.          Conditional-controlled Loop의 runtime은 다음과 같이 구성된다.           Initialization에 대한 runtime            Condition Check에 대한 runtime            Increment에 대한 runtime           for(i=0; i&lt;n; i++)인 Conditional-controlled Loop에 대한 runtime은 다음과 같다.      Initialization, Condition Check, Increment에 대한 runtime은 모두 Θ(1)이다.   body에 break이나 return이 없다면, Conditional-controlled Loop는 Ω(n)의 runtime을 갖는다(즉, 최소 n이 소비됨).   body의 runtime이 Θ(f(n))이면, Loop는 Θ(nf(n))의 runtime을 갖는다.   body의 runtime이 O(f(n))이면, Loop는 O(nf(n))의 runtime을 갖는다 (즉, 최악의 경우 nf(n)이 소비됨)       ■ 예시1.   int sum = 0; for(int i=0; i&lt;n; i++) {     sum += 1;\t// Θ(1) }   위 코드는 $Θ(n·1)=Θ(n)$의 run time을 갖는다.       ■ 예시2.   int sum = 0; for(int i=0; i&lt;n; i++) {     for(int j=0; j&lt;n; j++)         sum += 1;\t// Θ(1) }   위 코드는 $Θ(n·n·1)=Θ(n^2)$의 run time을 갖는다.       ■ 예시3.   for(int i=0; i&lt;n; i++) {     // search through an array of size m     // O(m); }   위 코드는 $O(n·m)$의 run time을 갖는다.       ■ 예시4.   for(int i=0; i&lt;n; i++){    // code which is Θ(f(i,n))}   위 코드의 run time은 다음과 같다.   \\[Θ(1 + \\sum_{i=0}^{n-1}{(1+f(i,n))})\\]  ※ 첫번째 항은 initialization(i=0)에 대한 run time Θ(1)을 의미한다.      ■ 예시5.   int sum = 0;for(int i=0; i&lt;n; i++){    for(int j=0; j&lt;i; j++)    {        sum += i+j; // Θ(1)    } // Θ(1+i·1)} // Θ(1+SUMⁿ(1+i·1))   위 코드의 run time은 다음과 같다.   \\(Θ(1+\\sum_{i=0}^{n-1}{(1+i)}) \\\\ = Θ(1+n+\\sum_{i=0}^{n-1}{i}) \\\\ = Θ(1+n+\\frac{n(n-1)}{2}) \\\\ = Θ(n^2)\\)     Serial Statements   ■ 예시1. 적합한 Notation      $\\text{size}=n$인 random list로부터 maximum entry를 찾는 경우에는 모든 값을 조사해야하므로 $\\text{run time}=Θ(n)$이다.   $\\text{size}=n$인 random list로부터 특정값(particular entry)를 찾는 경우 최악의 경우 모든 값을 조사해야하므로 $\\text{run time}=O(n)$이다.       ■ 예시2. leading term 기준으로 Notation 결정      $O(n) + O(n^2) + O(n^4) = O(n+n^2+n^4) = O(n^4)$   $O(n) + Θ(n^2) = Θ(n^2)$   $O(n^2) + Θ(n) = O(n^2)$   $O(n^2) + Θ(n^2) = Θ(n^2)$   코드에 따라서는 Notation이 섞여있을 수 있다. 이런 경우 leading term 기준으로 Notation을 표기한다.       Functions   ■ 개념   Function(혹은 subroutine)이란 별도로 분리되어 존재하는 코드를 의미한다. 가령, mathmatical functions과 같이 반복적인 연산을 하거나 initialization과 같이 연결된 tasks를 모아서 함수로 만들 수 있다.       ■ Function Call   Function Call을 하면, 어떤 프로그램의 binary로 점프를 하여 해당 부분을 수행한 뒤, 다시 본래의 프로그램 binary위치로 복귀하게 된다. 즉, Function Call의 과정은 다음과 같다.      함수를 구동하기 위한 적절한 환경을 구축한다.   함수에 넣어 줄 파라미터를 처리한다.   subroutine으로 점프한다.   subroutine을 실행한다.   return value를 처리한다.   clean up   위의 일련의 과정은 어떻게보면 Overhead이지만, 최신 프로세서는 이러한 과정을 constant time에 해줄 수 있는 instruction을 가지고 있다. 따라서 function call에 대한 overhead는 Ω(1)이다 (즉, 최대 constant time).       ■ 함수 안에서 발생하는 run time   함수 안에서 발생하는 run time은 $T_{f(n)}$ 혹은 $T(N)$이라고 표현한다.   ※ 이때 N은 함수가 받아들이는 파라미터의 사이즈를 의미한다. ※      References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91        ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo06/",
        "teaser": null
      },{
        "title": "[ALGO-07] Lect7. 삽입 정렬과 합병 정렬의 비교 분석을 통한 재귀적 알고리즘 복잡도 도출 방법",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/              이번 시간에는 삽입 정렬과 합병 정렬의 비교 분석을 통한 재귀적 알고리즘 복잡도 도출 방법에 대해 배운다.       삽입 정렬, Insertion Sort   Insertiong Sort란, 정렬된 array가 주어졌을 때, 새로운 element를 정렬 규칙이 깨지지 않도록 올바른 위치를 찾아서 삽입해주는 알고리즘이다.       Sorting Problem   Sorting 문제는 다음과 같이 구성된다.      Input: A sequence of $n$ numbers $&lt;a_1, a_2, …, a_n&gt;$   Output: A permutation (reordering) of the input sequence, $&lt;b_1, b_2, …, b_n&gt;$, such that $b_1 ≤ b_2 ≤ … ≤ b_n$       Code   Swap 알고리즘을 사용하는 방식과 사용하지 않는 방식, 두 가지 방식으로 Insertion Sort Algorithm 코드를 작성해볼 수 있다.   Insertion Sort with Swap   template &lt;typename Type&gt; void Insertion_Sort(Type *_array, int _n) {     for(int i=1; i&lt;_n; i++)     {         for int j=i; j&gt;0; j--)         {             if(_array[j-1] &gt; _array[j])                 std::swap(_array[j-1], _array[j]);             else                 break;         }     } }   코드 설명:          Complexity:      Swap Operation : $Θ(1)$   for Loop (j) : $O(1 + \\sum_{j=0}^{i}{1}) = O(1+i) = O(i)$   for Loop (i) : $O(1 + \\sum_{i=1}^{n}{i}) = O(1+\\frac{n(n+1)}{2}) = O(n^2)$   결과적으로 Insertion Sort는 $O(n^2)$의 Time Complexity를 갖는다.       Insertion Sort without Swap   Swap Algorithm은 세 줄의 코드로 구성되어 있다.   Swap Algorithm: \ttmp = a; \ta = b; \tb = tmp;   이러한 Swap 알고리즘을 사용하지 않으면 보다 최적화된 알고리즘을 얻을 수 있다.   template &lt;typename Type&gt; void Insertion_Sort_without_Swap(Type *_array, int _n) {     for(int i=1; i&lt;_n; i++)     {         Type tmp = _array[i];         for(int j=i; j&gt;0; j--)         {             if(_array[j-1] &gt; tmp) // 오름차순이 아닌 경우                 _array[j] = _array[j-1];             else             {                 _array[j] = tmp; \t            break;             }         }                  if(_array[0] &gt; tmp)             _array[0] = tmp;     } }   코드 설명:          합병 정렬, Merge Sort   Merge Sort Algorithm은 divide and conquer 알고리즘이다.       Divide and Conquer 알고리즘   Divide and Conquer 알고리즘이란, 문제를 쪼개어 각각을 따로 처리한 다음 결과를 하나로 합치는 알고리즘을 말한다.   Merge Sort 알고리즘에서는, 주어진 하나의 array를 반으로 나누어 각각을 sort한 다음 merge operation을 통해 합쳐준다.       Merging Example   아래의 예시를 통해 Merging을 수행하는 방법에 대해 살펴보자.         두 array의 포인터가 가리키는 index 값을 비교한다.   더 작은 값을 갖는 array의 값을 result array에 넣고, 해당 array의 포인터를 오른쪽으로 이동시킨다.   위 과정을 반복한다.       &lt;link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\"&gt; &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"&gt;&lt;/script&gt; &lt;script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js\"&gt;&lt;/script&gt;  &lt;button type=\"button\" class=\"btn btn-info\" data-toggle=\"collapse\" data-target=\"#demo\"&gt;Code Run&lt;/button&gt; &lt;div id=\"demo\" class=\"collapse\"&gt;   &lt;iframe src=\"https://code.sololearn.com/ct5m9F874CWD\" style=\"width:100%; heigth:100%; resize: vertical;\"&gt;&lt;/iframe&gt; &lt;/div&gt;       단점   Merging Algorithm은 in-place로 동작하지 않는다는 단점이 있다. 즉, 병합하기 전에 언제나 병합된 결과를 담는 array를 memory allocation해야한다.        in-place란?      새로운 메모리 없이 구동될 수 있는 알고리즘을 의미한다.      in-place가 아닌 알고리즘은, 언제나 memory allocation을 해야만 한다.       Code      알고리즘은 대략 다음과 같다.      list를 절반 정도씩 쪼개어 두 개로 나눈다.   두 sub lists에 대해 merge sort를 재귀적으로 호출한다(Recursively call).   sorted lists 결과를 merge한다.       ■ Problem: Recursively Call   이론상으로는 sub-list를 하나의 element가 남을 때 까지 재귀적으로 merge sort하여 합치는 것이 맞다. 하지만 이러한 방식은 function call에 의해 발생하는 overhead 문제가 있다. 따라서 실질적으로는 어떠한 threshold를 이용하여 threshold 이하의 element가 남을 때 다른 sorting algorithm(e.g., insertion sort)을 사용하기도 한다.       Merge Sort   template &lt;typename Type&gt; void Merge_Sort(Type *_array, int _first, int _last) {     if(_last-_first &lt;= NUM_THRESHOLD)     {         Insertion_Sort&lt;Type&gt;(_array, _first, _last);     }     else     {         int midpoint = (_first + _last)/2;                  Merge_Sort&lt;Type&gt;(_array, _first, midpoint); \t\tMerge_Sort&lt;Type&gt;(_array, midpoint, _last);         Merge(_array, _first, midpoint, _last);     } }       ■ Complexity      if절(_last-_first&lt;=NUM_THRESHOLD) : $T(\\text{NUM_THRESHOLD})=Θ(1)$   else절            int midpoint : $Θ(1)$       Merge_Sort() : $T(\\frac{n-1}{2})$       Merge_Sort() : $T(\\frac{n-1}{2})$       Merge() : $Θ(n)$           Merge Sort의 Time Complexity $T$는 아래와 같이 recursive하게 나타낼 수 있다.   \\(T(n) =  \\begin{cases} \tΘ(1) &amp; \\text{if $n=1$}\\\\ \t2T(\\frac{n}{2})+Θ(n) &amp; \\text{if $n&gt;1$} \\end{cases}\\) 위의 재귀적으로 표현된 Time Complexity는 recursion tree 기법을 이용하여 Asymptotic Analysis를 할 수 있다.       ■ Recursion Tree   Recursion Tree를 이용하여 Asymptotic Analysis를 하는 방법은 다음과 같다.           위의 재귀적으로 표현된 Time Complexity를 아래와 같이 표현한다.   \\[T(n)= \\begin{cases} \tc &amp; \\text{if $n=1$}\\\\ \t2T(\\frac{n}{2})+cn &amp; \\text{if $n&gt;1$} \\end{cases}\\]           위의 표현식을 recursion tree로 표현한다.                   recursion tree의 모든 nodes의 합을 구한다. \\(T(n)=cn·\\log(n)+cn\\)       결과적으로 Time Complexity가 $O(n^2)$인 Insertion Sort보다 $O(n\\log(n))$인 Merge Sort 알고리즘이 더 효율적이다.       References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91        ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo07/",
        "teaser": null
      },{
        "title": "[ALGO-09] Lect9. 힙 자료구조의 정의와 연산 및 복잡도 분석",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/              이번 시간에는 힙 자료구조의 정의와 연산 및 복잡도 분석에 대해 배운다.       힙 자료구조의 정의와 연산   이진힙 자료구조의 정의와 Push, Pop 연산 방법에 대해 학습한다.       Binary Heap   Binary Heap은 Tree에서 최소값, 최대값을 빠르게 찾을 수 있도록 하는 자료구조이다. Binary Heap은 특성에 따라 다음과 같이 분류될 수 있다.      Min Heap: (sub) tree의 모든 root node가 자손보다 언제나 값이 작은 tree   Max Heap: (sub) tree의 모든 root node가 자손보다 언제나 값이 큰 tree   ※ sibling끼리는 어떠한 관계성이 존재하지 않는다. ※  ■ (예시) Binary Min Heap          Operations   Binary Heap에서 수행할 수 있는 연산들은 다음과 같다.      Top: 가장 root node의 값을 찾음   Pop: root에 있는 값을 제거   Push: 어떤 임의의 값을 Heap의 특성을 깨지 않으면서 삽입       Top   ■ 방법   tree의 root node의 값에 접근한다.       Pop   ■ 방법         Root Node의 값을 제거 (그 결과, Root Node의 값이 비게 됨)   Root Node의 자식 Nodes 중 더 작은 값을 Root Node로 Promotion (그 결과, 자식 Node의 값이 비게 됨)   2번 과정을 반복       Push   Heap에 어떤 값을 삽입하는 방법에는 크게 두 가지가 있다.      Leaf Node에 삽입하는 방법 (item이 적당한 위치를 찾을 때까지 올라가는 방법)   Root Node에 삽입하는 방법 (item이 적당한 위치를 찾을 때까지 내려가는 방법)   우리는 이 중 일반적으로 자주 사용되는 ① Leaf Node에 삽입하는 방법에 대해 알아본다.       ■ 방법         아무 Leaf Node에 item을 삽입한다.   Binary Heap의 조건을 만족할 때까지 해당 item을 Promotion한다. (부모와 자리를 바꿈)   2번 과정을 반복하여 Binary Heap의 조건을 만족하도록 만든다.        Percolation(삼투압 작용)      Binary Min Heap에서 Push를 할 때, 규칙에 의해 Node가 아래로 내려오지는 못하고 위로만 올라가는 현상을 의미     (혹은 위로 올라가지는 못하고 밑으로만 내려가는 현상을 의미)           힙 자료구조의 구현 및 복잡도   완전 이진트리 기반의 힙 자료구조의 구현 방법 및 각 연산의 평균/최악의 복잡도에 대해 학습한다.       Perfect Binary Trees   Min Heap을 구현할 때, Perfect Binary Tree에 가깝도록 유지하면, 보다 효율적인 구현이 가능해진다. 따라서 Binary Heap을 구현하기에 앞서, Perfect Binary Tree에 대한 개념을 알아본다.   ■ 정의      Binary Tree의 $\\text{height}=h$일 때, 모든 leaf nodes가 다음의 조건을 만족한다.   \\[\\text{depth}=h\\]      ● 재귀적인 정의         $h=0$인 경우: node가 1개만 있는 tree   $h≥1$인 경우: sub-tree가 모두 perfect binary tree여야 함       ■ 특징   Perfect Binary Tree의 $\\text{height}=h$일 때, nodes의 총 개수 $n$은 다음과 같다.   \\[n=2^h-1\\]  ∵ 1, 3, 7, 15, 31, 63, …       Complete Binary Trees   min heap을 perfect binary tree에 가깝게 구현하면 좋지만, 데이터가 삽입/삭제되는 과정에 의해 nodes의 개수를 언제나 $n=2^h-1$에 맞출 수 없다. 따라서 perfect binary tree에 가까운 Complete Binary Tree 형태로 min heap을 구현하는 대안을 떠올려볼 수 있다.      Complete Binary Tree는 Perfect Binary Tree를 지향하는, Perfect Binary Tree로 변해가는 과정이라 생각할 수 있다. 즉, 왼쪽부터 Leaf Node를 채워 나가며 Perfect Binary Tree가 되도록하는 Tree 형태를 Complete Binary Tree라고 한다.       ■ 특징   Complete Binary Tree의 heigth $h$는 다음과 같다.   \\[h=[\\log(n)]\\]      Operations   Complete Binary Tree의 형태를 최대한 유지하며 연산을 수행하는 방법에 대해 알아본다.   Push         가장 왼쪽에 위치한 leaf node의 sibling node의 위치에 item을 삽입한다.   item이 적당한 자리를 찾을 때 까지 promotion한다.       Pop         Root Node의 값을 제거 (그 결과, Root Node의 값이 비게 됨)   가장 마지막 leaf node의 값을 root node 자리에 채워 넣음   기존의 leaf node를 올바른 위치로 percolation down한다.       Heap의 구현   Tree를 BFS order로 순회를 한 결과를 array에 채워나가는 방법으로 Heap를 구현한다.   ● 예시      ※ 편의상 array 0은 비웠다. (Trivial Cost)※       ■ 특징           자식 node에 접근하는 방법         \\[\\begin{cases} \t\\text{(왼쪽 자식 node의 index)} = \\text{(본인 index)}*2\\\\ \t\\text{(오른쪽 자식 node의 index)} = \\text{(본인 index)}*2 + 1 \\end{cases}\\]           부모 node에 접근하는 방법  \\[\\text{(부모 node의 index)} = [\\text{(본인 index)}/2]\\]          Operations   Search   ※ $\\text{Time Complexity}=Θ(1)$ ※       Pop   ※ $\\text{Time Complexity}=Θ(\\lg(n))$ (∵ 최악의 경우, height만큼 올라갔다 height만큼 내려온다.) ※       Push   ※ $\\text{Time Complexity}=Θ(1) \\sim Θ(\\lg(n))$ (∵ 최선의 경우 움직이지 않고, 최악의 경우 heigth만큼 움직인다.)  ※   ※ 평균적인 Time Complexity는 다음과 같다. ※   \\[\\text{Time Complexity} =  \\frac{1}{n}\\sum_{k=1}^{h}{(h-k)2^k} =  \\frac{2^{h+1}-h-2}{n} =  \\frac{n-h-1}{n} = Θ(1)\\]  따라서 Push는 평균적으로 Constant Time안에 수행된다.       Remove           해당 item이 존재하는지 여부를 판단하는 Time Complexity = $O(n)$            가장 큰 값을 갖는 item을 제거하는 Time Complexity = $O(n)$       ※ 큰 item은 대부분 leaf level에 존재하므로 $\\frac{n}{2}$만큼의 탐색 시간이 걸린다. ※           Run-time Analysis                          Average       Worst                       Search       $O(n)$       $O(n)$                 Push       $O(1)$       $O(\\lg(n))$                 Pop       $O(\\lg(n))$       $O(\\lg(n))$                 Remove       $O(n)$       $O(n)$                   References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91           ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo09/",
        "teaser": null
      },{
        "title": "[ALGO-10] Lect10. Heap 정렬, Quick-Sort의 알고리즘 및 최선/평균/최악 복잡도",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/              이번 시간에는 Heap 정렬, Quick-Sort의 알고리즘 및 최선/평균/최악 복잡도에 대해 배운다.       힙 정렬   주어진 배열을 힙으로 변환하는 방법과, 힙 정렬 알고리즘 구동 원리 및 복잡도 분석에 대해 알아본다.       ■ Min Heap Sort   Min Heap의 item을 오름차순으로 정렬하는 방법은 다음과 같다.      주어진 min heap에서 root node의 값을 Pop한다.   재정렬된 min heap에서 root node의 값을 Pop한다.   모든 nodes가 Pop될 때 까지 2번 과정을 반복한다.   위의 알고리즘을 사용하여 Min Heap Sort를 수행할 때, Complexity는 다음과 같다.   \\[\\sum_{k=1}^{n}{\\lg(k)} =  \\lg(\\prod_{k=1}^{n}{k}) = \\lg(n!) \\approx n\\lg(n) \\\\  ∴ \\text{Complexity} = O(n\\lg(n))\\]  ※ Pop operation의 complexity=$O(\\lg(n))$ ※       ● 문제점   n nodes를 갖는 min heap을 sort하기 위해서는, 기존의 n길이 array 이외에 추가적인 n 길이 array가 필요하다. 즉, 위와 같은 min heap sort 알고리즘은 in-place로 동작하지 않는다.   ※ in-place: 추가적인 메모리 공간이 거의 요구되지 않는 특성 ※       Max Heap Sort   위와 같은 (in-place로 동작하지 않는) 문제점을 해결하기 위해 max heap sort 알고리즘을 생각해볼 수 있다.   Heap Sort Algorithm   Heap Sort Algorithm 과정은 다음과 같다.           주어진 Array를 Complete Tree형태로 표현한다. ※ min-heap도 max-heap도 아닌 그냥 binary-tree 형태이다. ※ ※ array index=0에서 부터 시작함 (∴ $\\text{자식 node}=2k+1 \\text{and} 2k+2$; $\\text{부모 node}=(k-1)/2$) ※                뒤에서부터 sub-tree별로 max-heap을 만족하도록 재정렬해준다.               Complexity   $\\text{depth}=k$인 node의 값이 이동할 수 있는 최대 경로의 길이는 $h-k$와 같으며, depth=k에서 nodes의 개수는 $2^k$개이다. 따라서 최악의 경우 heap sort algorithm을 수행하는 데에 필요한 node의 움직임 수는 다음과 같다.   \\[\\sum_{k=0}^{h}{2^k(h-k)} =  (2^{h+1}-1)-(h+1)\\]  이때 node의 개수 $n$에 대해 위 식을 고쳐 쓰면 다음과 같다.   \\[n=2^{h+1}-1\\text{이므로}\\\\ \\text{complexity}=O{((2^{h+1}-1)-(h+1))}=O(n-\\lg(n+1))\\\\ ∵\\ \\lg(n+1)=h+1\\]  최종적으로 in-place heapify algorithm은 $O(n)$의 복잡도를 갖게 된다.       Max-heap 2 Min-heap   위의 과정을 통해 구한 max-heap을 min-heap으로 변환하는 과정은 다음과 같다.      root node의 값을 Pop한다. (⇒ 마지막 leaf node가 그 자리를 채운 뒤, percolation down된다.)   기존의 leaf node값이 존재하던 자리를 Pop된 값이 채운다.   위 1~2과정을 min-heap이 될 때 까지 반복한다.       ■ Complexity   이전에 구했듯 Heapification을 수행하는 데에는 $Θ(n)$이 소요된다 (complete tree→max heap). 이후 max heap을 min heap으로 변환하는 과정에서는 n번의 pop과 insert가 수행되므로 $Θ(n\\lg(n))$이 소요된다. (∵ $\\text{pop의 copmlexity}=\\lg(n)$ )                      Case       Run Time       Comments                       Worst       $Θ(n\\lg(n))$       No worst case                 Average       $Θ(n\\lg(n))$                         Best       $Θ(n)$       All or most entries are same                   퀵 정렬의 평균/최악 복잡도 분석   퀵 정렬 알고리즘 구동 원리 파악 후 평균/최악의 자료 분포 도출 및 각 상황에서의 복잡도를 분석한다.       ■ 복습   현재까지 $Θ(n\\lg(n))$으로 동작하는 sorting algorithms 두 가지에 대해 알아보았다.      heap sort : in-place에 동작함   merge sort : heap sort보다 빠르지만 in-place에 동작하지 않음   이번에는 in-place에 가까우며 heap sort보다 속도가 빠른 Quick Sort 알고리즘에 대해 알아본다.       Quick Sort   ■ 특징           in-place에 가깝다.           평균적으로 Heap Sort Algorithm보다 속도가 빠르다.           Complexity는 다음과 같다.                                  Case           Time           Memory                                           Best           $Θ(n\\lg(n))$                                         Average           $Θ(n\\lg(n))$           $Θ(\\lg(n))$                             Worst           $Θ(n^2)$           $Θ(n)$                           ※ Quick Sort에서는 최대한 데이터의 분포를 worst case가 안되도록 만드는 것이 핵심! ※  ※ pivot값을 median값으로 고를 수 있다면 Best Case가 된다. ※  ※ pivot값이 최소/최대값이라면 Worst Case가 된다. ⇒ $T(n)=T(n-1)+Θ(n)=Θ(n^2)$ ※           Divide-and-Conquer 방식으로 동작한다.                                  Merge Sort           Quick Sort                                           middle point를 기준으로 왼쪽-오른쪽 array로 둘을 나눈다.           특정 값(pivot)을 기준으로 작은 값을 왼쪽, 큰 값을 오른쪽 array에 나눈다 (partitioning).                           또한, Merge Sort와 마찬가지로 array size가 굉장히 작은 경우 partitioning을 수행하는 대신에 Insertion Sort와 같은 알고리즘을 수행하는 것이 일반적이다.           Median-of-Three   pivot값으로 median값을 선택하는 경우가 가장 이상적인 Case이다. 일반적으로 Quick Sort에서 median값을 지향하는 pivot 선택 방법은 Median-of-Three 방법이다. Median-of-Three 방법은 다음과 같다.      주어진 Array에서 첫 번째, 마지막, 가운데 위치의 값(총 3개)의 median값을 pivot으로 고른다.   pivot값을 기준으로 partitioning을 한다. ⇒ sub-array 생성   각각의 sub-array에 대해 첫 번째, 마지막, 가운데 위치의 값의 median값을 pivot으로 고른다.   각각의 sub-array에 대해 pivot값을 기준으로 partitioning을 한다.   3~4의 과정을 반복한다.   이때 in-place로 동작하기 위해 다음과 같이 위의 과정이 수행된다.      ● partitioning 과정      3개의 값(index=first, middle, last) 중 pivot값(=median)을 구한 뒤 따로 값을 저장한다.   3개의 값 중 최소값은 index first에, 최대값은 index middle 위치에 저장한다.   first에서부터 오른쪽으로 값을 탐색하며 pivot값보다 큰 값을 찾는다. 동시에 last에서부터 왼쪽으로 값을 탐색하며 pivot값보다 작은 값을 찾는다.   pivot값을 기준으로 큰 값과 작은 값이 감지되면 두 값을 swap한다.   두 pointer가 만날 때 까지 3~4번의 과정을 반복한다.   반복 과정이 종료되면 두 개의 pointer 중 더 큰 값을 마지막 array index에 넣고, 빈 자리에 pivot값을 넣어준다.   위의 partitioning과정을 재귀적으로 수행하면 아래와 같이 Quick Sort의 결과를 얻을 수 있다.         과정 자세히 보기                Code Implementation   template &lt;typename Type&gt; void Quicksort(Type *_array, int _first, int _last) {     if(_last - _first &lt;= NUM_THRESHOLD)         Insertion_Sort&lt;Type&gt; (&amp;_array[_first], _last-_first);     else     {         Type pivot = Find_Pivot&lt;Type&gt;(_array, _first, _last);         int low = _first + 1;         int high = _last - 2;         while(_array[low] &lt; pivot) low++;         while(_array[high] &gt; pivot) high--;         while(low&lt;high)         {             std::swap(_array[low], _array[high]);             low++;\thigh--;             while(_array[low] &lt; pivot) low++;             while(_array[high] &gt; pivot) high--;         }         _array[_last-1] = _array[low];         _array[low] = pivot;         Quicksort(_array, _first, low);         Quicksort(_array, high, _last);     } }       Memory Requirement      pivot, start_index, last_index를 stack에 저장해야한다.   따라서 Quicksort의 Memory Requirement는 $Θ(\\lg(n))$이다. 한편, Worst-case에서는 memory requirment는 $Θ(n)$가 된다.       Run-time Summary                          Average Run Time       Worst-case Run Time       Average Memory       Worst-case Memory                       Heap Sort       $Θ(n\\lg(n))$       $Θ(n\\lg(n))$       $Θ(1)$       $Θ(1)$                 Merge Sort       $Θ(n\\lg(n))$       $Θ(n\\lg(n))$       $Θ(n)$       $Θ(n)$                 Quicksort       $Θ(n\\lg(n))$       $Θ(n^2)$       $Θ(\\lg(n))$       $Θ(n)$                   References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91           ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo10/",
        "teaser": null
      },{
        "title": "[ALGO-11] Lect11. 최소신장트리 문제에 대한 프림, 크루스칼 두 알고리즘",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/              이번 시간에는 최소신장트리 문제에 대한 프림, 크루스칼 두 알고리즘에 대해 배운다.       최소신장트리 문제와 프림 알고리즘   그래프에서 발생하는 여러가지 문제들을 해결하는 방법에 대해 알아본다.   Minimum Spanning Trees   ■ 정의      Minimum Spanning Tree란 Graph의 모든 nodes가 최소한의 weights로 연결되는 Tree 자료구조이다.       ■ 쓰임   각 도시를 연결짓는 도로를 건설하려고 할 때, 건설 비용을 최소화하면서 모든 도시를 연결할 수 있는 길을 찾는 문제에서 Minimum Spanning Tree 자료구조를 사용할 수 있다.       Spanning Tree      Spanning Tree란 Graph의 모든 vertex가 연결되도록 구성된 Tree이다.           vertex의 개수가 $V$개일 때, Tree는 언제나 $V-1$개의 edges를 가지므로, Graph로부터 $V-1$개의 edges를 고르는 문제라 생각할 수도 있다.            Spanning Tree가 Weighted Tree인 경우, Spanning Tree의 weight는 모든 edges의 weight 합으로 정의된다.   \\[\\text{Spanning Tree의 weight} = \\sum_{i=1}^{V}{w_i}\\]      ※ Spanning Tree의 weigth를 최소화하는 문제를 Minimum Spanning Tree라고 한다. ※            Spanning Tree가 Unweighted Tree인 경우, 모든 edges의 $\\text{weigth}=1$이라 가정할 수 있다. 따라서 이 경우 Minimum Spanning Tree의 weight는 언제나 $V-1$이다.           ■ 특징           Unique하지 않다.       $V-1$개의 edges를 선택했을 때, 어떤 vertex를 root로 보느냐에 따라 다른 Tree가 된다.       Algorithms   Minimum Spanning Tree문제를 해결하는 대표적인 알고리즘은 다음과 같다.           Prim's Algorithm           Kruskal's Algorithm      두 알고리즘 모두 Greedy Algorithm임에도 불구하고 최적의 값을 도출할 수 있음이 증명되었다.        Greedy Algorithm이란?      매 순간 최선의 선택을 취하는 알고리즘       Prim’s Algorithm   개념   $N$개의 vertex를 갖는 graph로부터 minimum spanning tree를 결정하는 방법은 다음과 같다. $k$개의 vertex로 이루어진 minimum spanning tree가 주어졌다면, 기존의 minimum spanning tree에 $k+1$번째 vertex를 추가해나가는 방식으로 $N$개의 vertex를 갖는 minimum spanning tree를 결정할 수 있다.   이때 최소한의 weight를 갖도록 $k+1$번째 vertex를 선택하는 방법은, 기존의 tree에 직접적으로 연결된 edges 중 최소한의 weight를 갖는 edge를 선택하는 것이다.       ■ 예시      $e_k$를 minimum spanning tree와 $v_{k+1}$을 연결하는 최소의 weight를 가진 edge라고 할 때, 위 예시에서 $v_{k+1}$을 연결하기 위한 edge로 $e_k$를 선택하는 것이 가장 합리적인지를 생각해보자.    $e_k$를 선택하지 않는다면 다른 어떤 경로($\\tilde{e}$)를 통해 $v_{k+1}$으로 연결되어야 한다. 이때 $e_k$가 $v_k$를 연결하는 최소의 weight를 가졌으므로 다른 경로 $\\tilde{e}$는 반드시 $e_k$보다 weight값이 클 수밖에 없다. 따라서 $e_k$를 선택하는 것이 가장 합리적인 결정이다.       특징           어떤 vertex에서 시작하든지 상관 없다.          Implementation   ■ 배경 지식      distance: 현재 minimum spanning tree로부터 어떤 특정 vertex까지의 거리 ※초기값은 ∞. 단, root vertex의 distance=0 ※   visit: 해당 vertex가 이미 minimum spanning tree에 포함되었는지를 체크하는 flag ※ 초기값=0 ※   parent: 어떤 vertex가 minimu spanning tree에 포함될 때, 어떤 node와 직접적으로 연결되었는가를 체크하는 flag ※ 초기값=NULL ※       ■ 알고리즘           minimum distance를 갖는 unvisited vertex를 선택한다.       해당 vertex를 visited상태로 바꿔준다.   각 인접 vertex에 대한 distance를 고려하여 기존의 distance값을 업데이트해준다.   ①모든 vertex가 visited상태이거나 ②모든 unvisited vertex의 distance값이 ∞가 될 때 까지 1~3의 과정을 반복한다. ※ ②의 경우 unconnected graph ※       ■ 예제          복잡도      최종적으로 Prim’s Algorithm의 복잡도는 $Θ(V^2)$가 된다.       최적화   minimum distance를 구하기에 가장 적합한 자료구조는 min-heap 자료구조이다.      min-heap의 nodes의 개수는 $V$개가 된다. ⇒ $Θ(V)$의 memory and run time   최소값을 구하는 데(Pop)에 걸리는 비용은 $\\lg(V)$이다.   따라서 최소값을 찾는 데에 드는 총 $(V-1)\\lg(V)$가 소요된다. (∵ V-1번 위 과정을 반복) ⇒ $O(V\\lg(V))$   최소값을 구한 뒤, neighbors에 대해 distance를 update하는 데에 $E\\lg(V)$가 소요된다. (∵ edge만큼 연산되며, 각 update는 $\\lg(V)$가 소요됨) ⇒ $O(E\\lg(V))$   따라서 total run time은 다음과 같다. \\(O(V\\lg(V) + E\\lg(V)) = O(E\\lg(V))\\) ※ 이떄 edges의 개수 $E$는 최대 $V^2$의 값을 가질 수 있다. 따라서 edges의 개수가 많은 경우 오히려 min-heap(priority queue)를 사용하지 않는 것이 더 효율적이다. ※           Kruskal’s Algorithm   크루스칼 알고리즘은 프림 알고리즘 보다 간단한 알고리즘이다.   크루스칼 알고리즘은 weight에 따라 edges를 정렬하고, 최소 weight부터 차례로 minimum spanning graph에 추가시켜 나간다. 이때 tree에 cycle이 생기지 않도록(∵ cycle이 생기면 tree가 아니라 graph가 되니까) 추가하는 것이 핵심이다.       ■ 예시         graph 내에 존재하는 모든 edges를 weight 순서대로 정렬한다.   위에서부터 하나씩 순회하며 minimum spanning tree에 삽입 여부를 결정한다.            해당 edge를 포함시킬 때 cycle이 생성되면 무시한다. (skip)       해당 edge를 포함시킬 때 cycle이 생성되지 않으면 추가한다. (addition)           2번 과정을 $V-1$개의 edges를 고를 때 까지 반복한다.       복잡도      edges를 정렬하기 위한 merge sort(or heap sort) : $O(E\\lg(E))$   cycle 체크를 위한 DFS(or BFS) : $O(E)=O(V)$ (∵$ E&lt;V$이므로)   따라서 최종적인 run-time은 다음과 같다. \\(O(E\\lg(E) + E·V)\\\\ = O(E·V)\\ \\ (∵ E=O(V^2)\\text{이므로 }\\lg(E)=\\lg(V))\\)     특징      프림알고리즘($O(E\\lg(V))$ 혹은 $O(V^2)$)에 비해 비효율적($O(E·V)$)이다.        알고리즘이 단순하다.       cycle check를 위해 DFS(or BFS) 대신 disjoint set을 사용하면 complexity를 낮출 수 있다.           References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91           ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo11/",
        "teaser": null
      },{
        "title": "[ALGO-12] Lect12. 서로소 집합 자료구조의 원리 및 크루스칼 알고리즘",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/            이번 시간에는 서로소 집합 자료구조의 원리에 대해 이해하고, 크루스칼 알고리즘을 적용해본다.       Disjoint Set, 서로소 집합   서로소 집합(Disjoint Set) 자료구조의 원리를 이해하고, 각 연산의 복잡도를 분석해본다.   개념   Disjoint Set이란 공통으로 중복되는 데이터가 없는 집합 $S_1, S_2, ... S_k$의 집합이다. 이때 각 집합 요소($S_k$)는 하나의 대표 값으로 표현될 수 있다.   \\[C = \\{S_1, S_2, ..., S_k\\}\\]      Operations   Disjoint Set에 대해 대표적으로 세 가지 Operations을 수행할 수 있다.      make-set(x): 멤버가 오직 x뿐인 새로운 집합을 생성한다. (∴ 대표값=x)   union-set(x,y): x, y로 대표되는 집합을 합집합한다.   find-set(x): x를 멤버로 갖는 집합의 대표값을 반환한다.       ■ 응용      find-set(x)==find-set(y)인 경우, 두 멤버 x, y는 같은 집합에 포함되어 있다.&gt;   connected components       Connected Components      모든 edges에 대해 union-set 연산을 수행한 최종 결과로 connected components를 얻을 수 있다. ※ DFS에 비해 비효율적인 방법 ※       Implementation   ■ Poor Implementation   Disjoint Set 자료구조를 구현하기위한 가장 단순한 방법으로 아래와 같이 2D array를 사용하는 방법을 생각해볼 수 있다.      모든 vertex에 대한 대표자를 표시하여 구현할 수 있다.      멤버 x의 집합을 알아내는 Finding 연산: $Θ(1)$&gt;   union-set(x,y): $Θ(n)$ (∵ y가 속한 집합의 모든 멤버를 조사하여 x가 속한 집합의 대표자로 대표자값을 바꿔줘야 한다.)       ■ Implementation   2D array를 사용하는 방식은 연산에 비용이 많이 소요되기 때문에 실제로는 아래와 같이 트리 구조로 Disjoint Set을 구현한다.      대표자를 root값으로 두고, 멤버를 자식 nodes로 둔다.           union-set(x,y)을 수행하는 경우 단순히 root값을 자식 node로 추가해준다.              find-set(x): $O(h)$   union-set(x,y): $O(h)$   ※ 일반적인 tree 자료구조는 부모가 자식 nodes를 point하지만, disjoint set을 구현하기 위한 tree 구조에서는 자식이 부모를 point하는 방향으로 구현된다. ※       ● Generation of Disjoint Set   $n$개의 데이터에 대해 Disjoint Set를 구현하기 위해 길이가 $n$인 array를 생성한다. ※ 이때 parent의 초기값은 자기 자신이 된다. ※   parent = new int[n]; for(int i=0; i&lt;n; i++)     parent[i] = i;       ● Find-set(x): x의 parent를 찾는 연산   parent값이 자기 자신이 될 때 까지 parent를 참조한다.   int Find_Set(int x) {     while(parent[x] != x)         x = parent[x];     return x; }   \\[\\text{Complexity} : T_{\\text{find}}(n) = O(h)\\]      ● Union-set(x,y): x와 y가 속한 집합의 합집합   x와 y의 root parent를 구한 뒤, y의 root parent의 parent값을 x의 root parent로 변경해준다.   void Union_Set(int x, int y) {     x = Find_Set(x);     y = Find_Set(y);          if(x!=y)         parent[y] = x; }   \\[\\text{Complexity} : T_{\\text{union}}(n) = 2T_{\\text{find}}(n)+Θ(1) = O(h)\\]  ※ 이때 height값이 큰 집합을 x로 사용하면 height값을 줄여 complexity를 작게 만들 수 있다. ※       Example          ■ union-set(1,3)          ■ find-set(1) and find-set(3)          ■ union-set(3,5)          ■ union-set(5,7)          Worst-Case Scenario   같은 높이를 갖는 tree x, y를 union-set(x,y)하는 경우 tree x의 height가 커진다.      이때 각 level에서 nodes의 개수를 세 보면 Pascal's triangle의 수와 같다는 걸 알 수 있다.       Pascal’s Triangle   \\[\\begin{pmatrix} n\\\\ m \\end{pmatrix} = \\begin{cases} \t1 &amp; \\text{$m=0$ or $m=n$}\\\\ \t\\begin{pmatrix} n-1\\\\ m \\end{pmatrix} + \t\t\\begin{pmatrix} n-1\\\\ m-1 \\end{pmatrix} \t\t&amp; \\text{$0&lt;m&lt;n$} \\end{cases}\\\\ =\\frac{n!}{m!(n-m)!}\\]     이러한 Pascal’s Triangle을 알면 평균적으로 어느 정도 높이를 올라가야 root parent를 만날 수 있는지를 분석할 수 있다.   \\[\\sum_{k=0}^h{_nC_k} = \\sum_{k=0}^h{\\binom{h}{k}} = \\sum_{k=0}^h{\\frac{h!}{k!(h-k)!}} = 2^h = n\\\\ \\sum_{k=0}^h{k\\binom{h}{k}} = h2^{h-1}\\\\ ∴ \\text{average depth} = \\frac{h·2^{h-1}}{2^h}=\\frac{h}{2}=\\frac{\\lg(n)}{2}\\]  따라서 worst case일 때, height와 average depth의 complexity는 $O(\\lg(n))$이다. ⇒ Disjoint Set이 효율적인 자료구조임을 확인할 수 있다.       서로소 집합과 크루스칼 알고리즘   크루스칼 알고리즘에서의 연결요소 관리를 서로소 집합으로 변경했을 때 복잡도를 개선할 수 있는 원리에 대해 이해하고 코딩으로 구현해본다.   Kruskal’s Algorithm using Disjoint Sets   Kruskal’s Algorithm은 minimum spanning tree 문제에서 사용할 수 있는 알고리즘 중 하나이다(이전 시간에 배웠다.). 이전에 배운 Kruskal’s Algorithm에서는 cycle을 체크하는 방식으로 DFS나 BFS와 같은 traversal algorithm을 사용했다. 이때 Disjoint Set의 자료구조를 Kruskal’s algorithm에 도입하면, algorithm의 complexity를 줄일 수 있다.   ■ Example          Analysis   ■ Worst Case   아래 알고리즘에 대해 전체 edges 개수 $E$만큼 반복      두 vertex가 같은 집합에 포함되었는지를 확인 (find-set) : $O(\\lg(V))$   서로 다른 집합에 속하는 경우            두 집합을 합집합화 (union-set): $O(\\lg(V))$           따라서 Complexity는 다음과 같다.   \\[E·(\\lg(V) + \\lg(V)) = 2E\\lg(V)\\\\ \\text{Complexity} = O(E\\lg(V))\\]  한편, Kruskal’s Algorithm을 적용하기에 앞서 모든 edges의 weights를 정렬하는 데에 필요한 비용은 $O(E\\lg(E))=O(E\\lg(V))$이다. (∵$E=V^2$)   따라서 최종 complexity는 다음과 같다.   \\[\\text{overall complexity} = O(E\\lg(V) + E\\lg(V)) = O(E\\lg(V))\\]  ※ DFS(or BFS)를 이용한 Kruskal’s algorithm의 complexity $O(EV)$에 비해 효율적임을 알 수 있다. ※           References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91           ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo12/",
        "teaser": null
      },{
        "title": "[ALGO-13] Lect13. 다익스트라 알고리즘 및 복잡도",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/            이번 시간에는 다익스트라 알고리즘 및 복잡도에 대해 알아본다.       그래프 최단경로 알고리즘   그래프 내 최단경로 문제를 정의하고, 활용 분야 및 다익스트라 알고리즘의 핵심 아이디어를 이해한다.       Shortest Path   최단경로 문제를 해결하는 방법에는 대표적으로 Dijkstra's Algorithm이 있다.    Dijkstra’s Algorithm   Prim’s Algorithm과 유사하지만 모든 edges의 weights값이 양수여야만 한다는 제한조건을 갖는다.      이처럼 Dijkstra’s Algorithm은 최단경로를 하나씩 확정해 나가는 방법이다.       다익스트라 알고리즘과 복잡도   다익스트라 알고리즘의 구동 원리를 이해하고, 그래프 표현 자료구조 및 거리 저장 자료구조에 따른 복잡도 차이를 분석한다.       Dijkstra’s Algorithm   구성요소      Prim’s Algorithm과 마찬가지로 초기에는 initial vertex에 대한 정보만을 가지고 있다.        3개의 array(`distance`, `visit`, `parent`)를 갖는다.           동작방식      초기화   아래의 동작을 모든 vertex를 방문할 때 까지 $V$번 반복한다.            unvisited vertex 중 initial vertex로부터 가장 거리가 짧은 vertex를 방문(visit)한다.       방문한 vertex의 neighbors의 distance와 parent 정보를 업데이트한다.           ※ shortest path의 거리가 무한대(∞)라면 해당 graph는 unconnected-graph이다. ※       Example   ■ 예제1   initial vertex=K일 때, K로부터 다른 모든 vertex까지의 최단경로를 구해본다.      ※ unvisited vertex의 distance = ∞라면, unconnected-graph이다. ※       ■ 예제2: 최단거리뿐만 아니라 최단 경로도 고려하는 문제      previous flag를 추가하여 역순으로 추적할 수 있다.       Analysis      Initialization: $Θ(V)$ (∵vertex의 개수 $V$만큼 array를 순회)   아래의 동작을 $V-1$번 반복한다:            unvisited vertex 중 initial vertex로부터 가장 거리가 짧은 vertex를 방문(visit)한다: $Θ(V)$  (∵ 현재까지 조회된 vertex들의 distance를 차례로 탐색)       방문한 vertex의 neighbors의 distance와 parent 정보를 업데이트한다:                    adjacency matrix를 사용하는 경우: $Θ(V)$           adjacency list를 사용하는 경우: $Θ(E)$ (이때 $E&lt;V^2$)                           따라서 최종적으로 아래와 같은 복잡도를 갖는다.   \\[V+(V-1)·(V+V)\\text{ or }V+(V-1)·(V)+E\\\\ ∴ Θ(V^2)\\]      Dijkstra’s algorithm을 적용할 때, 단순히 distance table을 앞에서부터 차례로 순회하는 것이 아니라 priority queue(min binary heap)과 같은 자료구조를 활용하면 보다 최적화할 수 있다.           Initialization: $Θ(V)$ (∵vertex의 개수 $V$만큼 array를 순회)            아래의 동작을 $V$번 반복한다:                       unvisited vertex 중 initial vertex로부터 가장 거리가 짧은 vertex를 방문(visit)한다: $Θ(1)$  (∵ root로 바로 접근 가능)                        방문한 vertex의 neighbors의 distance와 parent 정보를 업데이트한다: $Θ(\\lg(V))$           ※ 이때, 업데이트 발생시, heap 내부 구조에서 또 다시 업데이트가 발생하여 다음의 추가 비용이 든다: $Θ(E\\lg(V))$                  따라서 최종적으로 아래와 같은 복잡도를 갖는다.   \\[V+V·(1+\\lg(V))+E\\lg(V)\\\\ ∴ Θ(V\\lg(V)+E\\lg(V))\\\\ = \\begin{cases} \tΘ(E\\lg(V)) &amp; \\text{if $E&lt;&lt;V^2$}\\\\ \tΘ(V^2\\lg(V)) &amp; \\text{if $E≒V^2$} \\end{cases}\\]  즉, edges의 개수가 극단적으로 많지($Θ(V^2\\lg(V))$) 않다면 priority queue를 사용하는 것($Θ(E\\lg(V))$)이 일반적인 array를 사용하는 것($Θ(V^2))$)보다 더 효율적이다.           References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91           ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo13/",
        "teaser": null
      },{
        "title": "[ALGO-14] Lect14. 의존관계가 정의된 과제 그래프 모델링과 위상정렬 및 임계경로",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/            이번 시간에는 의존관계가 정의된 과제들을 그래프로 모델링해보고 위상정렬 및 임계경로를 도출해본다.       DAG 구조와 활용 분야   의존 관계가 정의 된 과제들의 방향-비순환(Directed Acyclic) 그래프로의 모델링, 위상정렬의 정의, 활용 분야 및 알고리즘에 대해 학습한다.       Directed Acyclic Graph, DAG   DAG는 사이클을 포함하지 않는 유향(방향이 있는) 그래프이다.   ■ 특징           vertex $v_j$→ $v_k$인 경로가 존재한다면, $v_k$→$v_j$인 경로는 존재하지 않는다.       (∵ 존재한다면, acyclic의 규칙이 깨지므로)       Topological Sort   ■ 배경   주어진 작업 셋 간에 dependencies(의존성)이 있는 경우, 의존성을 무너뜨리지 않고 모든 작업을 마치는 방법에 대한 해결책이다. 이때 이러한 작업들은 Directed Acyclic Graph(DAG) 구조로 표현할 수 있다.       ■ 특징   Topological Sorting의 결과는 unique하지 않다.       ■ 응용      외출하기위해 옷을 입는 순서 (속옷→바지→티셔츠→…)   여러 개의 소스코드로 이루어진 프로그램의 컴파일 순서 결정   선행과목을 포함한 수강신청 (CS Basics→C Programming→Algorithms→…)       Algorithm   DAG $V$에 대해 Topological Sort를 수행하는 과정은 다음과 같다.           DAG $V$를 복사하여 DAG $W$에 저장한다.            아래의 과정을 반복한다 :                       DAG $W$ 내의 $\\text{in-degree}=0$인 vertex $v$를 조사한다. (즉, source인 vertex)           ※ in-degree는 vertex를 기준으로 들어오는 화살표를 의미한다. ※                       source에 해당하는 vertex $v$를 topological sort의 다음 순서에 넣는다.                        DAG $W$에서 vertex $v$를 제거한다.                       Example   아래와 같이 12개의 vertex로 구성된 DAG를 Topological Sort한 결과를 구하라.      Result:   \\[C,H,\\ D,I,\\ A,J,\\ B,F,\\  G,K,\\ E,L\\]  ※ vertex 선택 순서에 따라 topological sort의 결과는 달라질 수 있다. ※       Implementation           Initialization       Type array[vertex_size]; int ihead = 0, itail = -1;                Testing if empty:       ihead == itail + 1                For push       itail++; array[itail] = new vertex;                For pop       Type current_top = array[ihead]; ihead++;                topological sorting에 queue를 사용할 때 약간의 trick을 이용하여 topological sorting이 끝나면 queue의 sorting 순서대로 결과가 출력되게끔 만들 수 있다.              Analysis      vertex의 in-degree의 개수를 기록하는 table을 초기화: $Θ(V)$   아래의 과정을 $V$번 반복: $Θ(V)$            $\\text{in-degree}=0$인 vertex를 찾기위해 in-degree table을 스캔 : $O(V)$       발견한 source vertex들을 queue에 모두 push &amp;&amp; pop : $Θ(V)$       source vertex들의 neighbors의 in-degree값에 -1 연산을 수행 :                    adjacency matrix를 사용하는 경우: $Θ(V)$           adjacency list를 사용하는 경우: $Θ(E)$                           최종적으로 (adjacency list를 사용하는 경우) Topological Sorting은 다음의 complexity를 갖는다.      adjacency list를 사용하는 경우, $Θ(\\lvert{V}\\rvert+\\lvert{E}\\rvert)$.   adjacency matrix를 사용하는 경우, $Θ(\\lvert{V}\\rvert^2)$.   memory requirements = $Θ(\\lvert{V}\\rvert)$.           Critiacal Path, 임계 경로   tasks에 대해 dependency와 수행시간이 정해져있을 때&gt; DAG에서 Critical Path(임계 경로)와 Critical Time(임계 시간)을 구하는 알고리즘에 대해 배운다.       ■ 주요 아이디어   아래와 같은 dependency를 갖는 tasks가 주어졌을 때, 모든 작업을 마치는 데에 필요한 최소 시간을 구해보자.              한 번에 하나의 작업만 수행 가능한 경우:       A→B→C→D→E의 순서로(사실 어떤 순서든 상관이 없다) 작업을 처리한다면 총 소요시간은 다음과 같다.   \\[0.3+0.5+0.4+0.1+0.7=2.0(\\text{sec})\\]           B와 D를 동시에 수행할 수 있는 경우(병렬처리):       (B,D)→A→C→E   \\[\\max(0.7,\\ \\min{(0.7, 0.5)}+0.3+0.4+0.1)=1.3(\\text{sec})\\]      ※ 모든 Tasks가 끝나는 가장 빠른 시간을 Critical Time이라고 한다. ※       ※ Ciritical Time에 해당하는 경로를 Critical Path라고 한다. ※           Algorithm         전체 tasks가 끝나기 위해서는 39.4sec의 시간이 소요되고, 각각의 task는 다음의 Ciritical Time을 갖는다.                  A       B       C       D       E       F                       5.2       11.3       31.3       39.4       26.6       17.1           Ciritical Path는 previous Task를 이용하여 D를 기준으로 계산하면 된다.   \\[D←C←E←F←Φ : \\text{Ciritical Path}\\]  이 Critical Path가 tasks를 모두 수행하기 위해 가장 오래 걸리는 tasks에 대한 순서이다. 따라서 전체 공정을 개선하고싶다면 FECD 중 무언가를 개선해야한다.           References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91           ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo14/",
        "teaser": null
      },{
        "title": "[ALGO-15] Lect15. 동적계획법과 분할정복법의 차이 및 예제",
        "excerpt":"     강의 정보              강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고          교수: 성균관대학교 소프트웨어학과 허재필 교수님          사이트: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/            이번 시간에는 동적계획법과 분할정복법의 차이에 대해 이해하고, 예제 문제를 해결해본다.       동적계획법의 방법론 이해   동적계획법이 필요한 문제의 특성 및 분할정복법과의 차이점에 대해 학습한다.       Fibonacci Numbers   다이나믹 프로그래밍에 대해 이야기하기에 앞서 피보나치 수열에 대해 알아본다.   ■ Algorithm   long long F(long long n) { \tif(n&lt;=1) return 1;     return F(n-1)+F(n-2); }   ● 예시   1 → 1 → 2 → 3 → 5 → 8 → 13 → …       ■ Run-time   \\[T(n)= \\begin{cases} \tΘ(1) &amp; \\text{$n≤1$}\\\\ \tT(n-1)+T(n-2)+Θ(1) &amp; \\text{$n&gt;1$} \\end{cases}\\]  이러한 run time은 피보나치 수열만큼 증가함을 알 수 있다. 그 이유는 피보나치 함수 $F(50)$을 계산하기 위해 각 n에 대해 다음과 같이 반복적으로 호출되기 때문이다.   \\[F(45): \\text{8번 호출}\\\\ F(40): \\text{89번 호출}\\\\ F(30): \\text{10,946번 호출}\\\\ F(20): \\text{1,346,269번 호출}\\\\ F(10): \\text{165,580,141번 호출}\\\\ F(1): \\text{12,586,269,025번 호출}\\]  이러한 문제를 해결하기위한 방법으로 이전에 이미 구했던 결과를 저장하는 방법을 생각해볼 수 있다. 이러한 과정을 MemorizationM이라고 부른다.       Fibonacci with Memorization   Memorization을 사용하는 피보나치 함수는 다음과 같이 프로그래밍될 수 있다.   long long F(long long n, bool isFirstCall = false) {     static long long *memo;     if(isFirstCall)     {         if(memo!=NULL){delete[] memo;}         if(n!=0)         {             memo = new long long[n+1];             for(int i=0; i&lt;n+1; i++){ memo[i]=0; }         }     }     if(n&lt;=1) return 1; \tif(memo[n]==0){ memo[n] = F(n-1) + F(n-2); }     return memo[n]; }   ※ global variable을 사용하는 것 보단 static을 사용하는 코딩 스타일이 더 좋다. ※      ● 알고리즘 해석           memo static variable을 생성한다.            처음 호출되는 경우 (메모리 할당이 필요):                       memo에 값이 있는 경우: memo static 변수를 모두 삭제한다.                        n!=0인 경우(계산이 필요):           memo 변수의 크기를 +1 늘려준 뒤, memo array의 모든 값을 0으로 초기화한다.                        n&lt;=1인 경우: (계산 할 것도 없이) 1을 반환한다.            memo[n]==0인 경우(초기화된 직후): memo[n]의 위치에 F(n-1)+F(n-2)의 결과를 저장한다.            최종적으로 memo[n]을 반환한다.           Top-down and Bottom-up Algorithms   Top-down approach: F(50)→F(49)→….   ※ 대부분의 Divide-and-conquer algorithm이 Top-down 방식으로 수행된다. ※       Bottom-up approach: F(1)→F(2)→…   피보나치 수열 문제는 Top-down이 아닌 Bottom-up 방식으로도 해결할 수 있다.   e.g., Merge Sort: 미리 적당한 사이즈로 쪼개놓고, 위로 가는 방향       ● Fibonacci wiht Bottom-up approach   long long F(long long n) {     if(n&lt;=1){ return 1; }     long long ret=1, prev=1;     for(long long i=2; i&lt;=n; i++)     {         ret = ret+prev;         prev = ret-prev;     }     return ret; }   ※ 코드가 상당히 간단하다. ※       Dynamic Programming   반복적으로 활용되는 sub-problem을 저장하여 재활용하여 문제를 해결하는 방법이 다이나믹 프로그래밍이다. 이때 주로 Memorization 방식으로 속도를 향상시킨다.     ■ Divide-and-conquer와의 차이점   Divide-and-conquer: sub-problem간에 overlap이 거의 없다.   e.g., Merge Sort: 양쪽으로 나눈 array간에 공통적인 부분이 없다. 즉, 재활용할 여지가 없다.   Dynamic Programming: sub-problem에 overlap이 있다.   e.g., 피보나치 넘버: F(10)과같이 중복 문제가 계속 발생한다.   ※ 이러한 중복 문제를 overlapping sub-problem이라고 한다. ※  ※ overlapping sub-problem의 값을 미리 저장하고 재활용하는 것이 dynamic programming의 핵심이다. ※          행렬 체인 곱셈 문제의 동적계획법 기반 해결   Dynamic Programming으로 matrix multiplication하는 문제를 해결한다.   행렬 체인 곱셈 문제의 정의와 AI분야에서의 중요성, 그리고 동적계획법 기반의 알고리즘을 학습한다.   (추후 보충 예정)           References      [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91           ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","ALGO"],
        "url": "/computerscience-algorithm/algo15/",
        "teaser": null
      },{
        "title": "[Algorithms-Sanjoy Dasgupta-01] Ch2. Divide-and-conquer algorithms",
        "excerpt":"     교재 정보              교재명: Algorithms          Author: Sanjoy Dasgupta          사이트: https://github.com/eherbold/berkeleytextbooks              Ch2. Divide-and-conquer algorithms   ■ Divide-and-conquer:      problem을 sub-problems으로 나눈다.   재귀적으로 sub-problems을 해결한다.   answers를 모두 combining한다.       2.1. Multiplication   Divide-and-conquer 접근 방법이 어떠한 이점을 제공하는지 몇 가지 예시를 통해 살펴본다.   ■ 예시  📢이어서 더 자세히 다루도록 한다.      Multiplication   Search            Binary Search           Sort            Merge Sort               The product of two complex numbers (a.k.a. Gauss’s Trick)   \\[(a+bi)(c+di) = ac -bd +(bc+ad)i\\]  위 방정식은 4개의 곱($ac$, $bd$, $bc$, $ad$)으로 구성되어있지만, divide-and-conquer 방식으로 접근했을 때 아래와 같이 3개의 곱($ac$, $bd$, $(a+b)(c+d)$)으로 곱셈연산 횟수를 줄일 수 있다.   \\[(a+bi)(c+di) = ac -bd +\\{ (a+b)(c+d) -ac -bd\\}i\\\\ \\text{(∵ (bc+ad) = (a+b)(c+d) -ac -bd )}\\]  big-O 관점에서 위와 같은 곱셈 연산 횟수 감소는 독창성 낭비라고 보여지지만, 재귀적인 관점에서 불필요한 곱셈의 수를 줄이는 것은 매우 중요하다.       Regular multiplication (정규곱셈)   $x$와 $y$가 두 개의 $n$-bit 정수이며 이때 $n$은 2의 제곱수라고 가정하자. divide-and-conquer하게 $x$와 $y$의 곱을 표현하고자 각각을 sub-problem으로 쪼갠 결과는 아래와 같다.   \\[x = 2^{n/2}·x_L + x_R\\\\ y = 2^{n/2}·y_L + y_R\\]  이를 이용하여 $x$와 $y$의 곱 $xy$를 표현하면 다음과 같다.   \\[xy = (2^{n/2}·x_L+x_R)(2^{n/2}·y_L+y_R)\\\\ = 2^{n}·x_Ly_L + 2^{n/2}(x_Ly_R +x_Ry_L) + x_Ry_R\\]  여기서 $x_Ly_L$, $x_Ly_R$, $x_Ry_L$, $x_Ry_R$은 재귀적으로 다시 호출될 수 있다. (아래 수도 알고리즘 참고)   A divide-and-conquer algorithm for integer multiplication.  function multiply(x; y)     Input: Positive integers x and y, in binary     Output: Their product          n = max(size of x, size of y)     if n = 1: return xy          xL, xR = leftmost [n/2], rightmost [n/2] bits of x     yL, yR = leftmost [n/2], rightmost [n/2] bits of y     P1 = multiply(xL; yL)     P2 = multiply(xR; yR)     P3 = multiply(xL + xR; yL + yR)  \treturn P1×2n + (P3-P1-P2)×2^{n/2} + P2   따라서 $n$-bit inputs에 대한 전반적인 running time $T(n)$은 다음과 같다.   \\[T(n) = 4T(n/2) + O(n)\\\\ ∴ \\text{complexity} = O(n^2)\\]  running time = $O(n^2)$은 전통적인 접근방법($O(n^2)$)과 비교했을 때 나아진 점이 없다. 따라서 앞서 배운 Gauss’s trick(두 개의 복소수 곱)을 $xy$에 적용하면, 세 개의 곱($x_Ly_L$, $x_Ry_R$, $(x_L+x_R)(y_L+y_R)$)으로 표현될 수 있다.        [참고] Gauss's trick      (a+bi)(c+di)는 세 개의 곱(ac, bd, (a+b)(c+d))으로 표현된다.       따라서 다음과 같이 running time을 줄일 수 있다.   \\[T(n) = 3T(n/2) + O(n)\\\\ \\text{∴ complexity = $O(n^{1.59})$}\\]          2.2. Recurrence relations   Divide-and-conquer 알고리즘은 일반적인 패턴을 갖는다. 이를 공식화해보자.   복잡도 공식화   가정      $\\text{size}=n$인 problem을   $\\text{size}=n/b$인 $a$개의 subproblems으로 재귀적으로 해결한다.   이때 추가적으로 발생하는 $\\text{time}=O(n^d)$   \\[T(n) = a·T(n/b)+O(n^d)\\]  running time이 위와 같을 때 아래와 같이 overall running time을 계산할 수 있다.   \\[T(n)  = a·T(\\frac{n}{b})+O(n^d)\\\\ = a^2·T(\\frac{n}{b^2})+\\{ a·O({(\\frac{n}{b})}^d) + O(n^d) \\}\\\\ = a^3·T(\\frac{n}{b^3})+\\{ a^2·O({(\\frac{n}{b^2})}^d) + a·O({(\\frac{n}{b})}^d) + O(n^d) \\}\\\\ ...\\\\ = a^k·T(\\frac{n}{b^k})+{\\sum_{i=0}^{k-1}\\{a^i·O((\\frac{n}{b^i})^d)\\}}\\]  이때, $b^k=n$ 즉, $k=\\log_b(n)$일 때 위 방정식을 $T(1)$에 관해 표현할 수 있다.   \\[T(n) = a^{\\log_b(n)}·T(\\frac{n}{b^{\\log_b(n)}}) \t+{\\sum_{i=0}^{\\log_b(n)-1}\\{a^i·O((\\frac{n}{b^i})^d)\\}} \\\\ = a^{\\log_b(n)}·T(1) \t+{(\\sum_{i=0}^{\\log_b(n)-1}(\\frac{a}{b^d})^i)·O(n^d)} \\\\ = a^{\\log_b(n)}·T(1) \t+{(\\frac{ 1·( \\frac{a}{b^d}^{\\log_b(n)}-1 ) } { \\frac{a}{b^d}-1 })·O(n^d)} \\\\ = n^{\\log_b(a)}·T(1) \t+{(\\frac{ 1·( n^{(\\log_b(a)-d)}-1 ) } { \\frac{a}{b^d}-1 })·O(n^d)} \\text{(∵ 로그의 성질)}\\]  이때, 우항의 두 번째 term은 다음과 같이 쓸 수 있다.   \\[{(\\frac{ 1·( n^{(\\log_b(a)-d)}-1 ) } { \\frac{a}{b^d}-1 })·O(n^d)}\\\\ =  \\begin{cases} \tO({ n^{(\\log_b(a)-d)} }) · O(n^d) &amp; \\text{if $\\log_b(a) &gt; d$} \\\\ \t\\log_b(n) · O(n^d) &amp; \\text{if $\\log_b(a) = d$} \\\\ \tO(c) · O(n^d) &amp; \\text{if $\\log_b(a) &lt; d$} \\\\ \\end{cases}\\\\ =  \\begin{cases} \tO( n^{(\\log_b(a))} ) &amp; \\text{if $\\log_b(a) &gt; d$} \\\\ \tO(n^d\\log(n)) &amp; \\text{if $\\log_b(a) = d$} \\\\ \tO(n^d) &amp; \\text{if $\\log_b(a) &lt; d$} \\\\ \\end{cases}\\\\\\]  따라서 최종적인 overall running time은 다음과 같다.   \\[T(n) =  \\begin{cases} \tO( n^{\\log_b(a)} + n^{\\log_b(a)} ) &amp; \\text{if $\\log_b(a) &gt; d$} \\\\ \tO( n^{\\log_b(a)} + n^d\\log(n)) &amp; \\text{if $\\log_b(a) = d$} \\\\ \tO( n^{\\log_b(a)} + n^d) &amp; \\text{if $\\log_b(a) &lt; d$} \\\\ \\end{cases}\\\\ =  \\begin{cases} \tO( n^{\\log_b(a)} ) &amp; \\text{if $\\log_b(a) &gt; d$} \\\\ \tO( n^d\\log(n)) &amp; \\text{if $\\log_b(a) = d$} \\\\ \tO( n^d) &amp; \\text{if $\\log_b(a) &lt; d$} \\\\ \\end{cases}\\\\\\]      재귀적 호출을 tree구조로 해석하면 다음과 같이도 이해할 수 있다.      overall running time $T(n)$을 depth=$k$일 때의 time $T(\\frac{n}{b^k})$로 나타내면 다음과 같이 쓸 수 있다. (해석하자면, $T(\\frac{n}{b^k})$짜리 nodes가 $a^k$만큼 있고, 여기에 초기 $O(n^d)$ complexity를 더한다.)   \\[T(n) = a^k × T(\\frac{n}{b^k}) + O(n^d)\\]  (예시를 통해 재검토)           $T(n) = 4T(n/2) + O(n)$일 때, (즉 $a=4, b=2, d=1$)   \\[T(n) = O( n^{\\log_2(4)} ) = O(n^2) \\ \\ \\text{(∵ $\\log_2(4)&gt;1$이므로 )}\\]           $T(n) = 3T(n/2) + O(n)$일 때, (즉 $a=3, b=2, d=1$)   \\[T(n) = O( n^{\\log_2(3)} ) = O(n^{1.59}) \\ \\ \\text{(∵ $\\log_2(3)&gt;1$이므로 )}\\]          Binary Search   divide-and-conquer 알고리즘은 궁극적으로 binary search라고 해석할 수 있다.   ■ 예시: 정렬된 keys $z[0, 1, …, n-1]$를 포함하는 하나의 큰 파일에서 key값 $k$를 찾는 문제   Binary Search를 통해 이러한 문제를 해결하는 과정은 다음과 같다.   ● 수도 알고리즘   Binary_Search(z, 0, n-1, k): \tif(k==z[n/2]) { result=z[n/2]; } \telse if(k&lt;z[n/2]) { Binary_Search(z, 0, n/2-1, k); } \telse if(k&gt;z[n/2]) { Binary_Search(z, n/2, n-1, k); }       ● 복잡도   overall running time $T(n)$은 다음과 같다.   \\[T(n)  = T([n/2]) + O(1) &amp; \\text{즉, $a=1, b=2, d=0$}\\\\ = O(n^d\\log(n)) = O(\\log(n)) &amp; \\text{(∵ $\\log_2(1) = 0$)}\\]       내 생각      결국, Binary Search는 주어진 elements를 2개로 나누며, 각 search에 O(1)의 시간이 걸리는것이다.      이를 divide-and-conquer로 확장시켜 생각한다면 주어진 elements를 몇 개로 쪼갤지(a), 이렇게 쪼개면 각 단위의 elements의 size는 어떻게 되는지(b), 또한 각 단위를 처리하는데에 얼만큼의 시간이 소요되는지(d)와 관련시킬 수 있을 것 같다.           2.3. Mergesort   Divide-and-conquer 알고리즘은 숫자들의 list를 정렬하는 문제에 쉽게 적용 가능하다. 가령 merge sort는 list를 절반씩 두 개로 쪼개고, 재귀적으로 각 절반씩에 대해 정렬을 하며, 그 결과로 반환된 sorted sublists를 merge(병합)한다.       알고리즘   mergesort의 알고리즘은 다음과 같다.   function mergesort(a[1...n]): \t* Input:\tAn array of numbers a[1...n] \t* Output:\tA sorted version of this array  \tif n&gt;1: \t\treturn merge( mergesort(a[1..⌊n/2⌋)), mergesort(a[⌊n/2⌋+1...n])) \telse: \t\treturn a      input list의 element 개수가 2 이상이면, 절반씩 mergesort()한 뒤에 merge() 결과를 반환   input list의 element 개수가 1 이하이면, input list를 그대로 반환   이때 merge 함수는 다음과 같다.   function merge(x[1...k], y[1...l]): \tif k=0:\treturn y[1...l] \tif l=0:\treturn x[1...k] \tif x[1]≤y[1]: \t\treturn x[1] &amp; merge( x[2...k], y[1...l] ) \telse: \t\treturn y[1] &amp; merge( x[1...k], y[2...l] )      두 input list 중 비어있는 list가 있다면, 비어있지 않은 다른 input list를 반환   두 input list가 모두 비어있지 않다면,            두 input list 각각의 최소값(첫 번째 인덱스값)을 비교하여 적절히 concatenate and merge           ※ 이때 operator &amp;는 concatenation을 의미한다. ※       복잡도   mergesort의 overall running time $T(n)$은, mergesort의 재귀호출 시간 $2T(n/2)$와 merge의 시간복잡도 $O(n)$의 합으로 나타낼 수 있다.   ※ 길이가 각각 $k$, $l$인 두 lists를 합치는 merge 알고리즘의 running time은 $O(k+l)=O(n)$. ※   \\[T(n) = 2T(n/2) + O(n) \\\\ = O(n^d\\log(n)) \\ \\ \\  \\text{(∵$\\log_2(2)==1$)} \\\\ = O(n\\log(n)) \\ \\ \\  \\text{(∵ $a=2, b=2, c=1$)}\\]  즉, $T(n) = O(n\\log(n))$.       iteractive mergesort   위에서 언급한 알고리즘을 그림으로 도식화한 결과는 아래와 같다.      사실상 모든 과정이 merging만으로 수행됨을 확인할 수 있다. 이러한 관점에서 봤을 때 mergesort는 재귀적인 방법(recursive)이 아닌 반복적인 방법(iteractive)을 이용해볼 수 있다.   function iterative-mergesort( a[1...n] ) \t* Input:\telements a1, a2, ..., an to be sorted \t \tQ = [] (empty queue) \tfor i=1 to n: \t\tinject( Q, [ai] ) \t\twhile │Q│&gt;1: \t\t\tinject( Q, merge( eject(Q), eject(Q) ) ) \treturn eject(Q)           비어있는 queue Q를 생성            주어진 input list a[1…n]의 element a1부터 an까지에 대해 아래를 수행:       Q의 elements 개수가 2개 이상이면 아래를 반복:       2.1. Q를 두 번 eject하여 두 elements를 꺼내고 merge를 수행       2.2. merge의 결과를 Q에 inject ※ 이때 두 elements가 merge되어 하나의 element가 된다. ※            Q의 element를 반환       ※ inject: queue의 마지막에 새로운 원소 추가 (enqueue) ※ ※ eject: queue의 시작 원소를 제거하고 반환 (dequeue) ※ ※ queue의 구조: FIFO(First-In-First-Out) ※        재귀(recursive)와 반복(iteractive)      재귀는 적은 코드로 알고리즘을 작성할 수 있기 때문에 코드를 이해하고 유지하는 것이 중요한 경우에 주로 사용된다. function call에 따른 메모리 비용 및 CPU 사용 시간이 더 소요된다는 단점이 있다.      반복은 function call에 따른 메모리 비용이 없기 때문에 (즉, method 호출을 위한 메모리는 한 번만 필요하기 때문에) 성능 측면에서 더 유리하다.       위 알고리즘을 도식화 하면 다음과 같다.          An $n\\log(n)$ lower bound for sorting   Sorting algorithms은 trees로 묘사될 수 있다. 가령 3개의 elements $a_1, a_2, a_3$를 갖는 하나의 array의 정렬 과정은 다음과 같다.      이때 tree의 depth는 알고리즘의 the worst-case time complexity를 의미한다. 따라서 tree구조로 나타내면 알고리즘이 얼마나 최적화되었는지를 보여줄 수 있다.       ■ $n$-elements sorting algorithms   가령, $n$개의 elements에 대해 정렬알고리즘을 사용할 때 어떠한 비교 sequence에 의해 정렬된 결과 (e.g., ${a_1, a_2, …, a_n}$)는 tree의 leaves에 저장된다. $\\text{depth}=d$일 때 nodes의 개수는 $2^d$이므로, leaves의 총 개수는 $2^h$(h=height)이다. 한편, 가능한 모든 정렬의 개수는 $n!$이므로 다음과 같이 height를 표현할 수 있다.   \\[h  = \\log(n!) \\ \\ \\  \\text{(∵ $2^h=n!$)} \\\\ ≥ c·n\\log(n) \\ \\ \\  \\text{for all c≥0 (∵ $n!≥(\\frac{n}{2})^{\\frac{n}{2}})$}\\\\ ∴ h=Ω(n\\log(n))\\]  위 결과에 의해 $n$개의 elements를 정렬하는 알고리즘의 worst-case는 $Ω(n\\log(n))$에 비교되어야 함을 알 수 있다.       ● 예시   mergesort의 overall time complexity는 $O(n\\log(n))$으로, 위에서 구한 worst-case time complexity $Ω(n\\log(n))$과 일치한다. 즉, 적어도 정렬알고리즘의 worst-case time complexity는 $Ω(n\\log(n))$보다 빠를 수 없는데, mergesort가 이를 충족한다.           2.4. Medians   Get Medians with sorting   중앙값을 찾는 가장 간단한 방법은 단순히 sorting하여 중앙값을 반환하는 것이다. 하지만 이러한 방식은 단점이 명확하다.      too slow : $O(n\\log(n))$의 time complexity가 소요된다.   too much : 중앙값을 찾기 위해 중앙값이 아닌 모든 요소를 정렬해야한다.       Get Medians with Selection   selection을 통해 sorting보다 더 효율적으로 median값을 찾을 수 있다.       ■ Selection   SELECTION \tInput:\tA list of numbers S; an integer k \tOutput: The kth smallest element of S   ※ 이때 $k=1$은 $S$에서 최소값을 의미하며, $k=⌊│S│/2⌋$는 $S$에서 median을 의미한다. ※       ■ A randomized divide-and-conquer algorithm for selection   selection을 위해 divide-and-conquer 접근 방법을 사용해볼 수 있다.   ● 예시: 어떤 값 $v$에 대해 list $S$를 세 개의 분류로 쪼개는 방법      $v=5$일때 주어진 list $S$는 다음과 같이 쪼개질 수 있다.      ※ $S_L, S_v, S_R$ : 각각 $S$보다 작은값들, 같은 값들, 큰 값들의 집합 ※   이때 median값을 search하는 과정은 이러한 sublists 중 하나로 범위를 좁혀내려가며 얻어질 수 있다. 가령, $S$ 중 8번째로 작은 값은 $S_R$에서 세 번째로 작은 값임을 알 수 있다 ($S_L$과 $S_v$의 elements 개수가 5개이므로 8번째로 작은 값은 $S_R$에 속할 수 밖에 없다).   이를 일반화하면 다음과 같다.   \\[\\text{selection}(S,k) = \\begin{cases} \t\\text{selection}(S_L, k) &amp; \\text{if $k≤ │S_L│$} \\\\ \tv &amp; \\text{if $│S_L│&lt;k≤│S_L│+│S_v│$} \\\\ \t\\text{selection}(S_R, k-│S_L│-│S_v│) &amp; \\text{if $k&gt;│S_L│+│S_v│$} \\end{cases}\\]      ■ 복잡도   이때 세 개의 sublists는 linear time안에 계산된다 (더욱이, 계산은 in-place로 동작하며 추가적인 memory allocating이 필요없다👍). 따라서 적절하게 sublist를 선택한다면 계산해야하는 elements의 개수를 $│S│$에서 적어도 $\\max{(│S_L│, │S_R│)}$만큼은 감소시킬 수 있다.   이상적인 상황($│S_L│, │S_R│ ≒ \\frac{1}{2}│S│$)을 가정하면, selection의 overall running time은 다음과 같다.   \\[T(n) = T(n/2)+O(n)\\\\ ∴ T(n) = O(n)\\]  ※ 단, 위 복잡도는 이상적인 상황을 가정하였음에 유의해야한다. ※       ● $v$에 따른 복잡도   알고리즘의 복잡도는 결국 $v$값에 따라 달라진다.           Best-case: $│S_L│, │S_R│ ≒ \\frac{1}{2}│S│$   \\[T(n) = T(n/2) + O(n)\\\\ ∴ T(n) = O(n)\\]           Worst-case: $v$가 매번 최대값 혹은 최소값인 경우   \\[T(n) = \\sum_{i=\\frac{n}{2}}^{n}i = O(n^2)\\]           Average-case:       $S(25\\%) ≤ v ≤ S(75\\%)$라면( $S$의 50% 부분에 해당), $│S_L│, │S_R│ ≤ \\frac{3}{4}│S│$가 된다.       이때 Lemma에 의해  2번 split operation을 수행하면 평균적으로 전체 risk가 최대 $\\frac{3}{4}$ 크기로 감소한다.            Lemma란?      On average a fair coin needs to be tossed two times before a \"heads\" is seen.  \t즉, 동전이 앞면이기 위해서는 평균적으로 2번 던져야 한다.           따라서 Average-case의 time complexity는 다음과 같다.   \\[T(n) ≤ T(\\frac{3}{4}n) + O(2n)\\\\   ∴T(n) = O(n)\\]      The Unix sort command   ■ Mergesort vs. Median-finding : 서로 다른 성질      mergesort : 일단 생각없이 쪼개고, 이후에 열심히 정렬하면서 합침 (the most convenient way)   median-finding : 처음부터 신중하게 쪼개어 나감       ■ Quicksort ≈ Median-finding :   Quicksort는 median-finding 알고리즘의 방식과 유사하게 동작한다. median algorithm의 $v$ 대신 $\\text{pivot}$이라는 변수를 도입하여, $\\text{pivot}$을 기준으로 list를 쪼갠다. 이후 sublists 각각에 대해 정렬을 반복하는 과정을 거친다.   ● Time Complexity   Quicksort의 time complexity는 다음과 같다.      Worst-case : $Θ(n^2)$ (median-finding과 동일)   Average-case : $O(n\\log(n))$ (≠median-finding $O(n^2)$)   Quicksort는 sorting algorithms 중에서 가장 빠른 알고리즘이다.           2.5. Matrix Multiplication   두 개의 $n×n$ matrices $X$와 $Y$의 곱은 $n×n$ matrix $Z=XY$로 표현된다.      \\[Z_{ij} = \\sum_{k=1}^n{X_{ik}Y_{kj}}\\]  ※ 일반적으로 $XY≠YX$이다. 즉, matrix multiplication은 commutative하지 않다. ※       ■ 배경   꽤 오랜시간동안 matrix multiplication은 $O(n^3)$의 복잡도를 갖는다고 여겨졌다 (∵ $n^2$개의 entries가 계산되고 각 계산은 $O(n)$이 소요되기 때문). 하지만 독일 수학자 Volker Strassen에 의해 divide-and-conquer 알고리즘을 적용하면 더 효율적인 알고리즘으로 만들 수 있다는 것이 증명되었다.       Volker Strassen’s Idea   ■ 이전까지의 생각   $X$와 $Y$를 각각 4개의 $\\frac{n}{2}×\\frac{n}{2}$ blocks으로 쪼개면 다음과 같이 표현 가능하다.   \\[X= \\begin{bmatrix} \tA &amp; B\\\\ \tC &amp; D \\end{bmatrix} , Y= \\begin{bmatrix} \tE &amp; F\\\\ \tG &amp; H \\end{bmatrix}\\\\  ∴ XY =  \\begin{bmatrix} \tAE+BG &amp; AF+BH\\\\ \tCE+DG &amp; CF+DH \\end{bmatrix}\\]  이때의 시간 복잡도는 다음과 같다.   \\[T(n) = 8T(\\frac{n}{2}) + O(n^2)\\\\ ∴ T(n) = O(n^3)\\\\  \\text{(∵ $\\frac{n}{2}$의 크기를 갖는 8개의 submatrix 곱과 $O(n^2)$ 복잡도를 갖는 덧셈연산)}\\]      ■ Volker Strassen의 생각   Volker는 $n$ 크기 matrix를 7개의 subproblems으로 쪼갬으로써 복잡도를 줄였다.   \\[T(n) = 7T(\\frac{n}{2}) + O(n^2)\\\\ ∴ T(n) = O(n^{\\log_2{7}}) ≈ O(n^{2.81})\\]  ▶ 자세히: 7개의 subproblems으로 쪼개는 방법   \\[XY =  \\begin{bmatrix} \tP_5 + P_4 - P_2 + P_6 &amp; P_1 + P_2\\\\ \tP_3 + P_4 &amp; P_1 + P_5 - P_3 - P_7 \\end{bmatrix}\\]  이때 $P_1 = A(F-H), P_2 = (A+B)H, P_3 = (C+D)E, P_4 = D(G-E)$, $P_5 = (A+D)(E+H), P_6 = (B-D)(G+H), P_7 = (A-C)(E+F)$ .           2.6. The fast Fourier transform   지금까지 integers와 matrices에 대해 divide-and-conquer 알고리즘이 얼마나 유익한지를 살펴보았다. 이번에는 polynomials에 대해 divide-and-conquer 알고리즘을 적용해보자.   Polynomials with Divide-and-conquer   두 degree-$d$ polynomials $A(x), B(x)$의 곱은 degree-$2d$ polynomial $C(x)$로 표현된다.   \\[A(x) = a_0 + a_1x + ... + a_dx^d\\\\ B(x) = b_0 + b_1x + ... + b_dx^d\\\\ C(x)=A(x)·B(x) = c_0 + c_1x + ... + C_{2d}x^{2d}\\\\ \\text{이때 $c_k = a_0b_k + a_1b_{k-1} + ... + a_kb_0 = \\sum_{i=0}^k{a_ib_{k-i}}$}\\]  ※ for $i&gt;d$, take $a_i$ and $b_i$ to be zero. ※   이때, $c_k$를 계산하기 위해서는 $O(k)$단계가 소요되므로 $2d+1$개의 coefficients($c_0, c_1, … c_{2d}$)를 계산하기 위해서는 $Θ(d^2)$의 시간이 소요된다.       Fast Fourier Transform   Fast fourier transform을 이용하면 더 빠른 계산을 수행할 수 있다. 자세한 과정은 생략한다.           Exercise   (추후 추가 예정)           References      eherbold/berkeleytextbooks, github: https://github.com/eherbold/berkeleytextbooks           ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","Algorithms","Sanjoy Dasgupta"],
        "url": "/computerscience-algorithm/algorithmsSanjoyDasgupta/",
        "teaser": null
      },{
        "title": "[Algorithms-Sanjoy Dasgupta-02] Ch3. Decompositions of graphs",
        "excerpt":"     교재 정보              교재명: Algorithms          Author: Sanjoy Dasgupta          사이트: https://github.com/eherbold/berkeleytextbooks              Ch3. Decompositions of graphs       3.1. Why graphs?   Graph(그래프)를 통해 다양한 문제를 명확하고 정밀하게 표현할 수 있다. 이번 장에서는 그래프의 기본 연결 구조를 설명하는 알고리즘 중 가장 기본적인 알고리즘에 대해 알아본다.      Graph에 관한 기본적인 개념은 이전에 포스팅했던 “kmooc algorithm 시리즈”를 참고하길 바란다.     관련링크:        ● 예시 : Graph 자료구조의 사용 예시 - 지도의 구역 색칠하기 문제      가령 Figure 3.1.(a)와 같이 지도의 구역을 색칠하는 문제는 Figure 3.1.(b)와 같이 graph형태로 표현될 수 있다. 이렇게 표현하면, 각 구역이 어떤 구역과 맞닿아있는지를 쉽게 파악할 수 있어서 색상이 겹치지 않게 색칠하는 것이 가능해진다.       개념   \\[G=(V,E)\\]  ※ $G$: graph,  $V$: vertices,  $E$: edges ※       ■ 분류   Graph는 크게 두 가지로 분류 가능하다.           무향 그래프 (Undirected graph)       ※ vertex $x$와 $y$가 symmetric(대칭) 관계로 edge $e$에 의해 연결될 때, $e={x,y}$로 표현됨 ※       e.g., 위 “지도 색칠 문제”가 이에 해당            유향 그래프 (Directed graph)       ※ vertex $x$에서 $y$로 가는 directed edges $e$는 $e=(x,y)$로 표현됨 ※       e.g., World Wide Web: Internet에서 각 site 정보를 vertex에 담고 있다.           Representations   $n(=│V│)$개의 vertices $v_1, …, v_n$를 갖는 graph $G$를 표현하는 방식에는 크게 Adjacency matrix 방식과 Adjacency list 방식이 있다.   ■ Adjacency matrix      adjacency matrix 형태 ($n×n$ array)로 표현될 수 있다.   \\[a_{ij} =  \\begin{cases} \t1 &amp; \\text{if there is an edge from $v_i$ to $v_j$}\\\\ \t0 &amp; \\text{otherwise} \\end{cases}\\]  ● 특징      특정 edge가 constant time 내에 확인된다 : $Θ(1)$ (😍: 빠르군!)   $O(n^2)$의 메모리 공간을 요구한다. (🤔: edges 개수가 많지 않다면 공간 낭비가 심하군!)       ■ Adjacency list      각 vertex에 대해 $│V│$개의 linked lists 형태로 표현되며, 각 vertex는 연결된 vertices의 정보를 담고있다.   ● 특징      undirected graph의 경우 $2│E│$의, directed graph의 경우 $│E│$의 메모리 공간을 갖는다. : overall memory complexity = $O(│E│)$   특정 edge 탐색에는 $O(│V│)$의 시간 복잡도가 소요된다. (😅: adjacency matrix보단 느리지만 반복적인 탐색이 가능하므로 편리하군!)       ■ Adjacency matrix vs. Adjacency list: 무엇을 사용할 것인가?   graph를 표현하는 두 가지 기법 adjacency matrix와 adjacency list 은 어떤 특정 방법이 더 좋다고 말할 수 없다. 상황에 따라 특정 방법이 유리할 수도, 불리할 수도 있기 때문이다.        graph에서 edges의 최소 개수는 V개이며, 최대 개수는 V²개이다.           Case1. $│E│ \\approx │V│^2$ : dense graph       edges의 개수가 최대값에 가까운 경우, graph를 “dense“하다고 한다.            Case2. $│E│ \\approx │V│$ : sparse graph       edges의 개수가 최소값에 가까운 경우, graph를 “sparse“하다고 한다.       graph의 edges 개수 $│E│$는 graph에서 더 좋은 알고리즘을 선택하는 데에 중요한 요소이다.   ● 예시 : World Wide Web   검색엔진은 약 80억개의 웹사이트를 갖고 있다. 이 경우 adjacency matrix보다 adjacency list로 구현하는 것이 더 효과적이다. adjacency matrix로 표현하는 경우 $\\text{80억}×\\text{80억}$개의 메모리가 필요한데, 80억 개의 웹사이트 규모에 비해 각 웹사이트를 연결하는 edges의 개수는 sparse하기 때문에 adjacency list로 구현하는 것이 메모리 낭비를 줄일 수 있는 방법이기 때문이다.           3.2. Depth-first search in undirected graphs       3.2.1. Exploring mazes      위 그림과 같이 미로에서 길찾는 문제는 길의 각 지점을 vertices로 삼아 graph로 표현함으로써 해결할 수 있다.       ■ node $v$에서 갈 수 있는 모든 nodes를 찾는 알고리즘   procedure explore(G, v):  Input:\tG=(V,E) is a graph; v∈V Output:\tvisited(u) is set to true for all nodes u reachable from v  \tvisited(v) = true \tprevisit(v) \tfor each edge (v,u) ∈ E: \t\tif not visited(u):\texplore(u) \tpostvisit(v)   ※ 여기서 previsit(), postvisit()에 대해서는 3.2.4에서 자세히 다룬다. ※   그래프로 표현한 미로 문제에서 node $A$에서 갈 수 있는 모든 nodes를 찾기위해 explore(G,A)를 수행한 결과는 다음과 같이 tree edges로 표현할 수 있다.      ※ tree는 cycle이 없는 connected graph이다.  ※ 여기서 점선은 이전에 방문한 nodes로 되돌아가는 back edges를 의미한다. ※       ● 알고리즘 동작           vertex $v$를 방문했다고 표시            previsit($v$)            vertex $v$와 연결된 edges 집합 $E$에 있는 각 edge에 대해 아래 동작을 반복:       3.1. edge $(v,u)$일때, vertex $u$를 방문하지 않았다면, vertex $u$에 대해 explore을 실행            postvisit($v$)           ● 알고리즘 증명   graphs 연구에서 종종 사용되는 능동적 유도 과정을 통해 위 알고리즘이 vertex $v$에서 갈 수 있는 모든 vertices를 탐색가능함을 증명해본다.      알고리즘의 타당성을 위해, explore($v$)를 통해 직접적으로 연결되지 않은 vertex $u$를 탐색할 수 있음을 증명하면 된다.      explore($v$)는 $v$와 직접적으로 연결된 vertex $z$를 탐색할 수 있다. 이를 통해 explore($z$)를 재귀적으로 호출하게 된다.   따라서 vertex $v$와는 직접적으로 연결되지 않지만 vertex $v$의 neighbors와 직접 연결된 vertex $w$를 탐색할 수 있다. 이를 통해 explore($w$)를 재귀적으로 호출하게 된다.   이러한 행위를 반복함으로써 vertex $v$와 직접 연결되지는 않지만 도달할 수 있는 모든 vertices(e.g., $u$)를 탐색할 수 있다.       3.2.2. Depth-first search   depth-first search (DFS) 알고리즘은 explore을 반복적으로 수행함으로써 전체 graph를 순회한다.   ■ 알고리즘   procedure dfs(G): \tfor all v ∈ V: \t\tvisited(v) = false \tfor all v ∈ V: \t\tif not visited(v):\texplore(v)       ■ 실행시간 분석           각 node에 대해 고정적으로 실행되는 $O(1)$의 작업(e.g., visited (혹은 pre/postvisit))       ⇒ total complexity = $O(│V│)$            가보지 않은 곳으로 가기 위해 인접 edges를 탐색하기 위한 loop (in explore function)       ⇒ total complexity = $O(│E│)$ (∵ 각 edge마다 2번씩 탐색되므로)       따라서 DFS의 overall running time complexity = $O(│V│ + │E│) $       3.2.3. Connectivity in undirected graphs   undirected graph의 Connectivity(연결성)에 대해 알아본다.           Connectedg graph : 어떤 node에서 다른 node로 가는 path가 언제나 존재하는 경우            Unconnected graph : 어떤 node에서 다른 node로 가는 path가 존재하지 않을 수도 있는 경우                       connected components(= subgraph)로 구성된다.           ※ connected components는 분리된 connected regions을 의미한다. ※                   DFS 알고리즘은 각 node $v$에 정수 ccnum[v]값을 할당함으로써 graph의 연결성을 검증하는 수단으로 사용될 수 있다. (즉, connected graph라면 ccnum[v]의 값이 $│V│-1$개일 것이다.)   procedure previsit(v): \tccnum[v] = cc   ※ cc의 초기값은 0이며, DFS가 explore을 호출할 때마다 하나씩 값이 증가된다. ※       3.2.4. Previsit and postvisit orderings   explore 알고리즘에서 수행되는 previsit()과 postvisit()의 동작에 대해서 자세히 살펴본다. DFS알고리즘은 undirected graph의 연결성뿐만 아니라 보다 다양한 문제에서 사용될 수 있는데, 이러한 다재다능성은 previsit과 postvisit에 의해 실현된다. 가령, 각 node에 대해 previsit과 postvisit은 다음의 두 중요한 events 시간을 기록한다.           previsit : the moment of first discovery (해당 node를 처음으로 발견된 순간)       procedure previsit(v): \tpre[v] = clock \tclock = clock + 1            postvisit : the moment of final departure (해당 node를 떠나는 순간)       procedure postvisit(v): \tpost[v] = clock \tclock = clock + 1       tree edges에 previsit과 postvisit에 의해 기록된 시간을 표시하면 다음과 같다.      ※ 예를 들어, node $I$는 5번째 순간에 최초로 탐색되었으며, 8번째 순간에 $I$ 너머의 모든 vertices에 대해 탐색이 완료되어 explore()실행이 종료되었음을 알 수 있다. ※       ■ 특징   어떤 nodes $u$와 $v$에 대해 두 개의 구간 [$\\text{pre($u$)}$, $\\text{post($u$)}$]와 [$\\text{pre($v$)}$, $\\text{post($v$)}$]는 서로 disjoint(분리되)거나 하나가 다른 하나를 포함하는 관계이다.   ※ ∵ DFS알고리즘은 stack구조를 이용하여 구현되는데, stack은 LIFO(Last-In First-Out)으로 동작한다. 따라서 [$\\text{pre($u$)}$, $\\text{post($u$)}$]는 본질적으로 vertex $u$가 stack에 있는 동안의 시간을 의미한다. ※           3.3. Depth-first search in directed graphs   이전까지는 undirected graph에 대한 DFS에 대해 살펴보았다. 이번에는 edges가 방향을 갖는 directed graphs에 대해 DFS를 적용해본다.   3.3.1. Types of edges   ■ 배경지식: tree에서 nodes간의 관계에 대한 terminology(용어)      ※ 위 이미지를 기준으로 각 terminology에 대해 설명한다. ※      $A$: search tree의 root. 즉, 이 외의 모든 nodes가 descendant(자손)이다.   $E$: descendants로 nodes $F, G, H$ 를 갖는다. 반대로, 이 세 nodes $F,G,H$에 대한 ancestor이다.   $C$: node $D$의 parent(부모);   $D$: node $C$의 child(자식).   즉, 요약하자면 $v_{\\text{parent}}→v_{\\text{child}}$이다.       ■ Types of edges   undirected graph에 대해 DFS를 적용할 때, edges를 다음과 같이 두 종류로 분류할 수 있었다.      tree edges   nontree edges   이와 달리 undirected graph에서는 edges를 다음과 같이 네 종류로 분류할 수 있다.              Tree edges : DFS forest의 부분에 해당하는 edges  ($v_{\\text{parent}}→v_{\\text{child}}$)       ※ solid한 실제 edges에 해당. 이 외의 모든 edge 종류들은 점선에 해당하는 가상의 edges이다. ※            Forward edges : DFS tree에서 어떤 한 node에서 nonchild descendant로 향하는 edges ($v_{\\text{ancestor}}→v_{\\text{nonchild descendant}}$)            Back edges : DFS tree에서 ancestor를 향하는 edges ($v_{\\text{descendant}}→v_{\\text{ancestor}}$)            Cross edges : descendant으로도 ancestor으로도 향하지 않는 edges. 즉, 이미 완벽하게 탐색 완료된(= already postvisited) node를 향하는 edges           ■ Ancestor와 descendant의 관계   Ancestor와 descendant의 관계는 pre와 post 값을 통해 바로 읽을 수 있다. 가령, edge $(u,v)$의 edge 종류는 다음과 같이 판별 가능하다.           Tree edges (or Forward edges): $u→v$   \\[[u\\ [v\\ v]\\ u]\\]           Back edges : $v→u$   \\[[v\\ [u\\ u]\\ v]\\]           Cross edges :   \\[[u\\ u]\\ [v\\ v] \\text{ or } [v\\ v]\\ [u\\ u]\\]          3.3.2. Directed acyclic graphs   ■ Acyclicity Test with DFS   directed graph는 path의 종류에 따라 두 종류로 구분할 수 있다.      cyclic graph: circular(순환하는) path $v_0→v_1→…→v_k→v_0$를 갖는 graph   acyclic graph: cycle이 없는 graph   이때, 한 번의 DFS알고리즘을 통해 graph의 acyclicity를 확인할 수 있다.   \\[\\text{directed graph가 cycle을 갖는다.} ⇔ \\text{DFS에서 back edge가 존재한다.}\\]  ※ ∵ $(u,v)$가 back edge라면, search tree에 $v→u$ 경로와 함께 cycle을 구성할 수 있기 때문에 ※ ※ ∵ graph가 어떤 cycle $v_0→v_1→…→v_k→v_0$를 갖는다면, 이 cycle의 첫 번째 node $v_0$는 다른 모든 nodes를 descendants로 갖는데, 마지막 $v_k$는 $v_0$로 향하는 back edge를 갖는다. ※       ■ Directed Acyclic Graphs (DAGs)   Directed Acyclic Graphs (DAGs)은 인과관계, 계층구조, 시간 종속성과 같은 관계를 모델링하기에 좋다. 어떤 task를 수행하기 전에 다른 task를 할 수 없다는 제약조건은 두 nodes간의 edge 방향으로 간편하게 표현되기 때문이다.   ※ 반면 그래프가 Directed Cyclic Graphs인 경우에는 이러한 순서가 의미가 없게 된다. ※   작업의 순서는 linearize(혹은 topologically sort)를 통해 표현할 수 있으며, 이는 DFS로 구현 가능하다. 이때, linearize는 post numbers를 내림차순으로 나열함으로써 구현된다.   ※ DAGs는 back edges를 가질 수 없다는 점을 기억하자. ※ ※ DFS를 이용하여 dag의 nodes를 sort하는 데에는 linear time이 소요된다. (∵ linearize) ※       ● DAG의 특징           dag에서 모든 edge는 작은 post number를 갖는 node로 향한다.       ∵ linearized가 post numbers의 내림차순으로 구현되므로            모든 dag는 적어도 하나의 source와 적어도 하나의 sink를 갖는다.       ∵ linearization은 ① source를 찾아 출력하고 graph에서 삭제 ② graph가 빌 때까지 이러한 과정을 반복하는 과정을 통해 구현될 수 있기 때문에           3.4. Strongly connected components   3.4.1. Defining connectivity for directed graphs   ■ Connectivity in directed graphs   undirected graphs에서 connectivity는 각각의 connected components에 대해 DFS를 수행함으로써 증명되었다. 반면, directed graphs에서 connectivity는 다음과 같이 정의된다.   \\[\\text{path $u→v$와 path $v→u$가 모두 존재할 때, 두 nodes $u, v$는 connected이다.}\\]  이러한 정의를 통해 strongly connected components의 개념을 정의할 수 있다.       ■ Strongly Connected Components      위 그림에서 (a)는 directed graph를 보여주며, 이를 구성하는 strongly connected components를 하나의 single meta-node로 표현한 결과(meta-graph)는 (b)이다. 이때 이 meta-graph는 dag임이 명확하다 (∵ nodes간에 cycle이 존재한다는 것은 strongly connected component임을 의미하므로 meta-node로 치환된다).   ● directed graph의 특징   모든 directed graph는 strongly connected components로 구성된 하나의 dag로 표현될 수 있다. 이러한 특징은 directed graph가 두 가지 연결성 구조를 가지고 있음을 시사한다.      상위 레벨에 존재하는 하나의 dag. (linearized 가능하다.)   하위 레벨에 존재하는 dag의 nodes. (더 자세히 조사할 수 있다.)       3.4.2. An efficient algorithm   Directed Graph를 strongly connected components로 분해하는 것은 여러 유용한 정보를 제공해준다.   ※ 운좋게도, 발전된 DFS를 통해 분해 과정에 linear time이 소요된다. ※   ● 특징 1. explore subroutine이 node $u$에서 시작할 때, 이 subroutine은 $u$에서 도달가능한 모든 nodes를 방문한 시점에 종료된다.   이러한 특징으로 인해 sink strongly connected component 내부의 어떤 node에서 explore를 수행하면 정확하게 해당 component만을 얻을 수 있다.  ⇒ sink strongly connected component를 알고있을 때, 하나의 strongly connected component를 찾는 방법       ● 특징 2. DFS를 통해 구한 최대 post number에 해당하는 node는 source strongly connected component에 속한다.   이를 통해 graph의 source strongly connected components 내에 있는 node를 찾을 수 있다.   ● 특징 3. $C$와 $C’$가 strongly connected components일 때, $C$에 있는 어떤 node에서 $C’$에 있는 다른 node로 가는 edge가 존재한다면, $C$에서의 최대 post number는 $C’$에서의 최대 post number보다 크다.      증명:          $C$가 $C’$보다 더 먼저 탐색된 경우, 특징1$^*$에 의해 도달가능한 모든 nodes를 방문한 뒤 (즉, $C’$가 모두 탐색된 후)에 $C$에 대한 탐색이 종료된다. 따라서 늦게 종료된 $C$의 post number가 $C’$의 post number보다 크다.     $C’$가 $C$보다 더 먼저 탐색된 경우, 특징1$^*$에 의해 $C’$가 이미 종료된 후에 $C$가 탐색된다.      따라서 strongly connected components는 내부의 최대 post number를 내림차순으로 정렬하여 linearized할 수 있다.       특징2를 활용하여 sink strongly connected component를 알 수 있다. $G^R$과 $G$는 같은 strongly connected components를 가지므로 특징2를 활용하여 $G^R$에 대해 source strongly connected components를 구하면 이는 $G$의 sink strongly connected component가 된다.      ※ 이때 $G^R$은 graph $G$의 reverse graph이다. ※       따라서 DFS를 이용하여 directed graph를 strongly connected components로 분해하는 알고리즘은 다음과 같다.      graph $G^R$에 대해 DFS를 수행한다.   step1 과정을 통해 얻은 post numbers의 내림차순에 해당하는 nodes 순서대로, grpah $G$에 대해 undirected connected components algorithm(from Section 3.2.2)를 수행한다.   ※ linear-time에 수행된다. ※          위 graph에 이 알고리즘을 적용하는 과정은 다음과 같다.:           graph $G^R$에 대해 DFS를 수행하여 얻은 post numbers의 내림차순 결과는 다음과 같다.:       $G, I, J, L, K, H, D, C, F, B, E, A.$            위 결과 순서대로 graph G에 대해 undirected connected components algorithm 수행 결과는 다음과 같다.:       ${G,H,I,J,K,L}, {D}, {C,F}, {B,E}, {A}$               References      eherbold/berkeleytextbooks, github: https://github.com/eherbold/berkeleytextbooks           ","categories": ["computerScience-algorithm"],
        "tags": ["computer science","algorithm","Algorithms","Sanjoy Dasgupta"],
        "url": "/computerscience-algorithm/algorithmsSanjoyDasgupta02/",
        "teaser": null
      },{
        "title": "[ROS2-001] 001. 개발환경 구축",
        "excerpt":"     주요 참고자료              페이지: 001 ROS 2 개발 환경 구축 (오픈소스 소프트웨어 &amp; 하드웨어: 로봇 기술 공유 카페 (오로카))          작성자: 표윤석            본 글은 표윤석님의 001 ROS 2 개발 환경 구축 (오픈소스 소프트웨어 &amp; 하드웨어: 로봇 기술 공유 카페 (오로카)) 글을 바탕으로 작성되었습니다.           ROS2 개발 환경 구축   ROS 2 개발을 위해 다음과 같이 개발 환경을 구축한다.                  구분       내용                       기본 운영 체제       Linux Mint 20.2                 로봇 운영 체제       ROS 2 Foxy FItzroy                 통합 개발 환경 (IDE)       Visual Studio Code                 프로그래밍 언어       Python 3(3.8.0), C++ 14                 기타       CMake 3.16.3, Qt 5.12.5, OpenCV 4.2.0           개발환경은 오로카-001 ROS 2 개발 환경 구축에서 추천하는 설정으로 맞추었다. 이 외의 다른 선택사항이 궁금하다면 본 글을 참고하길 바란다.           기본 운영 체제   Linux Mint 20.x 대신에 Ubuntu 20.04.x LTS를 사용해도 된다. 이전에 졸업프로젝트할 때 Ubuntu 18.04 LTS를 사용한 경험이 있는데, Linux Mint는 써본 적이 없어서 궁금하기도 했다.       Linux Mint vs. Ubuntu                  Feature       Ubuntu 20.04       Linux Mint 20                       Release Dates       2020.04.23       2020.06.25                 Support Period       2025       2025                 Based on       Debian       Ubuntu 20.04                 Linux Kernel       Linux 5.4 kernel       Linux 5.4 kernel           Ubuntu 대신 Linux Mint를 사용하는 대략적인 이유[1]는 다음과 같다.      느리고 리소스를 많이 소모하는 Ubuntu Software Center와 달리 Linux Mint의 Software Manager는 더 가볍고 빠르다.   Linux Mint는 외부 패키지 제거, 누락된 키 추가, 중복 항목 제거 등 Ubuntu에서 얻을 수 없는 옵션들을 제공한다.   Ubuntu는 모든 AMD 시스템에서 정기적으로 충돌하는 반면 Linux Mint는 안정적이다.   Unity 기반 그래픽인 Ubuntu와 달리, Linux Mint의 Cinnamon은 가벼운 그래픽 환경을 제공하여 속도가 빠르다.   사실 Linux Mint가 절대적으로 Ubuntu보다 좋다고 말할 수는 없다(Ubuntu의 사용자의 수가 Linux Mint보다 훨씬 많다). 찾아 본 결과, 둘이 큰 차이가 있는 것 같진 않다. 만일 기존에 windows를 사용했었고, 초보자라면 Ubuntu보다는 Linux Mint를 추천한다.       부팅 방법   나는 하나의 노트북으로 Windows와 Linux Mint를 모두 사용해야 한다. 두 가지 OS를 하나의 PC에서 동시에 사용하는 방법은 아래와 같이 크게 두 가지가 있다.      Virtual Machine(가상 머신)   Dual Booting (듀얼 부팅)       ① Virtual Machine   Virtual Machine은 컴퓨팅 환경을 소프트웨어로 구현한 것이다. 즉, 하나의 PC안에 가상의 PC를 생성하여 사용하는 것이다. Virtual Machine을 구현하는 소프트웨어는 크게 세 가지가 있다.      VirtualBox   VMware   클라우드 가상머신 EC2(Amazon Web Services), Google Compute Engine(Google Cloud Platform)      위 이미지(출처: VirtualBox)처럼 새 창 자체가 하나의 PC처럼 동작한다.   Virutal Machine은 설치 및 제거가 비교적 쉽지만 실제 물리적 PC가 아니므로 오류가 심하고, 느리고, 일부 소프트웨어를 지원하지 않는다는 단점이 있다. 따라서 간단한 작업을 할 때에는 유용하지만 개발을 하기에는 적합하지 않은 환경이다. 따라서 나는 두 번째 방법을 사용했다.       ② Dual Booting   멀티 부팅은 하나의 PC의 자원을 나누어 여러 OS를 동시에 사용하는 것이다. 자원을 나눈다는 것은 partition을 나눈다는 것과 같으며, ~~~Virtual Machine과 달리 물리적 PC를 기반으로 하기때문에 일반적인 사용감과 동일하다.   Windows+Ubuntu를 사용하든 Windows+Linux Mint를 사용하든, 기본적으로 파티션 나누는 방식은 동일하다. 듀얼부팅 방법은 검색어(“windows linux mint dual boot”) 결과로 나오는 여러 포스팅 중 하나를 골라서 따라하면 된다.      Dual Booting을 사용하면, PC의 전원을 켤 때 위와 같은 화면이 나오며 OS를 선택할 수 있다.           Linux Mint 20.x 설치   .ISO파일 다운로드   LinuxMint에서 Linux Mint 20.2 Cinnamon Edition의 .ISO파일을 다운로드 받는다. 나는 Download mirrors에서 KAIST를 통해 다운로드받았다.   라이브 USB로 만들기      Rufus를 이용하여 USB를 만들 수 있다. 사이트에서 사용 방법을 참고하여 ISO 파일로부터 부팅 가능한 USB 드라이브를 생성해준다.   Installation   전원이 켜질 때 F2(또는 F10 또는 F12; 삼성 노트북은 F2)를 눌러 BIOS 설정으로 들어간다. BIOS 설정에서 [Boot] 를 선택한다. 이후 Priority선택에서 위 USB 드라이브를 1순위로 선택 &amp; Save &amp; Exit하여 해당 OS로 들어간다. 이후 바탕화면에 있는 설치파일을 이용하여 설치해주면 된다.   ※ 설치할 때에는 꼭 language를 영어로 하는 것을 추천한다. 개발할 때에는 영어 설정이 무조건 디폴트이다..! ※      [참고] BIOS에서 부팅 순서 변경 방법 : https://steemit.com/kr/@nightofwin/4fsdnu   [참고] Linux Mint 20설치 방법(ISO다운로드부터 설치까지): https://websetnet.net/ko/how-to-install-linux-mint-20-the-simplest-way-possible/           Linux Mint 초기 세팅   한글 사용 가능   초반에 설치할 때 language를 영어로했다면, 한글이 입력되지 않을 것이다. 한글을 사용하는 방법은 여러가지가 있는데, 그 중 Nabi 소프트웨어를 이용한 방법을 소개한다.           Software manager에서 “Nabi”를 검색 및 다운로드한다.                   [System Settings]-[Input Method]에 들어간다.                   [Korean]에서 Input method framework를 “Hangul”로 변경해준다.                   이후 종료 후 다시 접속하면 우측 아래에 나비모양 아이콘이 생성된 것을 확인할 수 있다.                   나비모양 아이콘을 클릭 및 설정을 변경하여 한영 변환키를 설정할 수 있다.           한국에 있는 mirror로 변경   mirror는 컴퓨팅에서 자료 모음의 복사본이다 (출처: 위키백과). 세계 각 지역에 다양하게 분포된 사용자가 해당 파일을 빠르게 다운로드할 수 있도록 하기 위해 지역별로 mirror를 두어 일종의 cache처럼 사용한다. 따라서 나는 mirror site를 한국으로 변경해주었다.           [Software Sources]을 연다.                   Mirrors의 site를 한국 국기가 있는 것들로 바꿔준다.                      로봇 운영체제   로봇 운영체제로는 ROS 2 Foxy FItzroy를 사용한다. 설치 방법은 크게 세 가지가 있다.      소스 빌드   미리 빌드된 파일 사용   데비안 패키지 사용   이 중 세 번째 방법(데비안 패키지 사용)이 설치 및 업데이트가 가장 간단하므로 해당 방법을 사용하여 설치한다.       ROS 2 설치 (데비안 패키지 이용)   데비안 패키지를 사용하여 설치하는 방법은 ros의 공식 문서: Installing ROS 2 via Debian Packages를 참고하였다.   1. Set Locale   Locale은사용자 인터페이스에서 사용되는 언어, 지역 설정, 출력 형식 등을 정의하는 문자열이다. locale이 UTF-8을 지원하도록 다음의 명령어를 통해 설정해줄 수 있다.   locale  # check for UTF-8  sudo apt update &amp;&amp; sudo apt install locales sudo locale-gen en_US en_US.UTF-8 sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 export LANG=en_US.UTF-8  locale  # verify settings       2. Setup Sources   ROS 2 apt repositories를 시스템에 추가해야한다.        APT(Advanced Packaging Tool)란?      데비안 GNU/Linux에서 소프트웨어 설치 및 제거를 위해 사용하는 설치 응용 프로그램이다. ① 사용자가 원하는 소프트웨어 패키지를 로드하고 ② 종속 항목을 자동으로 설치해주고 ③ 설치된 모든 소프트웨어에 대한 업데이트를 자동으로 해주는 역할을 한다. 설치나 업데이트를 할 때 관련 데이터는 repository라는 소프트웨어 패키지의 DB로부터 가져올 수 있다.   이를 위해 우선 아래와 같이 GPG key를 승인받는다.   sudo apt update &amp;&amp; sudo apt install curl gnupg2 lsb-release sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key  -o /usr/share/keyrings/ros-archive-keyring.gpg   이후 repository를 sources list에 추가한다.   echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu focal main\" | sudo tee /etc/apt/sources.list.d/ros2.list &gt; /dev/null       3. Install ROS 2 packages   repositories를 세팅한 후 apt repository caches를 업데이트해준다.   sudo apt update sudo apt install ros-foxy-desktop ros-foxy-rmw-fastrtps* ros-foxy-rmw-cyclonedds*           ros-foxy-desktop : ROS, RViz, demos, tutorials를 포함       ros-foxy-rmw-fastrtps* : rmw_fastrtps는 ROS 2와 eProsima’s Fast DDS 미들웨어간의 interface를 제공   ros-foxy-rmw-cyclonedds* : 쉽고, 빠르고, 신뢰할 수 있고, 작은 ROS 2를 위한 Eclipse Cyclone DDS 미들웨어로, 이를 통해 Eclipse Cyclone DDS를 기본 DDS 구현으로 사용할 수 있음       4. Environment Setup   다음의 file을 소싱(Sourcing)함으로써 테스트를 위한 환경을 설정할 수 있다.   source /opt/ros/foxy/setup.bash   여기까지 문제없이 설치되었다면, 몇 가지 examples를 시도해볼 수 있다. 터미널창을 두 개 열어서 아래의 예제를 수행해보도록 한다.           C++ talker       source /opt/ros/foxy/setup.bash ros2 run demo_nodes_cpp talker                Python listener       source /opt/ros/foxy/setup.bash ros2 run demo_nodes_py listener                실행결과                  5. ROS 개발 툴 설치   ROS 2를 이용한 로봇 프로그래밍에 필수인 소프트웨어들을 아래와 같이 설치한다.   sudo apt update &amp;&amp; sudo apt install -y \\   build-essential \\   cmake \\   git \\   libbullet-dev \\   python3-colcon-common-extensions \\   python3-flake8 \\   python3-pip \\   python3-pytest-cov \\   python3-rosdep \\   python3-setuptools \\   python3-vcstool \\   wget   python3 -m pip install -U \\  argcomplete \\  flake8-blind-except \\  flake8-builtins \\  flake8-class-newline \\  flake8-comprehensions \\  flake8-deprecated \\  flake8-docstrings \\  flake8-import-order \\  flake8-quotes \\  pytest-repeat \\  pytest-rerunfailures \\  pytest   sudo apt install --no-install-recommends -y \\  libasio-dev \\  libtinyxml2-dev \\  libcunit1-dev       6. ROS 2 빌드 테스트   workspace 폴더를 생성한 뒤 빌드하며 ROS 2 설치가 잘 되어 빌드에 문제가 없는지를 알아본다.   source /opt/ros/foxy/setup.bashmkdir -p ~/robot_ws/srccd ~/robot_ws/colcon build --symlink-installls   아래와 같이 하위폴더로 build, install, log, src 폴더가 생성되었다면 빌드에 성공한 것이다.          7. Run Commands 설정   source /opt/ros/foxy/setup.bash는 환경을 설정하기 위해 file을 소싱하는 명령어이다. 매번 환경설정을 불러오기가 귀찮으므로 아래와 같이 run commands(rc)만을 모아두는 bashrc 파일에 자주 사용되는 alias, source, export를 설정해두면 매우 편리하다.   nano ~/.bashrc   bashrc 파일의 기존 내용은 냅두고 맨 아래에 다음의 설정을 추가해준다.   source /opt/ros/foxy/setup.bashsource ~/robot_ws/install/local_setup.bashsource /usr/share/colcon_argcomplete/hook/colcon-argcomplete.bashsource /usr/share/vcstool-completion/vcs.bashsource /usr/share/colcon_cd/function/colcon_cd.shexport _colcon_cd_root=~/robot_wsexport ROS_DOMAIN_ID=7export ROS_NAMESPACE=robot1export RMW_IMPLEMENTATION=rmw_fastrtps_cpp# export RMW_IMPLEMENTATION=rmw_connext_cpp# export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp# export RMW_IMPLEMENTATION=rmw_gurumdds_cpp# export RCUTILS_CONSOLE_OUTPUT_FORMAT='[{severity} {time}] [{name}]: {message} ({function_name}() at {file_name}:{line_number})'export RCUTILS_CONSOLE_OUTPUT_FORMAT='[{severity}]: {message}'export RCUTILS_COLORIZED_OUTPUT=1export RCUTILS_LOGGING_USE_STDOUT=0export RCUTILS_LOGGING_BUFFERED_STREAM=1alias cw='cd ~/robot_ws'alias cs='cd ~/robot_ws/src'alias ccd='colcon_cd'alias cb='cd ~/robot_ws &amp;&amp; colcon build --symlink-install'alias cbs='colcon build --symlink-install'alias cbp='colcon build --symlink-install --packages-select'alias cbu='colcon build --symlink-install --packages-up-to'alias ct='colcon test'alias ctp='colcon test --packages-select'alias ctr='colcon test-result'alias rt='ros2 topic list'alias re='ros2 topic echo'alias rn='ros2 node list'alias killgazebo='killall -9 gazebo &amp; killall -9 gzserver  &amp; killall -9 gzclient'alias af='ament_flake8'alias ac='ament_cpplint'alias testpub='ros2 run demo_nodes_cpp talker'alias testsub='ros2 run demo_nodes_cpp listener'alias testpubimg='ros2 run image_tools cam2image'alias testsubimg='ros2 run image_tools showimage'       통합 개발 환경 (IDE)   VSCode는 다양한 운영체제 및 프로그래밍 언어를 지원하며, 32bit, amd64, arm64를 지원하므로 ARM 계열의 임베디드 보드에서도 사용가능하다는 장점이 있다. 따라서 ROS 2 개발 환경으로 VSCode를 설치하도록 한다.   VSCode 설치   1. 설치   visual studio-Download에서 .deb 파일을 다운로드 및 실행하여 설치한다.          2. 실행   터미널을 열어 아래 명령어를 통해 실행한다.   code       3. Extensions 설치      위 이미지에서 왼쪽 다섯 번째 줄의 Extensions을 클릭하여 아래의 항목들을 설치한다.      C/C++/Python Extensions (VS Code Extensions for C++ and Python)            C/C++       CMake       CMake Tools       Python           ROS Extensions (VS Code Extensions for ROS, URDF, Colcon)            ROS (code: ms-iot.vscode-ros)       URDF (code: smilerobotics.urdf)       Colcon Tasks (code: deitry.colcon-helper)           File Format Extensions (VS Code Extensions for XML, YAML, Markdown)            XML Tools (code: dotjoshihohnson.xml)       YAML (code: redhat.vscode-yaml)       Markdown All in One (code: yzhang.markdown-all-in-one)           useful Extensions (VS Code Extensions for Etc.)            Highlight Trailing White Spaces (code: ybaumes.highlight-trailing-white-spaces)       EOF Mark (code: msfukui.eof-mark)       Bracket Pair Colorizer (code: coenraads.bracket-pair-colorizer)       Better Comments (code: aaron-bond.better-comments)               4. Workspace 설정   [File]-[Add Folder to Workspace]를 선택한 뒤, 이전에 만들었던 robot_ws 디렉토리(혹은 사용하고자 하는 workspace)를 선택하여 workspace를 세팅할 수 있다.       5. User Settings   다음의 세팅을 통해 VSCode 환경에서 ROS 개발을 할 수 있다.   settings.json   settings.json은 VSCode의 사용자별 글로벌 환경 설정을 지정하는 파일로, 여기에 기술된 설정들은 모든 작업 공간(workspace)에서 적용된다 (e.g., 미니맵 사용, 세로 제한 줄 표시, 탭 사이즈 등).   [참고] How to edit settings.json in Visual Studio Code? : https://supunkavinda.blog/vscode-editing-settings-json   ~/.config/Code/User/settings.json 를 아래와 같이 수정해준다.   {    \"cmake.configureOnOpen\": false,    \"editor.minimap.enabled\": false,    \"editor.mouseWheelZoom\": true,    \"editor.renderControlCharacters\": true,    \"editor.rulers\": [100],    \"editor.tabSize\": 2,    \"files.associations\": { // Note some extensions for ROS      \"*.repos\": \"yaml\",      \"*.world\": \"xml\",      \"*.xacro\": \"xml\"    },    \"files.insertFinalNewline\": true,    \"files.trimTrailingWhitespace\": true,    \"terminal.integrated.scrollback\": 1000000,    \"workbench.iconTheme\": \"vscode-icons\",    \"workbench.editor.pinnedTabSizing\": \"compact\",    \"ros.distro\": \"foxy\",       // Version of ROS    \"colcon.provideTasks\": true // Use the task supporting colcon}       c_cpp_properties.json   C/C++을 사용하기 위한 관련 설정이다. C/C++는 어떤 표준을 기준으로 규칙을 사용할 것인지, 컴파일 경로, intelliSense 모드 등을 설정할 수 있다.   Ctrl+shift+p를 누른 뒤, “c/c++ edit configuration (json)”을 선택하여 ~/robot_ws/.vscode/c_cpp_properties.json위치에 파일을 자동으로 생성할 수 있다. 파일 내용은 다음과 같이 설정한다.   {    \"configurations\": [      {        \"name\": \"Linux\",        \"includePath\": [          \"${default}\",          \"${workspaceFolder}/**\",          \"/opt/ros/foxy/include/**\"        ],        \"defines\": [],        \"compilerPath\": \"/usr/bin/g++\",        \"cStandard\": \"c99\",        \"cppStandard\": \"c++14\",        \"intelliSenseMode\": \"linux-gcc-x64\"      }    ],    \"version\": 4}       tasks.json   VSCode에서 외부 프로그램을 CLI(Command Line Interface)를 통해 연동하는 기능을 task라고 한다. ROS 2에서 빌드할 때 사용되는 colcon과 관련된 build, test, clean 작업을  task로 만들 수 있다. ~/robot_ws/.vscode/tasks.json위치에 아래의 내용을 설정한다.   {  \"version\": \"2.0.0\",  \"tasks\": [    {      \"label\": \"colcon: build\",      \"type\": \"shell\",      \"command\": \"colcon build --cmake-args '-DCMAKE_BUILD_TYPE=Debug'\",      \"problemMatcher\": [],      \"group\": {        \"kind\": \"build\",        \"isDefault\": true      }    },    {      \"label\": \"colcon: test\",      \"type\": \"shell\",      \"command\": \"colcon test &amp;&amp; colcon test-result\"    },    {      \"label\": \"colcon: clean\",      \"type\": \"shell\",      \"command\": \"rm -rf build install log\"    }  ]}       launch.json   Launch는 Run and Debug에서 사용되는 실행 명령어이다. launch.json의 설정을 통해 Launch가 실행되기 전(즉, 디버깅하기 전)에 사용할 Task를 지정하거나 콘솔 기능을 설정할 수 있다. ~/rotob_ws/.vscode/launch.json 위치에 아래의 세팅을 통해 Python과 C++언어에 맞는 디버깅 툴을 지정할 수 있다.   {  \"version\": \"0.2.0\",  \"configurations\": [    {      \"name\": \"Debug-rclpy(debugpy)\",      \"type\": \"python\",      \"request\": \"launch\",      \"program\": \"${file}\",      \"console\": \"integratedTerminal\"    },    {      \"name\": \"Debug-rclcpp(gbd)\",      \"type\": \"cppdbg\",      \"request\": \"launch\",      \"program\": \"${workspaceFolder}/install/${input:package}/lib/${input:package}/${input:node}\",      \"args\": [],      \"preLaunchTask\": \"colcon: build\",      \"stopAtEntry\": true,      \"cwd\": \"${workspaceFolder}\",      \"externalConsole\": false,      \"MIMode\": \"gdb\",      \"setupCommands\": [        {          \"description\": \"Enable pretty-printing for gdb\",          \"text\": \"-enable-pretty-printing\",          \"ignoreFailures\": true        }      ]    }  ],  \"inputs\": [    {      \"id\": \"package\",      \"type\": \"promptString\",      \"description\": \"package name\",      \"default\": \"topic_service_action_rclcpp_example\"    },    {      \"id\": \"node\",      \"type\": \"promptString\",      \"description\": \"node name\",      \"default\": \"argument\"    }  ]}      C++은 디버깅툴로 GDB를 사용. GDB를 실행하기 전에 colcon build를 수행하도록 세팅.   Python은 디버깅툴로 debugpy를 사용. 별도의 빌드 없이 디버깅하도록 세팅.           기타   QtCreator   QtCreator는 GUI를 개발하는 데에 사용되는 프로그램이다.   터미널에서 아래의 명령어를 통해 손쉽게 설치할 수 있다.   sudo apt install qtcreator   이후 터미널에서 qtcreator를 입력하여 실행할 수 있다.       QtCreator ROS   QtCreator ROS는 ROS의 통일된 개발환경 구축을 위해 QtCreator의 플러그인 형태로 만든 것이다. QtCreator에 ROS 개발 환경 설정을 더 편하게 해놓았다. 설치 방법은 ROS Industrial Website의 문서를 참고했다.           설치파일 다운로드       ROS Industrial Website의 Docs » How to Install (Users)에서 가장 최신 버전의 설치 파일을 다운로드 받는다. 나는 Bionic Online Installer(.run)를 설치하였다.            설치       .run파일은 linux에서 직접적으로 설치가 불가능하다. 따라서 아래의 명령어를 통해 실행가능한 파일로 바꿔준 뒤, 실행한다.       cd ~/Downloadschmod +x qtcreator-ros-bionic-latest-online-installer.runsudo ./qtcreator-ros-bionic-latest-online-installer.run           ※ .run 파일이 설치된 경로로 이동해주어야한다. 나는 ~/Downloads경로에 설치했으므로 해당 디렉토리로 이동하였다. ※              기본세팅은 /root/QtCreator인데, 여기에 저장하면 관리가 어렵기때문에 설치 경로는 /home/(유저네임)/QtCreator로 지정해주었다.              Qt Creator 컴포넌트를 선택해준다.       Next를 눌러가며 설치를 완료해준다.            업데이트       QtCreator를 열어서 [Help]-[Check Updates]를 눌러준다.                   설치가 잘 되었는지 확인       [File]-[New File or Project]-[Projects]-[Other Project]에 “ROS Workspace” 항목이 존재한다면 모든 설치가 정상적으로 완료된 것이다.           Delete ROS 2   만일 추후에 ROS 2를 삭제하고싶다면(재설치 등의 이유로), 아래의 명령어를 통해 삭제 가능하다.   sudo apt remove ros-foxy-* &amp;&amp; sudo apt autoremove           References      “000 로봇 운영체제 ROS 강좌 목차”, 네이버카페 오로카 : https://cafe.naver.com/openrt   Linux-mint vs. Ubuntu : https://itsfoss.com/linux-mint-vs-ubuntu/   BIOS에서 부팅 순서 변경 방법 : https://steemit.com/kr/@nightofwin/4fsdnu           ","categories": ["robotics-ros"],
        "tags": ["robotics","ROS","ROS2"],
        "url": "/robotics-ros/ros2001/",
        "teaser": null
      },{
        "title": "[NeuromorphicVision-001] 뉴로모픽 비전(Neuromorphic Vision)의 배경 및 개념",
        "excerpt":"    랩세미나에서 Neuromorphic vision에 관한 발표를 보고 관심이 생겼다. 관련하여 배경과 주요 개념 및 현황을 조사하고자 본 글을 작성하게 되었다.       Neuromorphic Vision   배경   기존 프레임 기반 이미지 센서   현재 가장 대중적인 이미지 센서는 다음과 같다.      CCD(charge-coupled devices)   CMOS APS(Active Pixel Sensors)   두 이미지 센서는 모두 시각적 정보를 연속적인 “스냅샷(snapshot)” 영상 (frames)의 형태로 취득하는 프레임 기반 이미지 센서이다. 따라서 다음의 한계를 수반한다.           고정된 프레임 속도(frame rate)로 동작함에 따라 발생하는 문제              낮은 프레임 속도인 경우, 중요한 정보의 손실이 발생       높은 프레임 속도인 경우, 과하게 불필요한 정보까지 취득           ⇒ 데이터 집약적(data-intensive)이거나 지연에 민감한(delay-sensitive) 응용 분야(e.g., 고속 모터 제어, 자율 로봇 네비게이션)에서 치명적인 단점으로 작용       또한 기존의 이미지 센서는 시각정보를 디지털 정보로 받아들이기 위한 2차원 이미지 센서 어레이(2D image sensors array, CCD or APS), 시각 정보를 저장하기 위한 메모리 유닛(memory unit), 그리고 컴퓨터비전 알고리즘을 실행하기 위한 프로세싱 유닛(processing unit)으로 구성된다. 이러한 폰 노이만 구조[개념]는 이미지 센서와 프로세싱 유닛 간의 데이터 이동에 따라 다음의 한계를 수반한다.           지연 (latency)            높은 통신 대역폭(communication bandwidth)이 요구됨            높은 전력 소모 (high power consumption)           이벤트 기반 뉴로모픽 비전   고전적인 이미지 센서와 달리 생물학적 시스템, 예를 들어 인간의 시각 시스템은 시각 정보가 지나가는 렌즈(lens), 들어온 정보를 인지하고 전처리하는 망막(retina), 그리고 추출된 정보가 지나가는 시신경(optic nerves)과 최종적인 처리를 위한 시각피질(visual cortex)로 구성된다. 뉴로모픽 비전 센서는 이 중 망막(retina)의 역할에서 아이디어를 얻었다.      망막의 역할            불필요한 시각 정보를 버림       뇌에서 더 많은 정보 처리(e.g., 패턴 인식, 해석)를 하도록 가속화해줌           즉, 망막은 인간이 시각적 정보를 동시에 센싱하고 전처리할 수 있도록 해준다.   뉴로모픽 비전(Neuromorphic Vision)은 이러한 생물학적 비전 시스템을 모방하여 기존의 디지털 비전 센서들이 갖고있는 문제점을 해결하고자 하였다. 즉, 저전력(low-power) 고효율(high-efficient) 이미지 처리를 목표로 한다.           개념   뉴로모픽 비전은 뉴로모픽 비전 칩(Neuromorphic vision chips)을 통해 달성된다. 뉴로모픽 비전 칩은 여러 개의 특수 목적 비전 센서를 활용하여 고차원의 입력을 저차원의 출력으로 감소시켜준다.   구조   뉴로모픽 비전 칩은 입력을 전처리하기 위해 다양한 특수목적 비전 센서를 여러 개 사용한다.      특수목적 센서의 예시            운동방향 센서(direction-of-motion sensors)       속도센서(velocity sensors)       추적센서(tracking sensors)           이러한 각 지각 센서(perceptive sensors)를 통해 고차원의 입력을 저차원의 출력으로 감소시킬 수 있다.       구현      뉴런 ⇔ 트랜지스터 회로   시냅스 ⇔ 시냅스 모방 소자   뉴로모픽 비전 칩의 기능은 트랜지스터 혹은 새로운 장치 기반의 고전적인 디지털 VLSI[개념]기술로 제작된 아날로그 회로를 통해 구현된다. VLSI 기술을 사용하여 크기가 작고 밀집된 형태로 제작할 수 있으므로 기존의 프레임 기반 이미지 센서에 비해 이점을 갖는다.       특징      아날로그 방식(analogue) : 뉴로모픽 비전 칩의 기능은 아날로그 회로를 통해 구현된다.   에너지 효율적(energy-efficiently processing) : 디지털 시스템에 비해 10,000배 낮은 전력 소비   빠른 속도로 동작 (quickly processing) : 폰 노이만 구조가 아닌 병렬 구조로 연산을 수행하기 때문에 병목 현상을 해결하였다.   높은 동적 범위 (HDR, high dynamic range)   높은 시간적 해상도 (high temporal resolution)   비동기(asynchronously processing) : 비동기 이벤트 스트림을 생성하여 환경을 감지 및 인식   사건 중심(event-driven processing) : 프레임 기반이 아닌 사건 기반의 환경 인식   self-adaptive   확장성(scalable)           (부록)관련 개념   집적회로 (IC, Integrated Circuit)   IC(Integrated Circuit, 집적회로)란, 반도체에 만든 전자회로의 집합을 의미한다.   집적도에 따른 분류   IC는 동일한 면적에 얼마나 많은 논리 소자를 집적했는지에 관한 지표인 집적도에 따라 분류할 수 있다.      SSI (Small Scale Integration) : 소규모 집적회로 (~100개의 게이트)(e.g., 기본적인 게이트 기능, 플립플롭)   MSI (Medium Scale Integration) : 중간 규모 집적회로 (100~1,000개의 게이트) (e.g., 인코더, 디코더, 카운터, 레지스터, 멀티플레서, 디멀티플렉서, 소형 기억 장치)   LSI (Large Scale Integration) : 대규모 집적회로 (1,000~10,000개의 게이트) (e.g., 컴퓨터의 메인 메모리, 계산기의 부품)   VLSI (Very Large Scale Integration) : 초대규모 집적회로 (10,000~1,000,000개의 게이트) (e.g., 대규모 메모리, 대형 마이크로프로세서, 단일 칩 마이크로프로세서)            CMOS 기술의 설계 규칙 규격화로 그 제조 기술이 널리 보급되었음           ULSI (Ultra Large Scale Integration) : (1,000,000~개의 게이트) (e.g., 인텔의 486, 펜티엄의 ULSI)   최근에는 하나의 집적회로에 최소 억 단위개의 소자가 집적된다. 따라서 이러한 분류는 점점 의미가 없어지고 있는 추세이다.       제작 구성에 따른 분류   집적회로는 회로의 구성 요소에 따라 다음과 같이 분류할 수 있다.           저항-트랜지스터 로직 (Resistor-transistor logic, RTL)                     초기의 상용 논리군으로, 디지털 게이트의 기본 동작을 설명하기에 좋은 출발점이다.       최근에는 거의 쓰이지 않는다.                다이오드-트랜지스터 로직 (Diode-transistor logic, DTL)                     최근에는 거의 쓰이지 않는다.                트랜지스터-트랜지스터 로직 (Transistor-transistor logic, TTL)              양극성 트랜지스터와 NAND 회로를 사용한다.       가격이 저렴하며 논리회로 구현에 많이 사용된다.                이미터-결합 논리 (Emitter-coupled logic, ECL)              기본회로로 NOR 회로를 사용한다.       게이트 지연시간이 적어 고속회로로 사용된다.                금속-산화물 반도체 (Metal-oxide semiconductor, MOS)              금속 산화물을 사용하며, 절연체를 입혀서 구성되므로 집적도가 높다.       전력소모가 적으며 제조가 쉽다.                상보형 MOS (Complementary metal-oxide semiconductor, CMOS)              인버터회로에 P-채널과 N-채널 트랜지스터를 같이 구성한다.       소비전력이 적다.       속도가 느리다.               기타      폰 노이만 구조 : 대부분의 컴퓨터가 따르고 있는 기본 구조로, 주기억 장치, 중앙 처리 장치, 입출력 장치로 구성된다. 폰 노이만 구조 기반 반도체 칩은 정보를 순차적으로 처리하므로 대량의 비정형 정보에 대해 병목현상의 한계를 갖는다.           References      Indiveri, Giacomo, and Rodney Douglas. “Neuromorphic vision sensors.” Science 288.5469 (2000): 1189-1190.   Liao, Fuyou, Feichi Zhou, and Yang Chai. “Neuromorphic vision sensors: Principle, progress and perspectives.” Journal of Semiconductors 42.1 (2021): 013105.         다음 포스팅에서는 뉴로모픽 비전의 현황과 전망, 그리고 사례에 대해 알아본다.          ","categories": ["ai-computerVision"],
        "tags": ["robotics","robot vision","neuromorphic vision"],
        "url": "/ai-computervision/neuromorphicVision01/",
        "teaser": null
      },{
        "title": "[NeuromorphicVision-002] 뉴로모픽 비전(Neuromorphic Vision)의 현황(1)",
        "excerpt":"    랩세미나에서 Neuromorphic vision에 관한 발표를 보고 관심이 생겼다. 관련하여 배경과 주요 개념 및 현황을 조사하고자 본 글을 작성하게 되었다.       Neuromorphic Vision   현황   기존의 프레임 기반 카메라의 한계를 극복하기 위해 생물학적 망막을 모사한 실리콘 망막(silicon retina)이 개발되었다.           Mahowald and Mead, silicon VLSI retina in 1991       적응적 광수용체와 픽셀의 2차원 육각격자의 칩으로 구성된다. 광수용체, 양극성 세포(bipolar cells), 그리고 수평 세포(horizontal cells)로 구성된 생물학적 망을 모방하였다.            Parvo-Magno retina, Zaghloul and Boahen, silicon VLSI retina       5개의 망막 층으로 모델링되었다.            Tobi Delbruck’s team, practicable silicon retina DVS(Dynamic Vision Sensor) based on biological principles       오늘날 대부분의 실리콘 망막은 이러한 Tobi Delbruck and Christoph Posch의 구조에서 파생되었다.       실리콘 망막이 현실세계에 실용적으로 적용되기 시작한 것은 Tobi Delbruck 팀의 DVS(Dynamic Vision Sensor) 이후였으며, 그 이전까지는 뉴로모픽 비전의 가능성을 증명하기 위해 이론적 위주로 개발되어왔다.       Dynamic Vision Sensor (DVS)   구조   DVS의 픽셀 회로는 인간 망막의 삼층 모델을 모방하였다.              광수용체 층(Phtoreceptor layer)       광신호에 의해 발생한 광전류 $I_{\\text{ph}}$ 를 추적하여 전압 $V_{\\text{log}}$ 로 표현한다.            외부 플렉시폼 층(Outer plexiform layer)       스파이크 이벤트 $v_{\\text{diff}}$가 광전류 $I_{\\text{ph}}$의 변화에 따라 반응한다.       전류값이 증가하면(positive gradient) ON 스파이크, 감소하면(negative gradient) OFF 스파이크가 발생한다.                   내부 플렉시폼 층(Inner plexiform layer)       스파이크 이벤트에 의해 ON 이벤트(빛 증가를 의미, 하얀색으로 표현됨)와 OFF 이벤트(빛 감소를 의미, 검정색으로 표현됨) 출력한다.                  통신   칩 와이어링(Chip wiring) 문제로 인해 각 픽셀은 고유 케이블을 가질 수 없다. 이러한 문제는 Address Event Representation (AER) 기술을 통해 해결할 수 있다.         송신기(transmitter)에서 스파이크(spikes)에 의해 이진 이벤트(binary events)가 출력된다.   송신기로부터 출력되는 이진 이벤트는 주소 인코더(Address Encoder)를 통해 이진 주소(binary address)로 변환된다.   이진 주소는 버스 시스템을 통해 주소 디코더(Address Decoder)로 전달된다.   주소 디코더는 이진 주소를 이진 이벤트로 변환시킨다.   수신기(receiver)는 이진 이벤트를 스파이크로 변환한다.   이러한 과정을 통해 하나의 이벤트는 다음과 같이 정의된다.   \\[\\text{An event} : (x,y,t,p)\\]  ※ $(x,y)$: 픽셀 주소;  $t$: 타임스탬프;  $p$: 극성(positive/negative); ※       장점   DVS 픽셀은 강도(intensity)의 변화가 문턱값을 초과하는 이벤트에 자동적으로 반응한다. 즉, 이벤트 기반이라는 점에서 본질적으로 프레임 기반 카메라 센서와 구분된다. 이러한 특성으로 인해 이벤트 기반 뉴로모픽 비전 센서는 다음의 장점을 갖는다.           에너지 친화적인 특성 (Energy-friendly properties)       이벤트가 발생하는 경우에만 자동적으로 반응하기 때문에 에너지 친화적이다. 이는 차량에 장착되는 온보드 장치에서 매우 중요한 특성이다.            저지연 (Low latency)       각 픽셀이 독립적으로 동작하므로 프레임의 전역 노출이 필요하지 않아 지연시간이 매우 짧다. 가령, 제어 시스템을 위한 시간을 많이 할당해주기 위해서는 객체 감지 시스템에서 시간 소요를 줄이는 것이 관건이므로 이러한 특성은 인식 시스템(perception system)에서 매우 중요하다.            높은 동적 영역 (HDR, High dynamic range)       DVS와 같은 이벤트 기반 뉴로모픽 비전 센서는 프레임 기반 카메라의 동적 영역(60dB)에 비해 높은 동적 영역 (120dB)을 갖는다 (HDR). 이는 매우 어둡거나 밝은 자극에 대해 로버스트한 인지 시스템 갭라로 이어지므로 터널과 같은 공간을 지나는 자율주행 시스템에서 중요하다.            마이크로초 해상도 (Microsecond resolution)       아날로그 회로에 의해 밝기 변화가 빠르게 취득되므로, 이벤트는 마이크로초 해상도를 가질 수 있다. 이는 위급한 주행 상황에서 매우 중요한 특성이다.            모션블러가 없음 (No motion blur)       빠르게 주행하는 환경에서 프레임 기반 카메라는 모션 블러를 발생시켜 인지 성능이 감소한다. 따라서 이벤트 기반 뉴로모픽 비전은 동적 모션을 매우 정교하게 취득할 수 있다.           알고리즘   이벤트 노이즈 처리   이벤트 기반 뉴로모픽 비전 센서는 움직이는 물체로 인해 발생하는 밝기 변화를 잘 감지할 수 있다. 따라서 한편으로는 노이즈로 인한 밝기 변화에도 민감하기 때문에 원시 데이터(raw data)의 전처리가 중요하다. 이벤트 스트림에서 이벤트 노이즈를 제거하는 방법으로는 다음과 같이 두 가지 방법이 있다.              시공간 상관 필터 (*The spatial-temporal correlation filter*)       이벤트 $e_i=(x_i, y_i, t_i, p_i)$가 들어왔을 때, 현재 픽셀 위치 $(x_i, y_i)$의 가장 최근 주변 이웃들을 탐색한다 ($≤ \\text{distance } D$). 즉, 다음의 수식이 만족될 때, 노이즈가 발생하지 않았다고 판단한다.   \\[t_i-t_n&lt;d_t\\]             $d_t$ : 미리 정의된 문턱값       $t_i$ : 이벤트의 타임스탬프       $t_n$​ : 가장 최근에 발생한 이웃 이벤트의 타임스탬프                        모션 일관성 필터 (The motion consistency filter)       물체에 의한 움직임(object motion)인 경우, 로컬 영역에서 이전 이벤트와 일관된 움직임을 보여야한다. 따라서 속도 $(v_x, v_y)$를 통해 움직임 일관성을 평가하여 같은 움직임 영역인지를 판단한다. 만일 같은 평면을 공유하지 않으면 이벤트 노이즈(event noise)라 판단하여 제거한다.       구체적으로 모션 일관성 평면은 다음과 같이 공식화될 수 있다.   \\[ax_i+by_i+ct_i+d=0\\]             $(a,b,c,d)∈\\mathbb{R}^4$ : 평면 $M$을 정의       $(x_i,y_i)$ : 이벤트 $e_i$의 좌표       $t_i$ : 이벤트 $e_i$ 의 타임스탬프               비동기 이벤트의 표현   이벤트 기반 뉴로모픽 비전 센서는 단지 픽셀 레벨의 변화만을 전송하므로 희소한(sparse) 비동기 이벤트 스트림 데이터가 출력된다. 이는 CNN(*Convolutional Neural Network*) 기반 아키텍처와 같은 표준 비전 파이프라인에 직접적으로 적용할 수 없는 형태이다. 따라서 인코딩 기법을 통해 비동기 이벤트를 동기 이미지 혹은 그리드(*grid*) 표현으로 변환해주어야 한다.   여기서는 대표적인 최신 인코딩 기법으로 공간적 인코딩(Spatial encoding)과 시공간적 인코딩(Spatial-temporal encoding) 기법을 소개한다.           공간적 인코딩(Spatial encoding)       픽셀 위치 $(x_i, y_i)$에서의 이벤트 데이터를 고정된 시간 간격 (e.g., 30ms) 혹은 고정된 이벤트 개수(e.g., 500events)으로 저장하여 이벤트 스트림을 이벤트 프레임으로 변환해준다. 이벤트 프레임에서 픽셀 값은 일반적으로 마지막 이벤트의 polarity (positive event=1, negative event=-1) 혹은 고정된 간격에서 이벤트 개수의 확률적 특성으로 표현된다.                       상수 시간 프레임 (constant time frame)   \\[F_j^t=\\textbf{card}{(e_i \\mid T·(j-1) ≤t_i ≤T·j)}\\]                     $F_j^t$​ : 시간 간격 $T$​의 $j$​번째 프레임           $\\textbf{card}()$​ : 집합의 카디널리티(cardinality)           $e_i$ : 이벤트 스트림의 $i$번째 이벤트                                            상수 개수 프레임 (constant count frame)   \\[F_j^e=\\textbf{card}{(e_i \\mid E·(j-1) ≤i≤E·j)}\\]                     $F_j^e$ : 이벤트 $E$를 포함하는 $j$번째 프레임                                            이벤트 개수 프레임 (event count frame)              \\[{Hist}^+(x,y)=\\sum_{p_i=+1, t_i∈T}{δ(x-x_i, y-y_i)}\\]                     ${Hist}^+(x,y)$ : positive event에 대한 히스토그램           $δ$​ : 크로네커 델타 함수(Kronecker delta function)                                            시공간적 인코딩(Spatial-temporal encoding)       이벤트의 시공간 정보를 결합한 방식으로, 이벤트를 간결한 표현으로 변환해준다.                       활성 이벤트의 표면, Surface of active events (SAE)           픽셀값을 표현할 때, 강도(intensity)값 대신에 타임 스탬프값을 사용한다.   \\[\\text{SAE}:t_i↦P(x_i,y_i)\\]                     $t_i$ : 각 픽셀에서 가장 최근에 발생한 이벤트의 타임스탬프                   $(x_i,y_i)$에서 발생한 이전의 이벤트 정보를 무시한다는 단점이 있다.                                    Leaky integrate-and-fire (LIF)                      LIF는 생물학적 인지 원리와 계산 요소에서 영감을 받은 인공 뉴런으로, 뉴런이 DVS에서 발생된 입력 스파이크(이벤트)를 수신하여 막 전위(membrane potential)을 수정한다.막 전위가 미리 정의된 문턱값을 초과하면 스파이크 자극이 출력으로 전송된다.           LIF 뉴런은 다음과 같이 모델링될 수 있다.   \\[τ \\frac{dV}{dt}=-(V(t)-V_{\\text{reset}} )+RI(t)\\]                     $V(t)$ : 막전위           $I(t)$​ : 전체 시냅스 전류           $R$​ : 막저항           $τ$ : 막 시간 상수                   막전위가 threshold voltage $V_th$에 도달하면 reset voltage $V_{\\text{reset}}$​ 으로 리셋되며 뉴런이 스파이크를 출력한다 (fire). 또한 LIF 뉴런은 이벤트 데이터 표현뿐만아니라 스파이킹 신경망(SNN, spiking neural network)의 기본 유닛으로도 사용된다.                                    복셀 그리드 (Voxel grid)                      시간 도메인에서 이벤트 스트림의 해상도를 향상시키기 위한 이벤트 표현법이다. $N$개의 이벤트 집합 $(x_i, y_i,t_i,p_i )_{i∈[1,N]}$이 주어졌을 때, 시간 차원을 $B$개의 빈(bins)으로 나누어 범위를 $[0,B-1]$으로 스케일링한다.           이때 이벤트 복셀 그리드는 다음과 같이 정의된다.   \\[\\hat{t}=(B-1)\\frac{(t_i-t_1)}{(t_N-t_1)}\\\\ V(x,y,t)= \\sum_i^N{p_i k(x-x_i )k(y-y_i )k(t-\\hat{t})}\\\\ k(z)=max⁡{(0, 1-\\mid z \\mid)}\\]                     $k(z)$ : trilinear voting kernel                                            이벤트 스파이크 텐서 (EST, Event spike tensor)           EST는 end-to-end로 학습된 표현법이다. 주어진 시간 간격 $T$에서, EST는 컨볼루션 신호(convolved signal)를 샘플링하여 형성된다.   \\[S_±[x,y,t]= \\sum_{e_i∈p_±}{f_± (x_i, y_i, t_i ) k_c (x-x_i, y-y_i, t-t_i)}\\]                     $f_± (x_i,y_i,t_i)$​ : 각 이벤트에 할당된 측정값           $k_c$ : 이벤트 스트림으로부터 의미있는 신호를 얻기 위한 커널 컨볼루션 함수                                           데이터 세트   이벤트 기반 뉴로모픽 비전, 뉴로로보틱스, 그리고 자율주행 차량의 연구를 위한 다양한 데이터세트가 존재한다.           DET 데이터세트                     차선 검출을 위한 데이터 세트       터널, 다리, 육교, 그리고 도시 공간에서 주행한 다양한 교통 장면을 포함       라벨링된 1,280×800 pixels의 5,424 event frames을 포함                    학습 세트: 2,716 frames 검증 세트: 873 frame 테스트 세트: 1,835 frames           두 종류로 라벨: ‘차선이 아닌 픽셀’과 ‘차선인 픽셀’                                            N-CARS 데이터세트                     DVS를 통해 도시환경에서 기록된 데이터를 제공       12,336 자동차 샘플과 11,693 배경(noncar) 샘플을 포함                    학습 세트: 7,940 자동차 샘플+ 7,842 배경 샘플 테스트 세트: 그 외의 모든 샘플들                       각 예들은 잘못된 예제를 수동으로 보정한 반자동 프로토콜에 의해 라벨링된다.                        MVSEC 데이터세트                     다중 센서로 3차원 인식을 하기 위해 생성된 MultiVehicle Stereo Event Camera dataset       동기화된 스테레오 이벤트 기반 뉴로모픽 비전 시스템에 대한 첫 번째 데이터 세트       보정된(calibrated) 라이다 시스템으로부터 생성된 ground-truth 깊이 데이터를 포함       다양한 조도 및 주행 속도 환경에서의 긴 야외 시퀀스로 구성됨       이벤트 기반 시각적 주행거리 측정(visual odometry), 위치 파악(localization), 장애물 회피(obstacle avoidance), 그리고 3차원 재구성(3D reconstruction) 평가에 사용할 수 있음                        DDD17 데이터세트                     DAVIS 센서로 취득된 첫 번재 대규모 공공 데이터       스위스에서 독일까지 주행하며 고속도로와 도시 환경에서 기록한 데이터를 포함       1,000km 이상의 거리를 커버하는 서로 다른 날씨, 길, 그리고 빛 환경에서 수집된 12시간 이상의 데이터       속력, GPS 위치, 운전자 조향, throttle 그리고 브레이크와 같은 차량 데이터가 함께 수집됨                   References      Chen, Guang, et al. “Event-based neuromorphic vision for autonomous driving: a paradigm shift for bio-inspired visual sensing and perception.” IEEE Signal Processing Magazine 37.4 (2020): 34-49.         다음 포스팅에서는 뉴로모픽 비전의 현황과 전망, 그리고 사례에 대해 알아본다.          ","categories": ["ai-computerVision"],
        "tags": ["robotics","robot vision","neuromorphic vision"],
        "url": "/ai-computervision/neuromorphicVision02/",
        "teaser": null
      },{
        "title": "[References-001] opencv with C++ 프로젝트",
        "excerpt":"    기본   경로      \"filename.type\" : 코드가 위치한 곳   \"/filename.type\" : 코드가 위치한 프로젝트 기준           OpenCV   Mat   Generate   Mat() Mat(int rows, int cols, int type) Mat(Size size, int type) Mat (int rows, int cols, int type, const Scalar &amp;s) Mat (Size size, int type, const Scalar &amp;s) Mat (const Mat &amp;m)           Type                       CV_8U (=0) : 8-bit unsigned integer: uchar ( 0..255 )           ※ 일반적인 .JPG 파일의 type은 CV_8UC3이다. ※  ※ Vec3b(uchar) type으로 pixel단위 접근을 할 수 있다. ※                                   CV_8S (=1) : 8-bit signed integer: schar ( -128..127 )                                   CV_16U (=2) : 16-bit unsigned integer: ushort ( 0..65535 )           ※ Vec3w(ushort) type으로 pixel단위 접근을 할 수 있다. ※                                   CV_16S (=3) : 16-bit signed integer: short ( -32768..32767 )           ※ Vec3s(short) type으로 pixel단위 접근을 할 수 있다. ※                        CV_32S (=4) : 32-bit signed integer: int ( -2147483648..2147483647 )           ※ Vec3i(int) type으로 pixel단위 접근을 할 수 있다. ※                        CV_32F (=5) : 32-bit floating-point number: float ( -FLT_MAX..FLT_MAX, INF, NAN )           ※ Vec3f(float) type으로 pixel단위 접근을 할 수 있다. ※                                   CV_64F (=6) : 64-bit floating-point number: double ( -DBL_MAX..DBL_MAX, INF, NAN )           ※ Vec3d(double) type으로 pixel단위 접근을 할 수 있다. ※                              ※ 이러한 type 뒤에 channel의 개수를 붙여서 사용하기도 함 (e.g., CV_8UC1) ※  ※ channel이 하나 증가할 때 마다 8씩 증가된다. ※ e.g., CV_8UC1=0,  CV_8UC2=8,   CV_8UC3=16;         CV_8SC1=1,  CV_8SC2=9,  CV_8SC3=17;               Read   Mat img = imread(\"img.jpg\");           Write   int i=0; string title = \"img\"; imwrite(title+to_string(i), img);   ※ 이때, 저장할 경로의 directory가 이미 존재해야한다. 없을 경우 저장되지 않음. ※           Show   imshow(\"title\", Mat img); waitKey(0); destroyAllWindows();           Approach   each pixel   Mat::at(int row, int col) Mat::at(int i0, int i1, int i2) // in: Index along the dimension n Mat::at(const int *idx) // idx: Array of Mat::dims indices Mat::at(const Vec&lt;int,n&gt; &amp;idx) Mat::at(Point pt) // pt: Element position specified as Point(j,i).   img.at&lt;uchar&gt;(y,x) = 255; img.at&lt;Vec3b&gt;(y,x)[c] = 255;           type              Mat1b (=Mat&lt;uchar&gt;)  Mat2b (=Mat&lt;Vec2b&gt;)  Mat3b (=Mat&lt;Vec3b&gt;)  Mat4b (=Mat&lt;Vec4b&gt;)  Vec2b (=Vec&lt;uchar, 2&gt;)  Vec3b (=Vec&lt;uchar, 3&gt;)  Vec4b (=Vec&lt;uchar, 4&gt;)        Mat1w (=Mat&lt;ushort&gt;)  Mat2w (=Mat&lt;Vec2w&gt;)  Mat3w (=Mat&lt;Vec3w&gt;)  Mat4w (=Mat&lt;Vec4w&gt;)  Vec2w (=Vec&lt;ushort, 2&gt;)  Vec3w (=Vec&lt;ushort, 3&gt;)  Vec4w (=Vec&lt;ushort, 4&gt;)        Mat1s (=Mat&lt;short&gt;)  Mat2s (=Mat&lt;Vec2s&gt;)  Mat3s (=Mat&lt;Vec3s&gt;)  Mat4s (=Mat&lt;Vec4s&gt;)  Vec2s (=Vec&lt;short, 2&gt;)  Vec3s (=Vec&lt;short, 3&gt;)  Vec4s (=Vec&lt;short, 4&gt;)        Mat1i (=Mat&lt;int&gt;)  Mat2i (=Mat&lt;Vec2i&gt;)  Mat3i (=Mat&lt;Vec3i&gt;)  Mat4i (=Mat&lt;Vec4i&gt;)  Vec2i (=Vec&lt;int, 2&gt;)  Vec3i (=Vec&lt;int, 3&gt;)  Vec4i (=Vec&lt;int, 4&gt;)  Vec6i (=Vec&lt;int, 6&gt;)  Vec8i (=Vec&lt;int, 8&gt;)                 Mat1f (=Mat&lt;float&gt;)  Mat2f (=Mat&lt;Vec2f&gt;)  Mat3f (=Mat&lt;Vec3f&gt;)  Mat4f (=Mat&lt;Vec4f&gt;)  Vec2f (=Vec&lt;float, 2&gt;)  Vec3f (=Vec&lt;float, 3&gt;)  Vec4f (=Vec&lt;float, 4&gt;)  Vec6f (=Vec&lt;float, 6&gt;)                Mat1d (=Mat&lt;double&gt;)  Mat2d (=Mat&lt;Vec2d&gt;  Mat3d (=Mat&lt;Vec3d&gt;  Mat4d (=Mat&lt;Vec4d&gt;  Vec2d (=Vec&lt;double, 2&gt;)  Vec3d (=Vec&lt;double, 3&gt;)  Vec4d (=Vec&lt;double, 4&gt;)  Vec6d (=Vec&lt;double, 6&gt;)                Range   \\[\\text{Mat M }  = \\begin{bmatrix}  \t1&amp;2&amp;3\\\\ 4&amp;5&amp;6\\\\ 7&amp;8&amp;9 \\end{bmatrix}\\]  일 때,   Mat M_(행,열) = M(행의 범위, 열의 범위)   Example :   Mat M_0x = M(Range(0,1), Range(0,3));\t// [1,2,3] Mat M_1x = M(Range(1,2), Range(0,3));\t// [4,5,6] Mat M_2x = M(Range(2,3), Range(0,3));\t// [7,8,9]  Mat M_x0 = M(Range(0,3), Range(0,1));\t// [1;4;7;] Mat M_x1 = M(Range(0,3), Range(1,2));\t// [2;5;8;] Mat M_x2 = M(Range(0,3), Range(2,3));\t// [3;6;9;]  Mat M__ = M(Range(0,2), Range(1,3));\t// [2,3; 5,6;]       Circle   circle(img, Point2f, 3, Scalar(0, 255, 0), 3);           Resize   Mat::resize(size_t sz)\t// sz: New number of rows Mat::resize(size_t sz, const Scalar &amp;s) // s: Value assigned to the newly added elements   resize(src, dst, Size(width, height));           Convert Type   Mat::convertTo(OutputArray m, int rtype, double alpha=1, double beta=0)   src.convertTo(dst, CV_8UC3);           C++   min/max   int minVal = min({1,2,3,4}); // 1 int maxVal = max({1,2,3,4}); // 4   이런식으로 중괄호를 통해 여러 값에 대해 최소/최대값을 구할 수 있다.           Vector   array와 달리 동적으로 원소를 추가할 수 있으며 크기가 자동으로 늘어난다.   생성   vector&lt;int&gt; v;       추가 &amp; 삭제   v.push_back(1);\t// vector의 back에 원소를 추가 v.pop_back();\t// vector의 back에 원소를 삭제       접근   int v1 = v[i];\t// i번째 원소를 반환 int v1 = v[i];\t// i번째 원소를 반환 int v_front = v.front();\t// front([0]) 원소를 반환 int v_back = v.back();\t\t// back([size-1]) 원소를 반환       기타   bool isEmpty = v.empty();\t// vector가 비었으면 true를 반환 int size = v.size();\t\t// vector의 원소 개수를 반환           Queue   생성   queue&lt;int&gt; q;       추가 &amp; 삭제   q.push(1);\t// queue의 back에 원소 추가 q.pop();\t// queue의 front쪽 원소를 삭제       탐색   int front = q.front();\t// queue의 front쪽 원소를 반환 int back = q.back();\t// queue의 back쪽 원소를 반환       기타   bool isEmpty = q.empty();\t// queue가 비었으면 true를 반환 int size = q.size();\t\t// queue의 size를 반환           ","categories": ["etc-references"],
        "tags": ["references","opencv","C++"],
        "url": "/etc-references/opencv/",
        "teaser": null
      },{
        "title": "[ROS2-002] 002. 메시지 통신(message communication)",
        "excerpt":"     주요 참고자료              페이지: 001 ROS 2 개발 환경 구축 (오픈소스 소프트웨어 &amp; 하드웨어: 로봇 기술 공유 카페 (오로카))          작성자: 표윤석              본 글은 오로카(오픈 로보틱스 커뮤니티)의 표윤석님께서 작성하신 로봇 운영체제 ROS2 강좌를 공부한 뒤 정리한 내용으로, 내용의 생략과 수정이 있습니다. 원문을 참고하고싶으신 분들은 본 글의 References 파트를 참고해주시길 바랍니다.         메시지 통신(message communication)           노드(node) : 최소 단위의 실행가능한 프로세스     메시지(message) : integer, floating point, boolean 등과 같은 변수 형태의 데이터      ROS2에서 노드(node)끼리 데이터를 주고받는 것을 메시지 통신(message communication)이라고 한다. 이때 메시지 통신은 방법에 따라 다음과 같이 세 가지로 나누어진다.      토픽 (topic)   서비스 (service)   액션 (action)   파라미터 (parameter)           간단 비교 : 토픽 vs. 서비스 vs. 액션                  인터페이스(interface) : 데이터 통신에서 사용되는 데이터의 형태         다음의 세 가지 종류가 있음                  msg 인터페이스         srv 인터페이스         action 인터페이스                                 비교대상       토픽(topic)       서비스(service)       액션(action)                       연속성       연속성       일회성       복합(토픽+서비스)                 방향성       단방향       양방향       양방향                 동기성       비동기       동기       동기+비동기                 다자간 연결       1:1, 1:N, N:1, N:N (publisher:subscriber)       1:1 (server:client)       1:1 (server:client)                 노드 역할       발행자 (publisher) 구독자 (subscriber)       서버 (server) 클라이언트 (client)       서버 (server) 클라이언트 (client)                 동작 트리거       발행자       클라이언트       클라이언트                 인터페이스       msg 인터페이스       srv 인터페이스       action 인터페이스                 CLI 명령어       ros2 topic ros2 interface       ros2 service ros2 interface       ros2 action ros2 interface                 사용 예       센서 데이터, 로봇 상태, 로봇 좌표, 로봇 속도 명령 등       LED 제어, 모터 토크 On/Off, IK/FK 계산, 이동 경로 계산 등       목적지로 이동, 물건 파지, 복합 테스크 등                   토픽 (topic)         노드의 역할            Publisher : msg를 발간 (동작 트리거)       Subscriber : msg를 구독           메시지 형태 : 메시지 (message)   메시지 인터페이스 : msg 인터페이스   특징            연속성, 단방향, 비동기성       1:1 (default), 1:N, N:1 N:N 통신 가능 + 셀프 구독도 가능!       ROS 메시지 통신에서 가장 널리 사용되는 통신 방법 (대략 70%를 차지)           사용 예시            센서 데이터, 로봇 상태, 로봇 좌표, 로봇 속도 명령 등               CLI 명령어   ros2 node           ros2 node info : 노드 정보 확인                  ros2 topic           ros2 topic list : 현재 개발환경에서 동작중인 모든 노드의 토픽 정보 확인 가능                   ros2 topic info : 토픽 메시지 형태, 토픽의 발행 및 구독 정보 확인                   ros2 topic echo : 특정 토픽의 메시지 내용을 실시간으로 출력                   ros2 topic bw : 메시지의 대역폭(송수신받는 토픽 메시지의 크기) 확인                   ros2 topic hz : 토픽의 주기 확인                   ros2 topic delay : 토픽의 지연시간 확인                   ros2 topic pub : 토픽 발행                  ros2 bag           rosbag : 토픽을 파일 형태로 저장/불러오는 ROS의 기능으로 디버깅시 유용              ros2 bag record : rosbag 파일로 기록                   ros2 bag info : rosbag 파일의 정보를 확인                   ros2 bag play : rosbag 파일을 재생                  ros2 interface           ros2 interface show : 인터페이스 정보를 출력                   ros2 interface list : 인터페이스 목록을 출력                   ros2 interface packages                   ros2 interface package                   ros2 interface proto                  msg 인터페이스              서비스 (service)         노드의 역할            Service Client :  서비스를 요청(Request) (동작 트리거)       Service Server : 서비스를 응답(Response)           메시지 형태 : 서비스(*service*)   메시지 인터페이스 : srv 인터페이스   특징            일회성, 양방향, 동기성       1:1 통신           사용 예시            LED제어, 모터 토크 On/Off, IK/FK 계산, 이동 경로 계산 등               CLI 명령어   ros2 service           ros2 service list : 서비스 목록 출력                   ros2 service type : 서비스의 형태 확인                   ros2 service find : 특정 형태의 서비스를 사용하는 서비스명 찾기                   ros2 service call : 서비스 요청                       /clear                                   /kill                                   /reset                                   /set_pen                                   /spawn                                  ros2 interface       srv 인터페이스   ros2 interface show 명령어를 통해 서비스 인터페이스의 정보를 출력할 수 있다.      srv 인터페이스는 구분자로 나눠지는 다음의 구성 요소로 구성되어있음을 알 수 있다.      서비스 요청(Request) : e.g., float32 x, float32 y, float32 theta, string name   서비스 응답(Response) : e.g., string name   위 예시에서는 srv 인터페이스를 사용하여 service client와 service server가 각각 다음과 같이 서비스를 요청하고 서비스를 응답한다.              액션 (action)              노드의 역할              Action Client :  액션 목표(goal)를 지정 (동작 트리거)       Action Server : 액션 목표를 받아 특정 태스크를 수행 &amp; 중간 결과값인 액션 피드백(feedback)과 최종 결과값인 액션 결과(result)를 전송                메시지 형태 : 액션 (*action*) : 토픽과 서비스의 혼합              액션 목표(action goal), 액션 결과(action result) : 서비스 (service) ⇒ 일회성·비동기                    목표 전달(send_goal), 목표 취소(cancel_goal), 결과 받기(get_result)                       액션 피드백(action feedback) : 토픽 (topic) ⇒ 연속성·동기                    목표 상태(goal_state), 피드백(feedback)                                메시지 인터페이스 : action 인터페이스            특징                       복합(토픽+서비스), 양방향, 동기+비동기                        1:1 통신                        동기·비동기 방식의 혼합을 원활하게 구현하기위해 상태머신(state machine)을 사용                                   사용 예시              목적지로 이동, 물건 파지, 복합 태스크 등               CLI 명령어   ros2 node           ros2 node info : 노드의 정보를 출력                  ros2 action           ros2 action list : 액션 목록을 출력                   ros2 action info : 특정 액션의 정보를 출력                   ros2 action send_goal : 액션목표(action goal)를 전달              ※ 실시간으로 피드백을 표시하고싶다면 --feedback 옵션을 추가할 것 ※           ros2 interface           ros2 interface show : 인터페이스 정보를 출력                  action 인터페이스   ros2 interface show 명령어를 통해 액션 인터페이스의 정보를 출력할 수 있다.      action 인터페이스는 구분자로 나눠지는 다음의 구성 요소로 구성되어있음을 알 수 있다.      액션 목표(action goal) : e.g., float32 theta   액션 결과(action result) : e.g., float32 delta   액션 피드백(action feedback) : e.g., float32 remaining       파라미터 (parameter)         노드의 역할            Parameter Client :  서버로부터 파라미터 정보를 가져오거나 서버의 파라미터 정보를 세팅       Parameter Server : 클라이언트의 요청을 수행           특징            서비스의 목적은 서비스 요청과 응답이라는 RPC(Remote Procedure Call)인 반면 파라미터의 목적은 노드 내 매개변수를 서비스 데이터 통신 방법을 사용하여 쉽게 지정(set), 변경, 가져오는(get) 것이다.               CLI 명령어   ros2 param           ros2 param list : 파라미터 목록을 출력                   ros2 param describe : 파라미터 내용 확인                   ros2 param get : 파라미터 읽기                   ros2 param set : 파라미터 쓰기                          ros2 param dump : 파라미터 저장       아래와 같이 현재 폴더에 노드 이름으로 .yaml 파일 형태로 저장됨              ’./turtlesim.yaml’ 파일의 내용을 확인하면 다음과 같이 파라미터의 현재 값이 저장되어 있음              이후 노드 실행시 --ros-args --params-file 옵션과 함께 적어주면 노드 실행시 초기 파라미터값을 지정해줄 수 있음                   ros2 param delete : 파라미터 삭제              아래와 같이 삭제되었음을 확인할 수 있다.                      References      “008 ﻿ROS 2 노드와 데이터 통신,” 오로카, 2020년 09월 10일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24086.   “009 ﻿ROS 2 토픽 (topic),” 오로카, 2020년 09월 04일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24101.   “010 ﻿ROS 2 서비스 (service),” 오로카, 2021년 02월 08일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24128.   “011 ﻿ROS 2 액션 (action),” 오로카, 2020년 09월 10일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24142.   “012 ROS 2 토픽/서비스/액션 정리 및 비교,” 오로카, 2021년 03월 17일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24154.   “013 ROS 2 파라미터 (parameter),” 오로카, 2020년 09월 11일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24165.          ","categories": ["robotics-ros"],
        "tags": ["robotics","ROS","ROS2"],
        "url": "/robotics-ros/ros2002/",
        "teaser": null
      },{
        "title": "[LA-00] 선형대수 개요",
        "excerpt":"Linear Algebra   여기서는 선형대수(Linear Algebra)의 대략적인 개념을 살펴본다. 보다 자세한 개념은 이후의 포스팅을 통해 찬찬히 살펴보도록 한다.       Representations   선형대수(linear algebra)는 선형 방정식과 선형 변환에 관한 수학의 한 분야이다.           선형 방정식 (linear equation)   \\[a_1x_1 + ... + a_nx_n = b\\]           선형 변환 (linear map)   \\[(x_1, ..., x_n) \\mapsto a_1x_1 + ... + a_nx_n\\]      이때 선형 방정식과 선형 변환은 벡터 공간(vector space)과 행렬(matrices)을 통해 표현된다.      벡터 (Vector)   행렬 (Matrix)       Vector   \\[\\textbf{a} = (a_{1}, a_{2}, ..., a_{n})\\]  벡터(Vectors)는 숫자들의 배열이다.       Operations   Sum   : $\\textbf{a}_1 + \\textbf{a}_2$       Product                  Scalar product                $k\\textbf{a}$                               Dot product                $\\textbf{a}·\\textbf{b} = \\sum_{i}{a_i b_i}$ (Scalar)                          If $|\\textbf{a}| = 1$, then $\\textbf{a}·\\textbf{b}$ 는 $\\textbf{a}$의 방향을 따라 $\\textbf{b}$를 투영한 것의 길이       If $\\textbf{a}·\\textbf{b}=0$, then  $\\textbf{a}$와 $\\textbf{b}$는 orthogonal               Properties      $\\textbf{b}=\\sum_{i}{k_ia_i}$를 만족하는 ${k_i}$가…            존재 ⇒ Linearly Dependence       존재하지 않음 ⇒ Linearly Independence           n-차원 공간의 한 점으로 표현됨           Matrices   \\[\\textbf{A}^{n×m} =  \\begin{bmatrix} \ta_{11} &amp; a_{12} &amp; ... &amp; a_{1m} \\\\ \ta_{21} &amp; a_{22} &amp; ... &amp; a_{2m} \\\\ \t\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \ta_{n1} &amp; a_{n2} &amp; ... &amp; a_{nm} \\end{bmatrix}\\]      Properties           Matrices &amp; Vectors                       $d$-차원 vector는 $d×1$ matrix로 표현 가능                        Column vectors   \\[\\textbf{a}_{*} = (\\textbf{a}_{*1}, \\textbf{a}_{*2}, ..., \\textbf{a}_{*m})\\]                       Row vectors   \\[\\textbf{a}_* = (\\textbf{a}^T_{1*}; \\textbf{a}^T_{2*}; ...; \\textbf{a}^T_{n*};)\\]                          Operations   행렬의 다양한 연산에 대해 알아보자       Sum   The sum of two vectors   \\[\\textbf{a}+\\textbf{b} = \\begin{bmatrix} a_1\\\\ a_2\\\\ \\vdots\\\\ a_n \\end{bmatrix} + \\begin{bmatrix} b_1\\\\ b_2\\\\ \\vdots\\\\ b_n \\end{bmatrix} = \\begin{bmatrix} a_1+b_1\\\\ a_2+b_2\\\\ \\vdots\\\\ a_n+b_n \\end{bmatrix}\\]      The sum of two matrices   \\[\\displaylines{ {\\textbf{A}+\\textbf{B}} =  {\\begin{bmatrix} \ta_{11} &amp; a_{12} &amp; ... &amp; a_{1m} \\\\ \ta_{21} &amp; a_{22} &amp; ... &amp; a_{2m} \\\\ \t\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \ta_{n1} &amp; a_{n2} &amp; ... &amp; a_{nm} \\end{bmatrix} + \\begin{bmatrix} \tb_{11} &amp; b_{12} &amp; ... &amp; b_{1m} \\\\ \tb_{21} &amp; b_{22} &amp; ... &amp; b_{2m} \\\\ \t\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \tb_{n1} &amp; b_{n2} &amp; ... &amp; b_{nm} \\end{bmatrix}} \\\\ =  {\\begin{bmatrix} \ta_{11}+b_{11} &amp; a_{12}+b_{12} &amp; ... &amp; a_{1m}+b_{1m} \\\\ \ta_{21}+b_{21} &amp; a_{22}+b_{22} &amp; ... &amp; a_{2m}+b_{2m} \\\\ \t\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \ta_{n1}+b_{n1} &amp; a_{n2}+b_{n2} &amp; ... &amp; a_{nm}+b_{nm} \\end{bmatrix}} }\\]          Multiplication   Scalar Multiplication   \\[\\displaylines{ k\\textbf{A} =  \\begin{bmatrix} \tka_{11} &amp; ka_{12} &amp; ... &amp; ka_{1m} \\\\ \tka_{21} &amp; ka_{22} &amp; ... &amp; ka_{2m} \\\\ \t\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \tka_{n1} &amp; ka_{n2} &amp; ... &amp; ka_{nm} \\end{bmatrix} }\\]      Matrix Vector Product   \\[\\displaylines{ \\textbf{A}\\textbf{b} = \\begin{bmatrix} \\textbf{a}^T_{1*} \\\\ \\textbf{a}^T_{2*} \\\\ \\vdots\\\\ \\textbf{a}^T_{n*} \\end{bmatrix} · \\textbf{b}\\\\ = \\begin{bmatrix} \\textbf{a}^T_{1*}·\\textbf{b} \\\\ \\textbf{a}^T_{2*}·\\textbf{b} \\\\ \\vdots\\\\ \\textbf{a}^T_{n*}·\\textbf{b} \\end{bmatrix} \\\\ = \\sum_k{a_{*k}·b_{k}} }\\]     e.g., \\(\\begin{bmatrix} 1&amp;2&amp;2\\\\2&amp;3&amp;1\\\\1&amp;0&amp;2 \\end{bmatrix} \\begin{bmatrix} 1\\\\ 3\\\\ 5 \\end{bmatrix} =\\begin{bmatrix}  \t\\begin{bmatrix} 1&amp;2&amp;2 \\end{bmatrix}     \\begin{bmatrix} 1\\\\ 3\\\\5 \\end{bmatrix} \\\\      \\begin{bmatrix} 2&amp;3&amp;1 \\end{bmatrix} \t\\begin{bmatrix} 1\\\\ 3\\\\5 \\end{bmatrix} \\\\      \\begin{bmatrix} 1&amp;0&amp;2 \\end{bmatrix}     \\begin{bmatrix} 1\\\\ 3\\\\5 \\end{bmatrix} \\\\  \\end{bmatrix} = \\begin{bmatrix} \t1+6+10\\\\ \t2+9+5\\\\ \t1+0+10 \\end{bmatrix} = \\begin{bmatrix} 17\\\\ 16\\\\ 11 \\end{bmatrix}\\)       $\\textbf{A}$의 column vectors가 reference system을 나타낸다면, $\\textbf{A}\\textbf{b}$는 ${a_{*i}}$에 따른 vector $\\textbf{b}$의 global transformation이다.       Matrix Matrix Product   \\[\\displaylines{ \\textbf{C} = \\textbf{A}\\textbf{B} \\\\ = \\begin{bmatrix} \ta_{11} &amp; a_{12} &amp; ... &amp; a_{1l} \\\\ \ta_{21} &amp; a_{22} &amp; ... &amp; a_{2l} \\\\ \t\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \ta_{n1} &amp; a_{n2} &amp; ... &amp; a_{nl} \\end{bmatrix} \\begin{bmatrix} \tb_{11} &amp; b_{12} &amp; ... &amp; b_{1m} \\\\ \tb_{21} &amp; b_{22} &amp; ... &amp; b_{2m} \\\\ \t\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \tb_{l1} &amp; b_{l2} &amp; ... &amp; b_{lm} \\end{bmatrix}\\\\ = \\begin{bmatrix}  \t\\textbf{a}^T_{1*}\\\\ \\textbf{a}^T_{2*}\\\\ \\vdots\\\\ \\textbf{a}^T_{n*} \\end{bmatrix} \\begin{bmatrix} \t\\textbf{b}^T_{*1} &amp; \\textbf{b}^T_{*2} &amp; ... &amp; \\textbf{b}^T_{*m} \\end{bmatrix} \\\\ =  \\begin{bmatrix}  \t\\textbf{A}\\textbf{b}^T_{*1} &amp; \\textbf{A}\\textbf{b}^T_{*2} &amp; ... &amp; \\textbf{A}\\textbf{b}^T_{*m} \\end{bmatrix} }\\]  : row vectors와 column vector의 dot product로 계산 가능하다.           The columns of $\\textbf{C}$ : the transformation of the columns of $\\textbf{B}$ through $\\textbf{A}$.   \\[\\textbf{C} = \\textbf{A}\\textbf{B} =  \\begin{bmatrix} \t\\textbf{A}\\textbf{b}_{*1} &amp; \\textbf{A}\\textbf{b}_{*2} &amp; ... &amp; \\textbf{A}\\textbf{b}_{*m} \\end{bmatrix}\\]  \\[\\textbf{c}_{*i} = \\textbf{A}\\textbf{b}_{*i}\\]              Rank   랭크(Rank) 는 선형 독립인 행(또는 열)의 최대 개수 혹은 dimension of the image of the transformation $f(\\textbf{x}) = \\textbf{A}\\textbf{x}$을 의미한다.      e.g., $rank(\\textbf{A}) = 2$ when …   \\[A={\\begin{bmatrix}1&amp;2&amp;0&amp;1\\\\0&amp;0&amp;1&amp;1\\\\0&amp;0&amp;0&amp;0\\\\0&amp;0&amp;0&amp;0\\\\\\end{bmatrix}}\\]      Properties            $rank(\\textbf{A})≥0$   ⇔   $\\textbf{A}^{m×n}$ : null matrix       $rank(\\textbf{A})≤ \\min(m,n)$           Computations            Gaussian elimination on the matrix       Counting the number of non-zero rows                   Inverse   \\[\\textbf{A}\\textbf{B} = \\textbf{I}\\\\ \\textbf{B}=\\textbf{A}^{-1} (\\text{inverse matrix of $\\textbf{A}$)}\\]          조건              $\\textbf{A}$ : a square matrix of full rank                특징              역행렬이 존재한다면, 그 역행렬은 유일(unique)하다.       $\\textbf{A}\\textbf{A}^{-1} = \\textbf{I} = \\textbf{A}^{-1}\\textbf{A}$       $(\\textbf{A}\\textbf{B})^{-1} = \\textbf{B}^{-1}\\textbf{A}^{-1}$                $(\\textbf{A}+\\textbf{B})^{-1} ≠ \\textbf{A}^{-1} + \\textbf{B}^{-1}$                        $\\textbf{A}$의 $i$th row와 $\\textbf{A}^{-1}$의 $j$th column은..   \\[\\begin{cases} \t\\text{orthogonal} &amp; \\text{if $i≠j$} \\\\ \t\\text{their dot product is $1$} &amp; \\text{if $i=j$} \\end{cases}\\]                       응용              선형방정식의 해 구하기 : $\\textbf{x} = \\textbf{A}^{-1}\\textbf{b}$                   Determinant   \\[\\displaylines{ \\det(\\textbf{A}) = a_{11}\\textbf{C}_{11} + a_{12}\\textbf{C}_{12} + ... + a_{1n}\\textbf{C}_{1n} \\\\ = \\sum_{j=1}^n{a_{1j}\\textbf{C}_{1j}} }\\]          계산              Cofactor expansion       Gauss elimination                특징       $\\textbf{A}^{n×n}$이라고 할 때 …              $\\det(\\textbf{A})≠0$ ⇔ $\\textbf{A}$의 역행렬이 존재       Row operations                    $\\textbf{B}$ : $\\textbf{A}$의 두 행을 interchanging한 결과라면 ⇒ $\\det(\\textbf{B}) = -\\det(\\textbf{A})$           $\\textbf{B}$ : $\\textbf{A}$의 한 행에 숫자 $c$를 곱한 결과하면 ⇒ $\\det(\\textbf{B}) = c·\\det(\\textbf{A})$           $\\textbf{B}$ : $\\textbf{A}$의 한 행의 배수를 다른 행에 더한 결과라면 ⇒ $\\det(\\textbf{B}) = \\det(\\textbf{A})$                       $\\det(\\textbf{A}^T) = \\det(\\textbf{A})$       $\\det(\\textbf{A}^{-1}) = \\frac{1}{\\det(\\textbf{A})}$       $\\det(\\textbf{P}\\textbf{A}\\textbf{P}^{-1}) = \\det(\\textbf{A})$       $\\det(c\\textbf{A}) = c^n \\det(\\textbf{A})$        \\[\\det(\\text{diag}(a_{11},...,a_{nn}))      = \\begin{vmatrix}          a_{11}&amp;*&amp;0\\\\         *&amp;\\ddots&amp;*\\\\         0&amp;*&amp;a_{nn}     \\end{vmatrix} = a_{11}a_{22}···a_{nn}\\]               \\[\\begin{vmatrix}         a_{11}&amp;&amp;0\\\\         \\vdots&amp;\\ddots&amp;&amp;\\\\         a_{n1}&amp;...&amp;a_{nn}     \\end{vmatrix}     =a_{11}a_{22}···a_{nn}\\]              $\\det(\\textbf{A}·\\textbf{B}) = \\det(\\textbf{A}) · \\det(\\textbf{B})$       $\\det(\\textbf{A}+\\textbf{B}) ≠ \\det(\\textbf{A}) + \\det(\\textbf{B})$                        응용              기하적 분석                    역행렬의 존재 판별 : $\\det(\\textbf{A})≠0$ ⇒ 역행렬이 존재           선형변환의 스케일 성분 : $\\text{area, volume, …} = \\lvert \\det(\\textbf{A}) \\rvert $           도형의 방향 보존 : $\\det(\\textbf{A})&gt;0$ ⇒ 방향이 보존                       Eigenvalues 계산 : $D(\\lambda) = \\det(\\textbf{A}-\\lambda \\textbf{I})=0$                   특수행렬   Orthogonal Matrix   모든 column(or row) vectors가 직교(orthonormal)하는 $\\textbf{Q}$를 직교 행렬(orthogonal)이라고 한다.   \\[q^T_{*i}·q_{*j} =  \\begin{cases} \t1 &amp; \\text{if $i=j$}\\\\ \t0 &amp; \\text{if $i≠j$} \\end{cases},  ∀i,j\\]  ※ i.e., 본인과 같은 column(row) vector와의 내적은 1이고, 그 외는 0 ※           특징                       선형 변환으로서 norm은 보존된다. (norm = 1)                        $\\textbf{Q}\\textbf{Q}^T = \\textbf{Q}^T\\textbf{Q} = \\textbf{I}$                        행렬식은 unity norm(±1)을 갖는다 :   \\[1 = \\det(\\textbf{I}) = \\det(\\textbf{Q}^T\\textbf{Q})=\\det(\\textbf{Q})\\det(\\textbf{Q}^T)=\\det(\\textbf{Q})^2\\]                      Symmetric Matrix   대칭행렬(symmetric matrix)은 정방행렬에서 대각선에 대칭인 두 원소가 같은 행렬을 의미한다.      e.g., 대칭행렬의 예시   \\[\\begin{bmatrix} \t2&amp;1\\\\ 1&amp;5 \\end{bmatrix}, \\begin{bmatrix} \t3&amp;6&amp;1\\\\ 6&amp;2&amp;2\\\\ 1&amp;2&amp;4 \\end{bmatrix}\\]      특징            실수인 고유값을 갖는다.               Positive Definite Matrix   양의 정부호 행렬(Positive Definite Matrix) 은 모든 고유값(eigenvalues)이 양수인 대칭행렬(symmetric matrix)이다. (i.e., 대칭 행렬의 특수한 케이스)   \\[\\textbf{M} &gt; 0 \\\\  \\text{iff } \\textbf{z}^T\\textbf{M}\\textbf{z}&gt;0, ∀z≠0\\\\\\]          예시                       양의 정부호행렬의 예시1   \\[\\textbf{M}_1 = \\begin{bmatrix} 1&amp;0\\\\0&amp;1\\end{bmatrix} \\text{일 때, }\\\\ \\begin{bmatrix}z_1 &amp; z_2\\end{bmatrix} \\begin{bmatrix}1&amp;0\\\\0&amp;1\\end{bmatrix} \\begin{bmatrix}z_1\\\\z_2\\end{bmatrix} = z_1^2+z_2^2 &gt; 0\\]                       양의 정부호행렬의 예시2   \\[\\textbf{M}_2 = \\begin{bmatrix}2&amp;-1&amp;0\\\\-1&amp;2&amp;-1\\\\0&amp;-1&amp;2\\end{bmatrix} \\text{일 때,}\\\\ \\begin{bmatrix}z_1 &amp; z_2 &amp; z_3\\end{bmatrix} \\begin{bmatrix}2&amp;-1&amp;0\\\\-1&amp;2&amp;-1\\\\0&amp;-1&amp;2\\end{bmatrix} \\begin{bmatrix}z_1\\\\z_2\\\\z_3\\end{bmatrix} \\\\ =  2z_1^2-2z_1z_2 + 2z_2^2 - 2z_2z_3 + 2z_3^2\\\\ = z_1^2 + (z_1-z_2)^2 + (z_2-z_3)^2 + z_3^2 &gt;0\\]                       특징                       Invertible, with positive definite inverse                        모든 실수 eigenvalues &gt; 0                        Trace is &gt; 0                      대각합(Trace)은 정사각행렬의 주대각성 성분의 합을 의미한다.                                 Cholesky decomposition $\\textbf{A} = \\textbf{L}\\textbf{L}^T$           ※ $\\textbf{L}$ : 하삼각행렬                      숄레스키 분해(Cholesky decomposition)는 양의 정부호행렬(positive-definite matrix)의 분해에서 사용된다.   \\[\\textbf{A} = \\textbf{L}\\textbf{L}^*\\]            이때 $\\textbf{A}$의 모든 성분이 실수이면, $\\textbf{L}$의 모든 성분도 실수이므로 다음과 같이 분해된다.   \\[$\\textbf{A}=\\textbf{L}\\textbf{L}^T\\]                               Rotation Matrix   회전변환행렬(rotation matrix)은 $\\det=±1$을 만족하는 직교행렬(orthonormal matrix)이다.           예시                       2D rotations   \\[R(\\theta) =  \\begin{bmatrix} \t\\cos(\\theta) &amp; -\\sin(\\theta)\\\\ \t\\sin(\\theta) &amp; \\cos(\\theta) \\end{bmatrix}\\]                       3D rotations along the main axes   \\[R_x(\\theta) =  \\begin{bmatrix} \t1&amp;0&amp;0\\\\ \t0&amp;\\cos(\\theta)&amp;-\\sin(\\theta)\\\\ \t0&amp;\\sin(\\theta)&amp;\\cos(\\theta) \\end{bmatrix}\\]  \\[R_y(\\theta) =  \\begin{bmatrix} \t\\cos(\\theta)&amp;0&amp;-\\sin(\\theta)\\\\ \t0&amp;1&amp;0\\\\ \t\\sin(\\theta)&amp;0&amp;\\cos(\\theta) \\end{bmatrix}\\]                       특징                       Not commutative                      e.g., \\(R_x(\\frac{\\pi}{4})·R_y(\\frac{\\pi}{4}) = ... = \\begin{bmatrix}-1.414\\\\ 0.586\\\\ 3.414\\end{bmatrix}\\\\ ≠ R_y(\\frac{\\pi}{4})·R_x(\\frac{\\pi}{4}) = ... = \\begin{bmatrix}-1.793\\\\ 0.707\\\\ 3.207\\end{bmatrix}\\)                                    Affine Transformations      아핀 변환(Affine Transformation)이란, 기하학적 성질을 보존하는 두 아핀 공간 사이의 함수이다.     ※ 점, 직선, 평면이 보존되는 선형 매핑 ※    3차원 변환을 묘사하는 가장 쉽고 대중적인 방법은 행렬을 이용하는 것이다.   \\[\\textbf{A} = \\begin{bmatrix}\\textbf{R}&amp;\\textbf{t}\\\\\\textbf{0}&amp;\\textbf{1}\\end{bmatrix} \\textbf{A}^{-1} = \\begin{bmatrix} \t\\textbf{R}^T &amp; -\\textbf{R}^T\\textbf{t} \\\\ \t\\textbf{0} &amp; \\textbf{1} \\end{bmatrix} \\textbf{p} = \\begin{bmatrix}\\textbf{t}\\\\\\textbf{1}\\end{bmatrix}\\]     $\\textbf{R}$ : Rotation matrix   $\\textbf{t}$ : Translation vector       Combining Transformations   변환 행렬(transformation matrices)를 여러 개 결합하여(chaining) 간단하게 하나의 변환 행렬로 표현할 수 있다. 예를 들어 다음과 같이 로봇과 센서, 객체에 대한 행렬이 주어졌다고 가정하자.              $\\textbf{A}$ : the pose of a robot in the space            $\\textbf{B}$ : the position of a sensor on the robot            The sensor perceives an object at a given location $\\textbf{p}$, in its own frame       ※ The sensor has no clue on where it is in the world ※       이때 global frame에서 object의 위치는 다음의 과정으로 구할 수 있다.      $\\textbf{B}\\textbf{p}$ : the pose of the object wrt the robot   $\\textbf{A}\\textbf{B}\\textbf{p}$ : the pose of the object wrt the world           References      01.Linear Algebra, Introduction to Mobile Robotics - SS 2021, http://ais.informatik.uni-freiburg.de/teaching/ss21/robotics/.   “[선형대수학] 양의 정부호 행렬(positive definite matrix)이란?.txt”, bskyvision, 2017년 11월 22일 수정, 2022년 01월 14일 접속, https://bskyvision.com/205   “[기계학습] Positive-Definite Matrix 란?”, 자연의 원리에 귀를 기울이다 네이버 블록, 2017년 12월 7일 수정, 2022년 01월 14일 접속, https://m.blog.naver.com/sw4r/221157302215           ","categories": ["mathematics-linearAlgebra"],
        "tags": ["Mathematics","Linear Algebra"],
        "url": "/mathematics-linearalgebra/00_Overall_Linear_Algebra/",
        "teaser": null
      },{
        "title": "[I2MR-01] 01. Robot Control Paradigms",
        "excerpt":"로봇의 제어 패러다임의 종류에 대해 간략하게 살펴본다.       Robot Control Paradigms   로봇 패러다임(robot paradigms)은 로봇이 작동하는 방식에 대한 정신적 모델을 의미한다. 여기서는 대표적인 로봇의 제어 패러다임 몇 가지에 대해 간략하게 살펴본다.           고전/계층 패러다임(Classical/Hierarchical Paradigm)              [Sense → Plan → Act → Sense … ]       센싱이 플랜을 거쳐 액션으로 매핑되는 패러다임                반응적/행동-기반 패러다임(Reactive/Behavior-based Paradigm)                       Plan을 behavior로 대체           ⇒ [Sense ↔ Act]                        플랜의 과정이 사라지고, 센싱이 직접적으로 액션에 영향을 주는 패러다임                        하이브리드 숙고/반응적 패러다임(Hybrid Deliberative/Reactive Paradigm)              앞선 패러다임의 장점들을 결합한 패러다임       [Sense ↔ Act]에 Plan이 영향을 줌                   Classical/Hierarchical Paradigm         70년대, 로봇공학 초기에 사용되던 모델   계획(planning)에 중점을 둔 패러다임으로, 세계를 센싱하고 다음 액션을 계획한 뒤 행동한다.   센싱 데이터는 한 개의 글로벌 월드 모델로 모인다.   월드 모델은 프레임 문제(*frame problem*)와 폐쇄 월드 가정(*closed world assumption*)으로 인해 유지 관리가 어렵다는 한계가 있다.       예시              STRIPS (Stanford Research Institute Problem Solver)              박스들을 찾아서 디자인된 위치로 옮기는 작업을 수행       Perfect world model : 완벽한 세계를 가정 (취득된 정보를 100% 신뢰가능)       Closed world assumption                Stanford CART (1973), Stanford AI Laboratory/CMU(Moravec)                       classifcal pradigm of stanford CART                                      Reactive/Behavior-based Paradigm   반응적/행동-기반 패러다임(Reactive/Behavior-based Paradigm)은 어떠한 모델 없이 감지한 정보가 곧장 행동으로 반영되는 패러다임이다.              Planning을 극단적으로 버려버린 패러다임이다.            센싱된 정보들은 직접적으로 액션에 결합되며, 이는 각각 행동(behavior)이라 불린다.            이러한 Behavior 기반 접근은 planning의 몇 가지 레벨을 제공한다.            지능의 살아있는 예시들을 조사하는 생물학 및 인지 심리학에서 비롯되어, 로봇의 저비용 및 컴퓨팅 성능을 향상시켜준다.              내 생각 :         생물학적 시스템을 모방한 뉴로모픽 비전(Neuromorphic Vision) 역시 에너지 효율적으로 동작한다. 자연적으로 존재하는 구조가 가장 에너지 효율적일 수 있겠다(그야말로 자연적인 것이니깐).         참고1. [NeuromorphicVision-001] 뉴로모픽 비전(Neuromorphic Vision)의 배경 및 개념, Dazory Blog  참고2. [NeuromorphicVision-002] 뉴로모픽 비전(Neuromorphic Vision)의 현황(1), Dazory Blog                 Behavior(e.g., exploer, wander, avoid obstacles, …) 기반 접근은 다음과 같이 몇 가지 planning 레벨을 제공한다.                  특징   반응적 패러다임의 특징은 다음과 같다.      에이전트가 위치한 로봇은 위치한 환경의 필수적인 부분이다.   behavior은 로봇의 actions에 대한 기초적인 빌딩 블록 역할을 한다.            로봇의 활동(action)은 시퀀스 혹은 동시적으로 동작하는 behaviors의 결과로서 나타난다.           모듈화 가능한 behavior의 특성은 좋은 소프트웨어 디자인 원리를 제공한다.       예시   반응적/행동-기반 패러다임을 구현한 대표적인 예시로 아래의 구조를 살펴본다.      포함 아키텍처(Subsumption Architecture)   포텐셜 필드 방법론(Potential Fields Methodologies)       Subsumption Architecture      MIT의 Genghis는 subsumption 아키텍처로 개발되었다.           Rodney Brooks에 의해 86년 소개되었다.            “Behaviors are networks of sensing and acting modules.”                       AFSM(Augmented Finite State Machines)                      유한 상태 기계(Finite State Machine)는 컴퓨터 프로그램과 전자 논리 회로를 설계하는 데에 쓰이는 수학적 모델로, 유한한 개수의 상태를 가질 수 있는 오토마타, 즉 추상기계라고 할 수 있다. 이러한 기계는 한 번에 오로지 하나의 상태만을 가지게 되며, 현재 상태(Current State)란 임의의 주어진 시간의 상태를 칭한다.특정한 유한 오토마톤은 현재 상태로부터 가능한 전이 상태와, 이러한 전이를 유발하는 조건들의 집합으로서 정의된다. (위키백과)                                 내부적인 상태(internal state)가 존재하지 않는다.                       포함 아키텍처(subsumption architecture)는 역량 계층들(layers of competence)로 그룹화되며, 각 레이어들은 낮은 레벨의 레이어들을 포함하는(subsume) 구조로 되어있다.           Level 0 : Avoid(회피)                   Level 1 : Wander(배회)                   Level 2 : Follow Corridor(복도를 따라감)                  Subsumption architecture의 장점과 단점은 다음과 같다.      장점            타겟 도메인에서의 실시간 시스템의 반복적인 개발 및 테스트에 중점을 둔다.       제한된 작업별 인식(perception)을, 그것을 필요로하는 표현된 액션에 직접적으로 연결하는 것에 중점을 둔다.       분배 및 병렬 제어에 중점을 두어, 인식(perception)과 제어(control) 그리고 액션 시스템을 동물과 유사한 방식으로 통합한다.           단점            고도로 분산된 방해 및 억제 시스템을 통해 적응 가능한 액션 선택을 설계하는 것이 어렵다.       대용량 메모리 및 상징적 표현이 부족하다. (아키텍처가 언어를 이해하는 것을 제한)                   Potential Field Methods      GATech의 Callisto는 potential field 방법으로 개발되었다.           로봇을 마치 포텐셜 필드의 영향 하에 있는 입자(*particle*)처럼 다룬다.            로봇은 포텐셜의 미분을 따라 운행한다.            필드는 장애물, 원하는 운행 방향 및 타겟에 의해 형성된다.            결과 필드(vector)는 기본 필드(primitive fields)의 합으로 제공된다.              기본 포텐셜 필드(Primitive Potential Fields)의 예시                          필드의 세기는 장애물/타겟까지의 거리에 따라 변화한다.           로봇은 포텐셜 필드를 이용하여 복도를 따라갈 수 있다.           Level 0 : Collision Avoidance       레벨 0에서는 감지된 장애물의 반발 필드(repulsive fields)에 의해 충돌 회피(collision avoidance)가 수행된다.            Level 1 : Wander       레벨 1은 단일 필드(uniform field)를 추가한다.            Level 2 : Corridor following       레벨 2에서는 세 개의 필드들(수직(perpendicular) 2 + 단일(uniform) 1)에 의해 배회 필드(wander field)가 대체되어, 복도를 따라 가기 시작한다.           포텐셜 필드의 특징은 다음과 같다.           로컬 미니마(local minima, 극소값)에 빠질 수 있다.              이에 대한 해결책으로 다음의 것들이 존재한다.              역추적(Backtracking)       로컬 미니멈에서 탈출하기 위한 랜덤 모션       벽을 따라가기 위한 절차 계획자(procedural planner)       방문한 지역들의 포텐셜을 증가       고조파 기능(harmonic funcitons)을 통해 로컬 미니마를 회피                레이어 간 선호도가 존재하지 않는다.            시각화하기 용이하다.            다른 필드들과 합치기에 용이하다.            높은 업데이트 속도를 필요로 한다.            파라미터 튜닝이 중요하다.               Hybrid Deliberative/Reactive Paradigm         고전/계층적 패러다임(Classical/Hierarchical Paradigm)과 반응적/행동-기반 패러다임(Reactive/Behavior-based Paradigm)의 장점을 결합한 패러다임이다.            계획(planning)에 사용되는 월드 모델       폐쇄 루프, 반응적 컨트롤           1990년부터 현재(2017)까지 주를 이루는 패러다임이다.           References      “02. Robot Control Paradigms”, Introduction to Mobile Robotics - SS 2021, http://ais.informatik.uni-freiburg.de/teaching/ss21/robotics/.   “Subsumption architecture”, WIKIPEDIA, 2021년 11월 20일 수정, 2022년 01월 18일 접속, https://en.wikipedia.org/wiki/Subsumption_architecture](https://en.wikipedia.org/wiki/Subsumption_architecture).   “Hierarchical Control”, CS521: Robotics Winter 2017, Dr.Fang Tang, https://www.cpp.edu/~ftang/courses/CS521/notes/hierarchical%20control.pdf   “Reactive Architecture”, CS521: Robotics Winter 2017, Dr.Fang Tang, https://www.cpp.edu/~ftang/courses/CS521/notes/reactive%20architecture.pdf          ","categories": ["robotics-mobileRobotics"],
        "tags": ["robotics","mobile robotics","control","paradigm"],
        "url": "/robotics-mobilerobotics/I2MR01/",
        "teaser": null
      },{
        "title": "[DL-03] Lect4. Neural Networks (part2)",
        "excerpt":"AIAS-Lect4_Neural Networks (part2)   Data Preprocessing   일반적인 경우 우리가 가진 original data를 바로 학습에 사용하면 성능이 기대에 다소 못 미칠 수 있다. original data가 weight matrix에 너무 민감하여 optimize하기 어려운 경우가 그에 해당한다. 따라서 이런 경우 적절한 data preprocessing(전처리)를 통해 학습에 사용하기 좋은 형태로 변환해주어야 한다.   Normalization   아래 그림은 데이터가 편향되거나 너무 분산된 경우 zero-centering과 normalization을 통해 preprocessing하는 것을 보여준다.      zero-centering이란 data의 평균값을 0으로 맞춰주는 과정이다. 앞서 sigmoid의 단점으로 bias shift가 있다고 언급한 바 있다. 데이터에서도 마찬가지로 전체적으로 어느 방향으로 치우친 데이터보단 평균이 0인 데이터가 좋다는 입장에서 zero-centering을 한다.   normalization은 data feature의 스케일을 동일한 정도로 맞춰주는 과정이다. unnormalized data의 경우, 최적화하는 과정에서 큰 폭으로 단계를 거쳐 최적값에 도달한다. 반면 normalized data의 경우는 시작점과 무관하게 일정한 폭으로 최적값이 도달할 수 있다. 이러한 특징은 학습을 보다 빠르게 만들어준다.      normalization은 feature의 scale이 동일할 때 학습이 더 잘 될 것이라는 가정 하에 사용해야 한다. data 분석 없이 마구잡이로 사용하는 것이 아니다.    PCA and Whitening   아래 그림은 데이터가 편향되거나 너무 분산된 경우 decorrelation과 whitening을 통해 preprocessing하는 것을 보여준다.      decorrelation은   Whitening은 eigenbasis data(기저벡터)를 eigenvalue(고유값)로 나누어 정규화 하는 방법이다.      일반적으로 이미지 데이터에 대해서는 PCA나 whitening을 잘 사용하지 않는다.    Weight Initialization   이전에 배웠듯 머신러닝 학습 과정의 큰 틀은 다음과 같다.   feed-forward → loss function → backpropagation(optimization) → update model parameters → feed-forward → …   그렇다면 학습 가장 초반에 model parameters는 어떻게 초기화하는 것이 좋을까?   Weights and Activation function   학습 초기 initialization 과정은 매우 중요하다. model parameters가 어떻게 initialization되었는가에 따라 결과가 달라지기 때문이다. 또한 weigths와 activation function은 연관성이 깊은데, network가 deep해질 수록 activation functions에 의해 model parameters(weights)가 전혀 update가 안될 수도 있기 때문이다. 아래에서 사례를 통해 연관성을 살펴보자.   Small random numbers with tanh   \\[W-N(0,(1e-2)^2)\\]  위와 같이 평균이 0, 표준편차가 1e-2인 가우시안 분포를 따르는 random변수로 초기화 할 수 있다.   W=0.01 * np.random.randn(Din, Dout)   이런 경우 small networks에는 나쁘지 않게 작동하지만 network가 깊어질 수록 문제가 발생한다. 가령 activationo functions으로 tanh()함수를 사용한 아래의 경우를 살펴보자.   # Forward pass for a 6-layer net with hidden size 4096 dims = [4096] * 7 hs = [] x = np.random.randn(16, dims[0]) cnt = 1  for Din, Dout in zip(dims[:-1], dims[1:]):     W = 0.01 * np.random.randn(Din, Dout)     x = np.tanh(x.dot(W))     hs.append(x)     print('[',cnt,'layers]  mean:%.2f'%np.mean(x), 'std:%.2f'%np.std(x))     cnt=cnt+1 \"\"\" [ 1 layers]  mean:-0.00 std:0.49 [ 2 layers]  mean:0.00 std:0.29 [ 3 layers]  mean:-0.00 std:0.18 [ 4 layers]  mean:0.00 std:0.11 [ 5 layers]  mean:-0.00 std:0.07 [ 6 layers]  mean:-0.00 std:0.05 \"\"\"       위 코드는에서 W값은 mean=0, std=1e-2인 가우시안 분포를 따르는 랜덤변수이다.  각 tanh activation function을 통과한 결과에 대하여 평균과 분산을 출력한 결과는 위와 같다. 이를 통해 layer를 통과할 수록 분산이 매우 작아져 결국 출력값의 대부분이 0으로 수렴한다는 것을 알 수 있다. 출력값이 0에 수렴하는 이유는 tanh activation function의 꼴을 보면 이해할 수 있다.      tanh()함수는 입력의 절댓값이 작은 경우 출력값이 0에 가까워지는 특징을 가졌다. 이러한 함수 특징에 의해 layer를 통과할 수록 출력값이 0에 수렴하는 것이다. layer의 출력값이 0인 경우 학습이 전혀 되지 않는다. 그 이유는 chain rule에 의해 쉽게 설명 가능하다. \\(\\frac{∂L}{∂w_i^{l+1}}=\\frac{∂L}{∂f_{l+1}}*\\frac{∂f_{l+1}}{∂w_i^{l+1}}=\\frac{∂L}{∂f_{l+1}}*x_i^{l+1}≈0\\\\ (∵\\ f_{l+1}(x_i^{l+1},w_i^{l+1})=W^{l+1}*X^{l+1}\\ )\\) downstream gradient는 upstream gradient와 local gradient의 곱으로 계산할 수 있다. 이때 layers를 통과할 수록 x값이 0에 수렴하므로 결국 downstream gradient도 0으로 수렴하여 backpropagation이 전혀 진행되지 않음을 상상할 수 있다.   ### Increase std for initial weights from 0.01 to 0.05   weights의 초기값이 너무 작은 경우 위와 같이 deep network에서 layer를 통과할 수록 출력값이 0에 수렴해 backpropagation이 전혀 진행되지 않았음을 확인했다. 그렇다면 weights 표준편차의 초기값을 0.01에서 0.05로 늘려보자.   # Forward pass for a 6-layer net with hidden size 4096 dims = [4096] * 7 hs = [] x = np.random.randn(16, dims[0]) cnt = 1  for Din, Dout in zip(dims[:-1], dims[1:]):     W = 0.05 * np.random.randn(Din, Dout) #★     x = np.tanh(x.dot(W))     hs.append(x)     print('[',cnt,'layers]  mean:%.2f'%np.mean(x), 'std:%.2f'%np.std(x))     cnt=cnt+1 \"\"\" [ 1 layers]  mean:0.00 std:0.87 [ 2 layers]  mean:0.00 std:0.85 [ 3 layers]  mean:-0.00 std:0.85 [ 4 layers]  mean:-0.00 std:0.85 [ 5 layers]  mean:-0.01 std:0.85 [ 6 layers]  mean:-0.00 std:0.85 \"\"\"      위 코드는에서 W값은 mean=0, std=0.05인 가우시안 분포를 따르는 랜덤변수이다.  각 tanh activation function을 통과한 결과에 대하여 평균과 분산을 출력한 결과는 위와 같다. 한편, 이번에는 출력값의 대부분이 -1또는 1인것을 histogram을 통해 확인할 수 있다. 이 또한 tanh activation function의 특징을 통해 설명 가능하다.      tanh()함수는 입력의 절댓값이 큰 경우 출력값이 -1 또는 1에 가까워지는 특징을 가졌다. 이러한 함수 특징에 의해 layer를 통과할 수록 출력값이 -1 또는 1에 수렴하는 것이다. 이러한 경우를 saturated activation function이라고 하는데, 이 경우 또한 학습이 전혀 되지 않는다. 그 이유는 chain rule에 의해 쉽게 설명 가능하다. \\(\\frac{∂L}{∂w_i}=\\frac{∂L}{∂f}*\\frac{∂f}{∂w_i}=\\frac{∂L}{∂f}*\\frac{∂f}{∂s}*\\frac{∂s}{∂w_i}≈0\\\\ (∵\\ s=\\sum_i{w_ix_i}+b,\\ f:activation\\ function일\\ 때,\\ \\frac{∂f}{∂s}≈0\\ )\\) s를 loss function, f를 activation function이라고 하자. 출력이 -1또는 1인 경우의 activation function (tanh)의 미분값은 위 tanh그래프를 통해 0에 수렴함을 알 수 있다. 따라서 local gradient이 0으로 수렴하므로 downstream gradient도 0으로 수렴하여 backpropagation이 전혀 진행되지 않음을 상상할 수 있다.   Xavier (with tanh) Initialization   위의 사례를 통해 activation function이 tanh인 경우, weights가 너무 작거나 크면 backpropagation이 안되는 문제를 확인했다. 출력값의 분포를 적당하게 맞춰주는 작업이 필요해졌고, 그에 따라 Xavier initialization 방법이 제안되었다.   # Forward pass for a 6-layer net with hidden size 4096 dims = [4096] * 7 hs = [] x = np.random.randn(16, dims[0]) cnt = 1  for Din, Dout in zip(dims[:-1], dims[1:]):     W = np.random.randn(Din, Dout) / np.sqrt(Din) #★     x = np.tanh(x.dot(W))     hs.append(x)     print('[',cnt,'layers]  mean:%.2f'%np.mean(x), 'std:%.2f'%np.std(x))     cnt=cnt+1 \"\"\" [ 1 layers]  mean:0.00 std:0.63 [ 2 layers]  mean:0.00 std:0.49 [ 3 layers]  mean:-0.00 std:0.41 [ 4 layers]  mean:-0.00 std:0.36 [ 5 layers]  mean:0.00 std:0.32 [ 6 layers]  mean:0.00 std:0.29 \"\"\"      위 코드는에서 W값은 mean=0, std=1/sqrt(Din)인 가우시안 분포를 따르는 랜덤변수이다. 이때 입력의 루트값으로 나눠준 값을 표준편차로 정의하는 경우, 위와 같이 출력의 분산이 -1과 1 사이에 골고루 퍼지는 것을 확인할 수 있다. 이 원리에 대해서는 학부생의 수준을 넘어서므로 여기서는 논의하지 않는다. 다만 어떠한 작업(1/sqrt(Din))을 통해 출력의 분산을 적절히 scaling해주었다는 정도만 알면 된다. 이 경우 모든 layers에 대해서 적절한 입력이 주어지므로 학습이 잘 될 것임을 기대할 수 있다.   Xavier (with ReLU) Initialization   이번에는 Xavier initialization에 activation function으로 ReLU함수를 사용해보자.   # Forward pass for a 6-layer net with hidden size 4096 dims = [4096] * 7 hs = [] x = np.random.randn(16, dims[0]) cnt = 1  for Din, Dout in zip(dims[:-1], dims[1:]):     W = np.random.randn(Din, Dout) / np.sqrt(Din)     x = np.maximum(0, x.dot(W)) #★     hs.append(x)     print('[',cnt,'layers]  mean:%.2f'%np.mean(x), 'std:%.2f'%np.std(x))     cnt=cnt+1 \"\"\" [ 1 layers]  mean:0.40 std:0.58 [ 2 layers]  mean:0.28 std:0.41 [ 3 layers]  mean:0.19 std:0.29 [ 4 layers]  mean:0.14 std:0.20 [ 5 layers]  mean:0.10 std:0.14 [ 6 layers]  mean:0.07 std:0.10 \"\"\"      activation function을 ReLU함수로 바꾸니 위와 같이 출력이 다시 0으로 수렴하는 현상이 발생했다. 이러한 결과는 음수 입력값에 대해 항상 0을 출력하는 ReLU함수의 특징 때문이다.   Kaiming/MSRA (with ReLU) Initialization   위의 예시를 통해 Xavier initialization에 ReLU함수를 사용하면 출력이 0으로 수렴하는 문제가 발생함을 확인했다. 다음과 같이 weights 부분을 고침으로써 이러한 문제를 해결할 수 있다.   # Forward pass for a 6-layer net with hidden size 4096 dims = [4096] * 7 hs = [] x = np.random.randn(16, dims[0]) cnt = 1  for Din, Dout in zip(dims[:-1], dims[1:]):     W = np.random.randn(Din, Dout) / np.sqrt(Din/2) #★     x = np.maximum(0, x.dot(W))     hs.append(x)     print('[',cnt,'layers]  mean:%.2f'%np.mean(x), 'std:%.2f'%np.std(x))     cnt=cnt+1 \"\"\" [ 1 layers]  mean:0.56 std:0.83 [ 2 layers]  mean:0.58 std:0.83 [ 3 layers]  mean:0.58 std:0.83 [ 4 layers]  mean:0.56 std:0.83 [ 5 layers]  mean:0.55 std:0.80 [ 6 layers]  mean:0.55 std:0.80 \"\"\"      ReLU함수는 음수입력값이 0으로 출력된다는 문제로 인해, 입력의 절반 정도가 사라진다는 이유로 학습이 제대로 이루어지지 않았다. 따라서 Xavier initialization에서의 아이디어에 2로 나누어 입력의 절반이 없어진다는 점을 반영한다. 그 결과 위와 같이 좋은 분포를 출력함을 확인할 수 있다.   Stochastic Gradient Descent (SGD)   Gradient Steps   모델은 gradient를 통해 loss가 줄어드는 방향으로 model parameters가 업데이트 되며 학습이 이루어진다. 이때 model parameters는 다음과 같이 업데이트된다. \\(x'=x-α▽_xf(x)\\\\ α: learning\\ rate\\\\ ▽_x: gradient\\) 이때 learning rate(α)가 작으면 아래 그림에서 화살표 방향(gradient)으로 조금 이동하고, learning rate가 큰 경우, 화살표 방향으로 크게 이동한다.      가장 최적의 point를 global optimum이라 하고, 일부 영역에서만 놓고 봤을 때 최적의 point는 local mimum이라고 한다. 이때 learning rate가 너무 작은 경우, global minimum에 도달하는데 까지 걸리는 오랜 시간이 소요될 수 있다. 반면 learning rate가 너무 작은 경우, 섬세한 이동을 하지 못해 global minimum를 정확히 찾지 못할 수 있다. 이러한 learning rate는 아래 그림과 같이 gradient descent의 convergence(수렴) 여부에 영향을 미친다.      위 그림은 big learning rate인 경우 발산할 위험성이 있음을 보여준다. 따라서 적절한 learning rate를 결정하는 것이 학습에 있어서 매우 중요하다.   SGD 탄생 배경   Single Training Sample의 경우   다음과 같이 주어진 조건에서 최적의 model parameters θ={W,b}을 찾아보자.      single training sample에 대해서 학습하는 방법은 아래와 같다. \\(θ^{k+1}=θ^k-α▽_θL_i(θ^k,x_i,y_i)\\) 즉, 현재 parameters θ에서 gradient(▽)와 Loss function(L)의 곱을 learning rate(α)만큼 빼주면 새로운 parameters가 된다.   Multiple Training Samples의 경우   이번에는 single training sample이 아닌 n개의 multiple training samples {xi, yi}에 대해 최적의 model parameters를 구한다고 생각해보자. 이때의 cost는 아래와 같이 계산할 수 있다. \\(Cost\\ L=\\frac{1}{n}\\sum_{i=1}^nL_i(θ, x_i, y_i)\\) n개의 single training sample 각각에 대한 loss의 평균을 구하면 multiple training samples에 대한 cost를 구할 수 있다.   Too Many Training Samples의 경우   그렇다면 아주 training samples가 너무 많은 경우는 어떻게 될까? 모든 training samples 각각에 대한 loss를 계산한 뒤 평균을 내는 방법은 연산량이 너무 많아 비효율적이다. 이러한 배경에서 stochastic gradient descent(SGD)가 탄생했다.   SGD의 개념   n개의 training samples에 대해 gradient를 계산하려면 O(n)만큼의 연산이 필요하다. 확률에 근거하면 전체 training samples에 대한 loss는 전체 training samples의 loss 기댓값으로 나타낼 수 있다. \\(\\frac{1}{n}(\\sum_{i=1}^n{L_i(θ, x_i, y_i)})= \\mathbb{E}_{i~[1,...,n]}[L_i(θ,x_i,y_i)]\\) 이때 loss의 기댓값은 일부 training samples만으로 아래와 같이 추정할 수 있다. \\(\\mathbb{E}_{i~[1,...,n]}[L_i(θ,x_i,y_i)] ≈ \\frac{1}{|S|}\\sum_{j∈S}(L_j(θ,x_j,y_j))\\ with\\ S⊆{1,...n}\\) 이러한 전체 training samples의 subset을 minibatch라고 하며 아래와 같이 수학적 기호로 표현할 수 있다. \\(B_i=\\{\\{x_1,y_1\\},\\{x_2,y_2\\}, ... ,\\{x_m,y_m\\}\\}=\\{B_1, B_2,...,B_{n/m}\\}\\)   용어 정리   SGD에서 새롭게 알게 될 용어들에 대해 간단히 살펴보자.   Epoch   Epoch은 한국말로 “시대”를 뜻한다. AI에서 epoch은 전체 dataset에 대하여 한 번 학습을 완료한 상태(forward &amp; backward pass과정을 거친 상태)를 의미한다.   Batch   batch는 나누어진 dataset을 의미한다. gradient descent에서는 전체 dataset을 한꺼번에 학습시키는데, 이로인해 발생하는 비효율 문제를 해소하고자 나온 알고리즘이 바로 stochastic gradient descent이었다. 여기서 SGD는 전체 dataset을 잘게 쪼개어 batch 단위로 학습시킨다고 볼 수 있다. batch를 mini-batch라고도 부르며, 일반적으로 사이즈는 32, 64, 128개를 주로 사용한다.   Iteration   iteration은 한국말로 “되풀이”라는 뜻이다. 전체 dataset을 쪼개어 batch 단위로 학습을 수행할 때, 1 epoch을 달성하기 위해서는 여러 번의 실행이 필요하다. 이러한 실행 반복을 iteration이라고 한다. 전체 data의 양이 N개라고 할 때 batch size가 n이면, iteration은 N/n이 되는 관계이다.   학습 과정   Stochastic Gradient Descent(SGD)는 다음과 같은 학습 과정을 거친다.   Loop: \t1. Sample a batch of data \t2. Forward prop it through the graph(network), get loss \t3. Backprop to calculate the gradients \t4. Update the parameters using the gradient   samples의 개수가 많은 경우 일반적인 gradient descent보다 stochastic gradient descent 알고리즘을 더 많이 사용하는데, 그 이유는 아래의 그림으로 설명할 수 있다.      gradient descent의 경우 한 step을 이동할 때 아주 정확한 방향으로 최적화를 수행한다. 반면 stochastic gradient descent는 한 step을 이동할때 방향적 정확도는 gradient descent보다 떨어짐을 확인할 수 있다. 하지만 그럼에도 불구하고 대량의 데이터에 대해 stochastic gradient descent가 더 좋은 성능을 보이는 이유는, 빠르기 때문이다. gradient descent가 최적의 한 step을 계산하는 동안 stochastic gradient descent는 이미 몇 step 이동해있기 때문이다.      비유하자면, gradient descent는 완벽주의자, stochastic gradient descent는 행동주의자이다. 여기서 완벽한 것도 좋지만 조금 틀리더라도 일단 해보는 게 중요하다는 교훈을 얻을 수 있다.    (Fancier) Optimizers   Gradient descent의 학습속도 문제를 해결하고자 stochastic gradient descent가 제안되었다. 하지만 SGD는 gradient 기반 optimizer이므로 local minimum에 갇힐 수 있다는 문제점을 가지고 있다.      위 이미지는 local minimum(혹은 saddle point)에 갇히는 경우를 나타낸다. gradient값이 일시적으로 0인 경우, gradient descent가 최적화를 중지한다는 문제를 해결하고자 여러가지 optimizers가 제안되었다.   SGD + Momentum   먼저 SGD의 학습은 아래와 같이 이루어졌다. \\(x_{t+1}=x_t-α▽f(x_t)\\)   while True: \tdx = compute_gradient(x) \tx -= learning_rate * dx   여기에 momentum의 개념을 더한 SGD+Momentum optimizer는 아래와 같다. \\(v_{t+1}=ρv_t+▽f(x_t)\\\\ x_{t+1}=x_t-αv_{t+1}\\)   vx = 0 while True: \tdx = compute_gradient(x) \tvx = rho * vx + dx \tx -= learning_rate * vx   momentum이란 “현재 가고있는 방향을 유지하려는 성질”이다. 자연계의 대부분의 현상은 갑작스럽게 변하지 않고 서서히 변한다는 가정 아래 momentum의 개념을 추가할 수 있다.      여기서 rho(ρ)값은 일반적으로 0.9 혹은 0.99를 사용하며, 관성을 유지하는 정도를 나타낸다.   AdaGrad   한편, SGD+momentum optimizer는 learning rate가 너무 작으면 학습시간이 너무 길고, learning rate가 너무 크면 발산한다는 문제가 있었다. 이러한 문제를 해결하고자 AdaGrad optimizer가 등장했다. AdaGrad는 “시간이 지남에 따라 optimum에 가까워진다”라는 가정 하에 learning rate를 계속 감소시키며 학습을 진행한다. \\(h←h+▽f(x)^2\\\\ x←x-α\\frac{1}{\\sqrt{h}}▽f(x)\\) 이처럼 h에 이전 기울기의 제곱값을 누적해서 더한 뒤, model parameter(x)를 업데이트 할 때 gradient와 learning rate의 곱에 √h를 나누어 준다. h값은 학습이 진행됨에 따라 값이 커지므로, x값의 learning rate(α)가 학습이 진행됨에 따라 감소하게 된다. 이를 코드로 나타내면 아래와 같다.   grad_squared = 0 while True: \tdx = compute_gradient(x) \tgrad_squared += dx * dx \tx -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)   그 외의 optimizers         잘 모르겠다면 일단 Adam을 써라!!    Regularization   이전에 잠시 loss가 최소가 되는 model parameters는 unique하지 않다는 것을 언급한 바 있다. 따라서 최적의 model parameters를 찾기 위해 regularization의 개념이 필요하다.      혹시 까먹었을까봐     Regularization은 model이 training data에 대하여 너무 잘 맞는 **overfitting**을 방지하기 위해 추가되는 항이다.    Model Ensembles   Model ensembles란 독립적인 모델 여러 개를 함께 사용하여 overfitting을 방지하고 성능을 향상시키는 방법이다. 각각의 모델을 따로 학습시킨 뒤 test time때 각각의 결과를 평균내는 방식으로 진행된다.      하지만 model ensembles을 하기 위해서는 좋은 컴퓨팅 성능이 필수적이다. 따라서 개인 차원에서는 잘 안하고 회사 차원에서 시도해볼 수 있는 기법이다.    model ensembles에서 변형을 가하는 방법은 다음과 같이 매우 다양하다.      Same model but different initializations   Same model but different optimization/objective function   Same model but different datasets   Top models discovered during cross-validation   Different checkpoints (i.e. iteration) of a single model   Running average of parameters during training   이러한 model ensembles의 종류에 대해 살펴보자.   Dropout   각 node에서 forward pass를 할 때, 랜덤한 확률로 일부 neurons을 zero로 만드는 기법을 dropout이라고 한다. dropping 확률은 hyperparameter로 일반적인 경우 0.5를 많이 사용한다.      아래와 같이 코드로 구현할 수 있다.   p = 0.5 # probability of keeping a unit active. higher = less dropout  def train_step(X):     \"\"\" X contains the data \"\"\"          # forward pass for example 3-layer neural network     H1 = np.maximum(0, np.dot(W1,X) + b1)     U1 = np.random.randn(*H1.shape) &lt; p # first dropout mask     H1 *= U1 # drop!     H2 = np.maximum(0, np.dot(W2,H1) + b2)     U2 = np.random.randn(*H2.shape) &lt; p # second dropout mask     H2 *= U2 # drop!     out = np.dot(W3, H2) + b3          # backward pass: compute gradients ... (not shown)     # perform parameter update ... (not shown)   dropout은 다음과 같은 가정 하에 탄생했다. 예를 들어, 고양이 이미지를 판별하는 모델을 만들어보자. 모델은 뾰족한 귀가 있고, 길쭉한 꼬리가 있고, 털이 있으면 고양이라고 판단한다고 하자. 이때 스코티쉬폴드(귀가 접힌 고양이 종류)는 귀가 뾰족하지 않아서 고양이가 아니고, 스핑크스 고양이(털이 없는 고양이)는 털이 없어서 고양이가 아니라고 판별하는 사태가 벌어질 수 있다. 이러한 overfitting을 방지하기 위해 일부 node를 의도적으로 차단시켜 “귀가 납작하지만 길쭉한 꼬리와 털을 가졌으니 고양이”라고 판단하도록 하는 게 바로 dropout 방식이다.   dropout을 model ensemble관점에서 보면 다음과 같이 해석할 수 있다. 각각의 binary mask를 하나의 model이라 보면, 같은 model parameters를 공유하는 여러 개의 model의 조합으로 볼 수 있다.   한편, train time때는 확률(p)을 통해 일부 nodes를 zero로 만들어 학습하지만, test time때는 모든 nodes를 사용한다. 이때, 각 node가 켜질 확률을 반영하여 각 layer를 통과할 때마다 확률 p를 곱해주어야 한다. 예를 들어 이러한 동작을 수행하는 코드는 아래와 같다.   def predict(X):     # ensembled forward pass     H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations     H2 = np.maximum(0, np.dot(W2, X) + b2) * p # NOTE: scale the activations     out = np.dot(W3, H2) + b3   dropout을 수행하는 전체 코드(train+test)는 아래와 같다.   \"\"\" Vanilla Dropout: Not recommended implementation (see notes below)\"\"\"  p = 0.5 # probability of keeping a unit active. higher = less dropout  def train_step(X):     \"\"\" X contains the data \"\"\"          # forward pass for example 3-layer neural network     H1 = np.maximum(0, np.dot(W1,X) + b1)     U1 = np.random.randn(*H1.shape) &lt; p # first dropout mask     H1 *= U1 # drop!     H2 = np.maximum(0, np.dot(W2,H1) + b2)     U2 = np.random.randn(*H2.shape) &lt; p # second dropout mask     H2 *= U2 # drop!     out = np.dot(W3, H2) + b3          # backward pass: compute gradients ... (not shown)     # perform parameter update ... (not shown)  def predict(X):     # ensembled forward pass     H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations     H2 = np.maximum(0, np.dot(W2, X) + b2) * p # NOTE: scale the activations     out = np.dot(W3, H2) + b3   train time에는 확률 p를 이용하여 일부 node를 drop시키고, test time에는 모든 nodes를 키되, 이러한 확률을 반영하여 activation function을 지날 때 마다 확률 p를 곱해준다. 이때 model parameters를 학습시킬 때 애초에 확률 p가 곱해진 형태로 나오도록 하는 트릭을 통해 아래와 같이 test time의 코드는 기존과 같이(확률 p를 반영하지 않도록) 만들 수 있다.   \"\"\" Vanilla Dropout: Not recommended implementation (see notes below)\"\"\"  p = 0.5 # probability of keeping a unit active. higher = less dropout  def train_step(X):     \"\"\" X contains the data \"\"\"          # forward pass for example 3-layer neural network     H1 = np.maximum(0, np.dot(W1,X) + b1)     U1 = (np.random.randn(*H1.shape) &lt; p) / p # ★ first dropout mask     H1 *= U1 # drop!     H2 = np.maximum(0, np.dot(W2,H1) + b2)     U2 = (np.random.randn(*H2.shape) &lt; p) / p # ★ second dropout mask     H2 *= U2 # drop!     out = np.dot(W3, H2) + b3          # backward pass: compute gradients ... (not shown)     # perform parameter update ... (not shown)  def predict(X):     # ensembled forward pass     H1 = np.maximum(0, np.dot(W1, X) + b1) # ★ no scaling necessary     H2 = np.maximum(0, np.dot(W2, X) + b2) # ★ no scaling necessary     out = np.dot(W3, H2) + b3   Data Augmentation   Data augmentation은 입력 데이터에 약간의 변형을 가하여 데이터의 양을 늘리는 일종의 트릭이다. 데이터 변형은 “실제 상황에서 발생할 수 있을 정도”를 지키는 한에서 다음과 같이 다양하게 변형을 가할 수 있다.   Horizontal Flips      Random crops and scales         일반적인 경우, 5개(양쪽 모서리와 정 가운데)로 쪼갠다.    Color Jitter      밝기나 대조값을 조정할 수 있다.   그 외      그 외에 translation, rotation, stretching, shearing, lens distortions 등 다양한 변형 방법을 조합하여 나만의 data augmentation을 수행할 수 있다.   DropConnect   Training: Drop connections between neurons (set weights to 0) Testing: Use all the connections   DropConnect는 뉴런간의 connections을 dropping하는 방식이다. nodes를 random하게 dropping하는 dropout과는 달리 nodes는 모두 살리고 nodes간의 connections을 random하게 dropping한다는 점에서 차이가 있다.      dropconnect의 동작을 간단하게 표현하면 다음과 같다.   Cutout   Training: Set random image regions to zero Testing: Use full image      train time에 cutout은 이미지의 일부 영역의 data값을 zero로 만듦으로써 수행된다.   Mixup   Training: Train on random blends of images Testing: Use original images      Mixup은  train time에 서로 다른 두 개의 이미지를 섞고, 그 비율에 따라 target label을 정의함으로써 수행된다.   Cutmix   Training: Train on random blends of images Testing: Use original images      Cutmix는 train time에 서로 다른 두 개의 이미지를 잘라 붙이고, 그 비율에 따라 target label을 정의함으로써 수행된다.   Hyperparameter Tuning   이전에 parameters는 모델이 학습하는 model parameters와 사람이 정의하는 hyperparameters로 나눠진다고 설명한 바 있다. 이러한 hyperparameters의 종류에는 다음의 것들이 있다.      Network architecture (e.g., layers 개수, weights 개수)   Number of itarations   Learning rate(s) (i.e., solver parameters, decay, etc.)   Regularization (more later next lecture)   Batch size   Grid Search vs. Random Search   아래는 hyperparameter를 선택하는 방법 중 grid search와 random search를 비교한 그림이다.      세로축 방향의 hyperparameter는 어떤 값이든 크게 중요치 않은 값이고, 가로축 방향의 hyperparameter는 특정 값(볼록 튀어나온 부분)에서 성능이 크게 향상되는 중요한 값이라고 하자. grid search의 경우 왼쪽 그림에서 확인할 수 있듯이 생각보다 성능이 나쁘다. 반면 오른쪽 그림과 같이 random search를 하는 경우, 성능이 좋은 hyperparameters를 발견할 가능성이 더 높다.   이러한 random search를 통해 rough한 공간 내에서의 좋은 hyperparameters 영역을 찾은 뒤, 이후 세부적인 영역 내에서 더 좋은 hyperparameters를 찾는 방식을 적용하면 더 좋은 성능을 끌어올릴 수 있다.   Example: run coarse search for 5 epochs   max_count = 100 for count in xrange(max_count):     reg = 10**uniform(-5,5)     lr = 10**uniform(-3,-6)          trainer = ClassifierTrainer()     model = init_two_layer_model(32*32*3, 50, 10) # input size, hidden size, number of classes     trainer = ClassifierTrainer()     best_model_local, stats = trainer.train (X_train, y_train, X_val, y_val, model, two_layer_net, num_epochs=5, reg=reg, update='momentum', learning_rate_decay=0.9, sample_batchs=True, batch_size=100, learning_rate=lr, verbose=False)   실행결과:      일부 hyperparameters에 대하여 val_acc값이 대략 0.4 이상으로 좋게 나오는 것을 확인할 수 있다. 이러한 영역 내에서 더 좋은 hyperparameters를 찾기 위해 아래와 같이 수행할 수 있다.   max_count = 100 for count in xrange(max_count):     reg = 10**uniform(-4, 0) # ★     lr = 10**uniform(-3, -4) # ★   실행결과:      이러한 영역 좁히기 과정을 통해 더 나은 hyperparameters를 결정할 수 있다. 이때 가장 성능이 좋은 val_acc:0.53100, lr: 9471549e-04, reg: 1.433895e-03, (14/100) 을 살펴보면, 위에서 조정해준 영역 reg(-4,0), lr(-3,-4)의 경계면에 맞닿아있는 것을 확인할 수 있다. 이러한 경우 설정해준 경계면 바깥에 더 좋은 hyperparameters가 있을 수 있다는 합리적 의심을 해볼 수 있다.   Guideline   Step1: Check initial loss      먼저 학습 초기에는 overfitting을 걱정할 필요가 없다 (오히려 overfitting으로 향해 나아가야 하는 단계이므로). 따라서 regularization loss(a.k.a. weight decay) 없이 학습 과정을 수행한다.   Step2: Overfit a small sample   전체 training data는 너무 크므로 small training data로 sampling을 한 뒤, 해당 samples에 대해서만 일단 최대한 100% training accuracy를 가지도록 architecture도 조금 수정하고, learning rate도 조금 수정하고, weigth initialization도 조금 손 봐 가며 학습을 수행한다 (일단은 overfitting 걱정하지 말고, 경향성을 살펴보자는 의미).      예를 들어 loss가 줄어들지 않는다면 learning rate를 조금 키울 수 있고, loss가 발산하면 learning rate를 조금 줄이는 식으로 hyperparameters를 rough하게 결정할 수 있다.    Step3: Find LR that makes loss go down   이후 전체 training data에 대하여 이전의 단계 (overfitting)를 수행해본다.      일반적으로 초기 learning rate의 값으로 1e-1, 1e-2, 1e-3, 1e-4를 주로 사용한다.    Step4: Coarse grid, train for ~1-5 epochs   step3으로 부터 확인한 영역 내에서 learning rate나 weight decay(regularization loss)등의 값을 선택한 뒤, 몇 개의 models에 대해 1~5 epochs만큼 train을 수행해 본다.      일반적으로 초기 weight decay값으로 1e-4, 1e-5, 0를 선택할 수 있다.    Step5: Refine grid, train longer   step4에서 성능이 좋았던 models를 선택하여 learning rate decay 없이 좀 더 길게(10~20 epochs) training을 수행해본다.   Step6: Look at loss curves   이때 loss curves를 확인해본다. loss curves는 일반적으로 다음의 경우로 나타날 수 있다.           training이 더 필요한 경우              training accuracy와 validation accuracy가 모두 계속해서 증가하고 있다면 아직 학습이 더 필요한 상태라는 뜻이다.            overfitting이 발생한 경우              training accuracy는 증가하지만 validation accuracy는 오히려 감소한다면, overfitting이 발생한 것이다. 이런 경우 regularization을 증가시키거나 더 많은 data를 입력하는 방안을 택할 수 있다.            underfitting이 발생한 경우              training accuracy와 validation accuracy간에 gap이 매우 작은 경우는 underfitting되었음을 의미한다. 이런 경우 더 길게 training을 시키거나, 더 큰 model을 사용하는 방안을 택할 수 있다.       Step7: GOTO step5   loss curve를 확인한 후 다시 step5로 돌아갈 수 있다.        내가 풀려는 문제와 유사한 모델을 가져와서 사용된 hyperparameters에서부터 시작해본다면 여러가지 시행착오를 줄일 수 있다.    ","categories": ["ai-deepLearning"],
        "tags": ["Deep Learning","AIAS"],
        "url": "/ai-deeplearning/DL03/",
        "teaser": null
      },{
        "title": "[DL-04] Lect5. Deep Learning Programming",
        "excerpt":"AIAS-Lect5_DL Programming   Deep Learning Platform   소소한 팁      데이터의 양보다는 데이터의 퀄리티가 더 중요하다. 예를 들어 얼굴인식 모델을 만들 때, 너무 고정적인 환경에서의 데이터만 주어진다면 데이터가 10만장이어도 중복성때문에 사실상 유용성은 떨어질 수 있다. 반면, 30개의 데이터를 사용하더라도 성별, 옷차림, 헤어스타일, 악세사리, 안경 등의 practical 환경에서 가능한 여러가지 변화를 준 데이터를 사용한다면 훨씬 성능이 나아질 수 있다.       model을 만드는 것보다 더 중요한 것은 goal을 정의하는 것이다.    AI의 역사   초기 인공지능 분야는 비주류였다. 하지만 빅데이터, HW의 발전, SW의 발전 등의 요인으로 짧은 기간동안 큰 발전을 이룰 수 있었다.      Big Data : Datasets이 더욱 커졌으며 수집 및 저장이 용이해졌다.   Hardware : Graphics Procesisng Units(GPUs)기반 딥러닝 등장과 대규모 병렬 컴퓨팅(Massively Parallel Computer, MPP)   Software : 향상된 기술, 새로운 models과 toolboxs   ML Pipeline   ML Pipeline이란?   ML pipeline은 아래 diagram과 같이 몇 가지 components로 구성되어 있다.1     이처럼 model은 일부이고 다양한 components를 거쳐 최종적인 ML이 완성된다. 예를 들어, 일반적인 ML은 다음의 과정을 거친다.           Data &amp; Streaming : 분산 데이터 저장 및 스트리밍. 심하게 noisy한 data는 삭제된다. Google Cloud Storage, 아파치 카산드라, 아파치 하둡 등.            Users : 데이터 준비 및 분석의 단계. Jupyterhub, 아파치 제플린, 아파치 스파크 등.            Frameworks &amp; Cluster : Deep Learning Tools와 분산 호스팅 사용의 단계. 엔지니어링 영역. Tensorflow, PyTorch 등.            Models : Machine learning model을 빌딩. 사이언티스트의 영역.            Model Serving : Model을 clients에게 전송       Deep Learning Hardware   CPU와 GPU의 차이점      재밌게 봤던 영상: Mythbusturs Demo GPU versus CPU - NVIDIA. CPU와 GPU의 차이점을 시각적으로 재미있게 설명해준다.                   CPU(Central Processing Unit)2       개수는 적지만 각 코어가 빠르고 훨씬 capable하다. 순차적인 tasks 처리에 유용하다.                 number of cores = 4  ⇒ 직렬처리에 최적화된 몇 개의 코어로 구성됨       Memory = System RAM  ⇒ cache가 매우 작으므로 대부분의 메모리를 RAM에서 가져다 사용한다. RAM은 보통 8, 12, 16, 32GB의 크기를 갖는다. 이는 GPU의 메모리 크기와 비교했을 때 매우 적은 양.                       GPU(Graphics Processing Unit)3       개수는 많지만 각 코어가 비교적 느리고 단순한 작업만을 수행한다. 병렬처리에 유용한다.                 number of cores = 4  ⇒ 다수의 소형 코어로 구성됨. 이를 통해 병렬처리를 효율적으로 처리 가능.       Memory = 11GB GDDR6 ⇒ 칩 안에 RAM이 내장되어있는 내장 메모리를 사용한다.           CPU와 GPU의 Communication   하드웨어가 균형적으로 구성되지 않는 경우, PC의 성능이 저하되는 병목현상이 나타날 수 있다.      Bottle Neck이란? bottle neck은 이름 그대로 해석할 때 “병(bottle)의 목(neck) 현상”이다. 병에 들어있는 물체가 병의 주둥아리(bottle neck)으로 인해 한 번에 나오지 못하고 지연이 발생하는 현상과 관련한 용어이다.    CPU Bottle Neck CPU가 여러 복잡한 프로세스를 처리할 때, CPU의 처리 속도가 데이터의 전송속도를 따라가지 못하는 경우 발생한다.      실제 딥러닝 모델을 학습시킬 때 구간별로 소요시간을 측정해볼 것! 병목현상이 어디에서 발생하는지를 확인할 수 있다.    해결방법      CPU와 GPU의 사양을 일치시킨다.   CPU와 GPU에 걸리는 처리 부하의 균형을 맞춘다.   HDD보다는 SSD를 사용한다.   Deep Learning Software      언어는 주로 PyTorch(Facebook), TensorFlow(Google)를 사용한다.       Microsoft Azure, amazon web services, Google Cloud Platform, Alibaba Cloud 등에서 좋은 GPU를 대여해주기도 한다.    Deep Learning Frameworks에서의 주요 포인트      실행력 : 새로운 idea가 있을 때 빠르게 실행하는 것이 중요하다. ppt 100슬라이드보담 빠르게 데모 보여주는게 더 임팩트가 있다. 그런 측면에서 DL framework를 활용하면 실제와 모델을 맵핑하기 쉬워진다.   autograd : pytorch등은 기본적인 함수의 gradient는 자동으로 구해준다. 하지만 ReLU같은 간단한 module에 대해서 forward와 backward정도는 구해본다면 이해도가 높아질 것이다.   GPU : 머신러닝 모델은 GPU 내에서 효율적으로 돌아간다. (cuDNN, cuBLAS, OpenGL 등)   Numpy vs. PyTorch   PyTorch를 사용하면 보다 간단하고 빠르게 머신러닝 코드를 구현할 수 있다. Numpy와 PyTorch로 각각 모델을 구현해본 뒤 차이점을 살펴보자.   Numpy   import numpy as np np.random.seed(0)  N,D = 3, 4 # N=batch개수, D=차원  x = np.random.randn(N,D) y = np.random.randn(N,D) z = np.random.randn(N,D)  a = x * y b = a + z c = np.sum(b)  # 각 node별로 gradient 계산하기&lt;sup&gt;[4](#mFoot)&lt;/sup&gt; grad_c = 1.0 grad_b = grad_c * np.ones((N,D)) grad_a = grad_b.copy() grad_z = grad_b.copy() grad_x = grad_a * y grad_y = grad_q * x      Numpy로 구현한 코드는 API가 깔끔하고 숫자를 다루기 쉽다는 장점이 있으나, gradients 수식을 직접 계산해야므로 번거롭고, GPU 위에서 돌릴 수 없다는 단점이 있다.   PyTorch   import torch  N, D = 3, 4 x = torch.randn(N,D, required_grad=True) y = torch.randn(N,D) z = torch.randn(N,D)  a = x * y b = a + z c = torch.sum(b)  c.backward() # 위에서 required_grad값을 True해주었으므로 backward()를 통해 바로 gradient를 계산할 수 있다. print(x.grad)     1: Overview of ML Pipelines (https://developers.google.com/machine-learning/testing-debugging/pipeline/overview?hl=ko)  2: Intel Core i7-7700k 기준  3: (NVIDIA RTX 2080 Ti 기준)  4: gradient 계산 관련 포스팅 참고       PyTorch: 기본 개념      Tensor : numpy array와 유사한 개념으로, numpy와 달리 GPU위에서도 돌아간다.   Autograd : tensors로 구성된 computational graphs를 만드는 package로, 자동으로 gradients를 계산해준다.   Module : 하나의 neural network layer로, state와 learnable weigths를 저장할 수 있다.   위의 내용을 코드와 함께 하나씩 살펴보자.      해당 내용은 PyTorch 공식 홈페이지의 Learning PyTorch with Examples를 참고했습니다.       PyTorch 문법과 관련된 내용은 PyTorch 공식 홈페이지의 Docs에서 확인할 수 있습니다.    PyTorch: Running Example   Running example:      A fully-connected ReLU network with one hidden layer and no biases   trained to predict y from x by minimizing squared Euclidean distance.      위와 같은 그림을 많이 봤을 것이다. 2개의 layer로 구성되었다는 것을 알 수 있다. 위 그림에서 생략된 부분을 자세히 나타내면 아래 그림과 같다.      layer별로 weigth parameter가 존재하며, 첫 번째 layer에는 ReLU activation function도 존재함을 확인할 수 있다. 이와 같이 모델을 구성하고자 할때 PyTorch를 이용하여 코드를 작성해보자.   PyTorch: Tensors   PyTorch는 Tensors를 지원한다. 이를 통해 모델을 디자인할 수 있다.   Tensor란?           전체 코드 보기 [참고]       import torch    device = torch.device('cpu')    N, D_in, H, D_out = 64, 1000, 100, 10 x = torch.randn(N, D_in, device=device) y = torch.randn(N, D_out, device=device) w1 = torch.randn(D_in, H, device=device) w2 = torch.randn(H, D_out, device=device)    learning_rate = 1e-6 for t in range(500):     h = x.mm(w1)     h_relu = h.clamp(min=0)     y_pred = h_relu.mm(w2)     loss = (y_pred - y).pow(2).sum()        grad_y_pred = 2.0 * (y_pred - y) \t\tgrad_w2 = h_relu.t().mm(grad_y_pred) \t\tgrad_h_relu = grad_y_pred.mm(we.t()) \t\tgrad_h = grad_h_relu.clone() \t\tgrad_h[h&lt;0] = 0 \t\tgrad_w1 = x.t().mm(grad_h)    \t\tw1 -= learning_rate * grad_w1 \t\tw2 -= learning_rate * grad_w2                   Create random tensors for data and weights              device = torch.device('cpu')     N, D_in, H, D_out = 64, 1000, 100, 10 x = torch.randn(N, D_in, device=device) y = torch.randn(N, D_out, device=device) w1 = torch.randn(D_in, H, device=device) w2 = torch.randn(H, D_out, device=device)     learning_rate = 1e-6           먼저 N, D_in, H, D_out과 같은 hyperparameters를 정의한다. 여기서 N은 batch size를 의미하며, D_in, H, D_out은 각각 input layer, hidden layer, output layer의 크기를 의미한다.       hyperparameters(N, D_in, H, D_out)을 결정했다면 자동적으로 x, y, w1, w2의 사이즈가 결정된다. 결정된 사이즈를 갖는 x, y, w1, w2에 대해 random tensors를 생성하여 초기화 해 준다.       learning_rate도 마찬가지로 초기화해준다.       한편, device는 코드가 돌아갈 장치를 지정해준다. 값을 'cpu'가 아닌 'cuda:0'와 같이 GPU로 지정하면 학습 속도가 향상된다.            Forward pass: compute predictions and loss              for t in range(500):     h = x.mm(w1)     h_relu = h.clamp(min=0)     y_pred = h_relu.mm(w2)     loss = (y_pred - y).pow(2).sum()           이 예제에서는 activation function으로 ReLU함수를 사용한다. 구성된 텐서 수식을 통해 예측값(y_pred)와 예측값에 따른 loss를 계산한다.            Backward pass: manually compute gradients                  grad_y_pred = 2.0 * (y_pred - y) \t\tgrad_w2 = h_relu.t().mm(grad_y_pred) \t\tgrad_h_relu = grad_y_pred.mm(we.t()) \t\tgrad_h = grad_h_relu.clone() \t\tgrad_h[h&lt;0] = 0 \t\tgrad_w1 = x.t().mm(grad_h)           gradient는 loss가 줄어드는 방향을 알려준다. gradients를 구하는 수식을 계산해준다.            Gradient descent step on weights              계산한 gradients값을 w1, w2에 적용해준다. 이를 통해 다음 반복부터는 loss를 줄일 수 있게 될 것이다.       PyTorch: Autograd   PyTorch는 autograd 기능을 지원한다. 이를 통해 gradients를 손수 계산하는 대신 코드상에서 자동으로 계산되게 만들 수 있다.           전체 코드 보기       import torch    N, D_in, H, D_out = 64, 1000, 100, 10 x = torch.randn(N, D_in) y = torch.randn(N, D_out) w1 = torch.randn(D_in, H, requires_grad=True) w2 = torch.randn(H, D_out, requires_grad=True)    learning_rate = 1e-6 for t in range(500):     h = x.mm(w1)     h_relu = h.clamp(min=0)     y_pred = h_relu.mm(w2)     loss = (y_pred - y).pow(2).sum()        loss.backward()    \t\twith torch.no_grad(): \t\t\tw1 -= learning_rate * grad_w1 \t\t\tw2 -= learning_rate * grad_w2 \t\t\tw1.grad.zero_() \t\t\tw2.grad.zero_()              왼쪽은 Autograd를 사용하기 이전 코드이고 오른쪽은 PyTorch에서 지원하는 Autograd를 사용한 코드이다. 주황색 박스를 보면, 기존의 수식을 직접 대입하는 방식에서 loss.backward()와 같이 자동으로 계산하게 하는 코드로 바뀐 것을 확인할 수 있다. 이때 autograd를 수행하는 parameter는 requires_grad=True와 같이 명시해주어야 한다(빨간 박스부분). 해당 인자를 통해 PyTorch가 computational graph를 만들기 때문이다.      x와 y는 parameters가 아니라 data다. 따라서 학습가능하지 않고 불변하므로 gradients를 계산할 수 없다.    torch.no_grad()는 이 부분에 대해서는 computational graph를 생성하지 말라는 것을 의미한다. 한편, 다음 iteration에서는 새로운 data가 들어오므로 메모리를 들고있을 필요가 없다. 따라서 w.grad.zero_()를 통해 gradient를 초기화해준다.      TIP! gradient초기화해주는 부분(w.grad.zero_())에서 함수 끝에 언더바(_)가 있는 걸 확인할 수 있다. 이것은 결과값이 return으로 반환되는 게 아니라, 그 자체를 변형시킨다는 것을 의미한다.    Autograd함수 직접 정의하는 방법   forward와 backward함수 작성을 통해 자유롭게 autograd함수를 만들 수 있다.           직접정의한 Autograd 함수: MyReLU()       class MyReLU(torch.autograd.Function): \t@staticmethod \tdef forward(ctx, x): \t\tctx.save_for_backward(x) \t\treturn x.clamp(min=0) \t@staticmethod \tdef backward(ctx, grad_y): \t\tx, = ctx.saved_tensors \t\tgrad_input = grad_y.clone() \t\tgrad_input[x&lt;0] = 0 \t\treturn grad_input                MyReLU()를 사용한 전체 코드       import torch    N, D_in, H, D_out = 64, 1000, 100, 10 x = torch.randn(N, D_in) y = torch.randn(N, D_out) w1 = torch.randn(D_in, H, requires_grad=True) w2 = torch.randn(H, D_out, requires_grad=True)    learning_rate = 1e-6 for t in range(500): \t\ty_pred = my_relu(x.mm(w1)).mm(w2)     loss = (y_pred - y).pow(2).sum()        loss.backward()    \t\twith torch.no_grad(): \t\t\tw1 -= learning_rate * grad_w1 \t\t\tw2 -= learning_rate * grad_w2 \t\t\tw1.grad.zero_() \t\t\tw2.grad.zero_()           Forward pass: compute predictions and loss Backward pass: compute gradients   forward()는 predictions와 loss를 계산하는 역할을 한다. 위 예시에서는 x.clamp(min=0)을 통해 ReLU결과를 반환한다. 한편 여기서 ctx는 일종의 cache memory역할을 한다.   backward()는 gradients를 계산하는 역할을 한다.      일반적인 경우에는 backward()함수를 정의할 필요가 없다. backward가 필요없는 경우에는 ctx도 필요없으므로 autograd를 하지 않는 경우는 해당 부분을 삭제함으로써 memory증가를 막아줘야 한다.    PyTorch: nn   Computational graph와 autograd는 복잡한 연산을 정의하거나 자동으로 derivatives를 구하는 데에 효과적이다. 하지만 매우 큰 neural networks에 대해서는 autograd가 너무 low-level일 수 있다. 따라서 PyTorch는 nn package를 통해 이러한 문제를 해결한다.   nn package는 neural network layers와 거의 동등한 modules 세트를 정의한다. Module은 input Tensors를 받고, output Tensors를 계산한다 (물론 learnable parameters를 포함하는 tensors같은 internal state도 보유할 수 있다.). nn package는 또한 유용한 loss functions 세트도 정의하고 있다.      주요 코드 비교           전체 코드보기       import torch    N, D_in, H, D_out = 64, 1000, 100, 10 x = torch.randn(N, D_in) y = torch.randn(N, D_out)    model = torch.nn.Sequential( \t\t\t\t\ttorch.nn.Linear(D_in, H), \t\t\t\t\ttorch.nn.ReLU(), \t\t\t\t\ttorch.nn.Linear(H, D_out))    learning_rate = 1e-2 for t in range(500): \ty_pred = model(x) \tloss = torch.nn.functional.mse_loss(y_pred, y)    \tloss.backward()    \twith torch.no_grad(): \t\tfor param in model.parameters(): \t\t\tparam -= learning_rate * param.grad \tmodel.zero_grad()           PyTorch:Autograd 코드(일부)   ...  learning_rate = 1e-6 for t in range(500):     h = x.mm(w1)     h_relu = h.clamp(min=0)     y_pred = h_relu.mm(w2)     loss = (y_pred - y).pow(2).sum()  ...   PyTorch:nn 코드(일부)   ...  model = torch.nn.Sequential( \t\t\t\t\ttorch.nn.Linear(D_in, H), \t\t\t\t\ttorch.nn.ReLU(), \t\t\t\t\ttorch.nn.Linear(H, D_out))  learning_rate = 1e-2 for t in range(500): \ty_pred = model(x) \tloss = torch.nn.functional.mse_loss(y_pred, y)  ...           PyTorch 문법 helper       torch.nn.Sequential(*args) : A sequential container. Modules will be added to it in the order they are passed in the constructor.       torch.nn.Linear(in_features, out_features, bias=True) : Applies a linear transformation to the incoming data: $y=xA^T+b$       parameters              in_features – size of each input       sampleout_features – size of each output       samplebias – If set to False, the layer will not learn an additive bias. Default: True           torch.nn.ReLU(inplace=False) : Applies the rectified linear unit function element-wise: $ReLU(x)=(x)^+=max(0,x)$       parameters              inplace – can optionally do the operation in-place. Default: False           위와 같이 model을 정의하면, for문에서 위와 같이 사용할 수 있다. 이처럼 PyTorch.nn class의 장점을 활용하면 코드를 좀 더 간결하고 유연하게 만들 수 있다.   새로운 modules 직접 정의하는 방법   __init__과 forward함수 작성을 통해 자유롭게 모듈을 정의할 수 있다.           직접 정의한 module: TwoLayerNet()       import torch    class TwoLayerNet(torch.nn.Module): \tdef __init__(self, D_in, H, D_out): \t\tsuper(TwoLayerNet, self).__init__() \t\tself.linear1 = torch.nn.Linear(D_in, H) \t\tself.linear2 = torch.nn.Linear(H, D_out)    \tdef forward(self, x): \t\th_relu = self.linear1(x).clamp(min=0) \t\ty_pred = self.linear2(h_relu) \t\treturn y_pred                  autograd가 backward를 핸들링하므로 여기서는 backward()함수를 정의할 필요가 없다.                 TwoLayerNet을 사용한 전체 코드       import torch    N, D_in, H, D_out = 64, 1000, 100, 10 x = torch.randn(N, D_in) y = torch.randn(N, D_out)    model = TwoLayerNet(D_in, H, D_out)    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) for t in range(500): \ty_pred = model(x) \tloss = torch.nn.functional.mse_loss(y_pred, y)   \t \tloss.backward() \toptimizer.step() \toptimizer.zero_grad()              새로운 module을 정의할 때 nn의 module을 사용할 수 있다.    PyTorch: optim   [PyTorch: Tutorials &gt; 예제로 배우는 파이토치(PyTorch) &gt; PyTorch: optim]   이전에 PyTorch:nn package를 이용해서 신경망을 구현한 코드에서 더 나아가 모델의 가중치(loss)를 직접 갱신하는 대신 optim package를 이용하여 가중치를 갱신할 optimizer를 정의해보자. optim package는 일반적으로 딥러닝에 사용되는 SGD+momentum, RMSProp, Adam과 같은 다양한 최적화(Optimization) 알고리즘을 정의한다.   주요 코드 비교           전체 코드보기       import torch    N, D_in, H, D_out = 64, 1000, 100, 10 x = torch.randn(N, D_in) y = torch.randn(N, D_out)    model = torch.nn.Sequential( \t\t\t\t\ttorch.nn.Linear(D_in, H), \t\t\t\t\ttorch.nn.ReLU(), \t\t\t\t\ttorch.nn.Linear(H, D_out))    learning_rate = 1e-4 optimizer = torch.optim.Adam(model.parameters(), \t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlr = learning_rate)    for t in range(500): \ty_pred = model(x) \tloss = torch.nn.functional.mse_loss(y_pred, y)   \t \tloss.backward()   \t \toptimizer.step() \toptimizer.zero_grad()           PyTorch:nn 코드(일부)   ...  for t in range(500): \ty_pred = model(x) \tloss = torch.nn.functional.mse_loss(y_pred, y)  \tloss.backward()  \twith torch.no_grad(): \t\tfor param in model.parameters(): \t\t\tparam -= learning_rate * param.grad \tmodel.zero_grad()   PyTorch:optim 코드(일부)   ... optimizer = torch.optim.Adam(model.parameters(), \t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlr = learning_rate)  for t in range(500): \ty_pred = model(x) \tloss = torch.nn.functional.mse_loss(y_pred, y) \t \tloss.backward() \t \toptimizer.step() \toptimizer.zero_grad()           PyTorch 문법 helper       torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) : Implements Adam algorithm.       parameters              params (iterable) – iterable of parameters to optimize or dicts defining parameter groups       lr (float, optional) – learning rate (default: 1e-3)       betas (Tuple[float, float], optional) – coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))       eps (float, optional) – term added to the denominator to improve numerical stability (default: 1e-8)       weight_decay (float, optional) – weight decay (L2 penalty) (default: 0)       amsgrad (boolean, optional) – whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond (default: False)           torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) : Implements Adam algorithm.       parameters              params (iterable) – iterable of parameters to optimize or dicts defining parameter groups       lr (float, optional) – learning rate (default: 1e-3)       betas (Tuple[float, float], optional) – coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))       eps (float, optional) – term added to the denominator to improve numerical stability (default: 1e-8)       weight_decay (float, optional) – weight decay (L2 penalty) (default: 0)       amsgrad (boolean, optional) – whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond (default: False)           위 코드는 optimizer를 Adam으로 정의하고, 이를 이용하여 for문에서 parameters와 zero gradients를 업데이트해주었다.   PyTorch: Pretrained Models   torchvision을 이용하면 pretrained models를 쉽게 사용할 수 있다. pytorch/vision 깃허브에 들어가면 자세한 내용을 확인할 수 있다.   PyTorch: torch.utils.tensorboard   TensorBoard utility를 이용하여 TensorBoard UI내에서 PyTorch 결과를 시각화 할 수 있다. TensorBoard에 대한 자세한 내용은 TensorFlow-TensorBoard 페이지에서 확인 가능하다.   PyTorch: Dynamic Computational Graphs   코드의 실행 단계에 따라 computational graph가 동적으로 생성된다.   ⇒ 비효율적   PyTorch: Static Computational Graphs   먼저 computational graph를 생성한 뒤, 해당 graph를 매 iteration마다 재사용한다.   graph = build_graph()  for x_batch, y_batch in loader: \trun_graph(graph, x=x_batch, y=y_batch)   ","categories": ["ai-deepLearning"],
        "tags": ["Deep Learning","AIAS"],
        "url": "/ai-deeplearning/DL04/",
        "teaser": null
      },{
        "title": "[DL-05] Lect6. CNN-Convolution Neural Network",
        "excerpt":"AIAS-Lect6_CNN: Convolutional Neural Networks(1)   Introduction   이번 장에서는 Convolutional Neural Networks에 대해서 알아본다.   CNN의 등장 배경   \\[Linear\\ Score\\ Function:\\ f=Wx\\\\ 2-layer\\ Neural\\ Network:\\ f=W_2max()\\]  CNN 이전까지는 위와 같은 fully-connected layer(=MLP, multilayer perceptron) 방식이 주로 사용되었다.      하지만 이러한 알고리즘을 사용하는 경우 이미지 학습이 제대로 이루어지지 않는다는 단점이 있었다. 예를 들어 이미지의 경우 (채널을 무시한다고 할 때) 2차원 array의 형태로 구성되어 있다. 이때 학습시키기위해 image의 pixel value를 flatten시킨다면 본래 이미지의 공간상의 영역을 제대로 표현하지 못해 좋은 구조라고 할 수 없다.   이러한 니즈에서 나타난 구조가 바로 convolutional neural networks(CNNs)구조이다. 공간상의 정보를 유지하면서 hidden vector를 표현한다는 것이 특징이다.   Convolutional Neural Networks(CNN)   Fully Connected Layer      Fully connected layer란 하나의 layer의 모든 뉴런이 그 다음 layer의 모든 뉴런과 연결된 상태를 의미한다. 이때 이 layer는 flatten되어 1차원 배열의 형태라는 특징을 갖는다. fully connected layer는 기존의 image 인식에 주로 사용되었던 구조인데, 몇 가지 한계가 있다.      image가 고해상도인 경우, input layer의 뉴런의 개수가 증가하므로 전체 parameters의 수가 급격히 증가한다.   이미지 pixel을 flatten한 형태이므로 영상 전체의 관계(topology)를 고려하지 못하여 입력 데이터의 변형에 취약하다. (따라서 굉장히 많은 학습 데이터를 필요로한다.)   Convolution Layer   Convolution layer(CL)은 fully connected layer(FCL)의 단점을 보완하여 이미지 데이터에 대해 보다 나은 성능을 도출하기 위해 고안된 layer 모델이다. FCL은 flatten된 1차원 배열의 형태이지만 CL은 입력 데이터의 형태를 유지한 3차원 배열의 구조를 갖는다. 한편 CL은 입력 데이터의 모든 뉴런에 연결되는 FCL와 달리, filter내에 존재하는 뉴런에만 연결된다. 아래의 이미지에서 파란색은 입력 데이터를, 짙은 파란색은 filter 영역을, 하얀색은 연산 결과를 의미한다.      한편 CL의 이러한 구조는 Hubel &amp; Wiesel의 실험에서 보여준 아이디어를 잘 설명한다. 아래 그림은 layer별 결과를 이미지화 한 것이다.      앞쪽 layer일 수록 저수준의 특성(edge, blob)을 판별하는 데에 유용하고 뒤로 갈 수록 점점 고수준의 특성으로 조합해나가는 것을 확인할 수 있다.   Filter   위에서 설명한 바와 같이 input layer에 filter(or kernel)을 연산하여 output layer의 결과를 도출한다. filter의 구조는 아래와 같이 생겼다.      filter 영역에는 weight parameter(W)가 존재한다. 입력 데이터 X에 이러한 W를 연산하여 feature map을 도출한다.   Filter 연산   일반적인 영상처리 관점에서 봤을때, 이미지에 필터로 연산을 할 때에는 convolution 연산을 사용한다. Convolution layer라는 이름처럼 convolution layer도 convolution연산을 사용할 것 같지만 실은 그렇지 않다. convolution은 “하나의 함수를 reverse &amp; shift한 결과를 다른 함수에 곱하는 것”이다. 즉, reverse의 과정이 존재한다. 반면 cross-correlation은 convolution에서 ‘reverse‘만 하지 않는다. 즉 수식으로 풀어쓰면 아래와 같다. \\(convolution\\ (f*g)(t)={\\int^{∞}_{-∞}}{f(τ)g(t-τ)}dτ\\\\ cross-correlation\\ (f*g)(t)={\\int^{∞}_{-∞}}{f(τ)g(t+τ)}dτ\\\\\\)   이때 deep learning관점에서 봤을 때 filter의 weights는 정해진 값이 아니라 학습시키는 값이다. 따라서 result가 올바른 결과를 도출하게끔 개선된다. 따라서 convolution연산이나 cross-correlation이나 결과적으로 같은 결과를 도출한다. 따라서 tensorflow와 같은 deep learning framework에서는 convolution이 아닌 cross-correlation으로 CNN이 구현된다. cross-correlation은 입력데이터와 필터의 weights의 대응되는 원소간 곱의 합계이므로 fused multiply-add(FMA)라고도 부른다. 이러한 filter 연산을 통해 dimensional descent/expansion의 효과를 얻을 수 있다.   한편, Filter의 특성을 결정짓는 요소는 다음과 같다.      Input layer의 사이즈 ⇒ filter size를 결정   filter의 개수 ⇒ 출력 데이터의 depth를 결정   Padding ⇒ 출력 데이터의 width, height를 결정   Stride ⇒ 출력 데이터의 width, height를 결정   Input layer의 사이즈   filter는 input layer size의 영향을 받는다. 먼저 input layer를 살펴보자.      위에서 언급했듯 input layer는 3차원 데이터(height, width, depth)의 크기를 갖는다. 이때 depth가 3개인 RGB image data를 생각해보자. 이미지에서 R channel, G channel, B channel은 각각 동시성을 가지며 같은 공간 영역을 나타낸다. 따라서 spatial structure를 유지한다는 convolution layer의 철학에 맞게 filter 또한 이러한 성질을 반영해야 한다. 따라서 filter의 depth는 input layer의 depth와 같은 크기를 갖는다.   Filter의 개수   filter를 간단하게 표현하자면, input data의 어떠한 특성을 도출하기 위한 값이다. 예를 들어, filter1은 input data의 edge특성을 나타낼 수 있고, filter2는 input data의 검정색 영역의 분포 특성을 잘 나타낼 수 있다. 한편 하나의 이미지에서 이러한 특성은 동시에 발생한다. 따라서 convolution layer를 통과한 결과의 depth로 특성을 동시에 표현할 수 있다. 기본적으로 filter의 depth는 input data의 depth와 같으므로 연산 결과의 depth는 1의 크기를 갖는다. 이러한 결과를 얼마나 겹치느냐에 따라 output data의 depth가 결정되는 것이다. 예를 들어, filter를 N개 사용하는 경우에는 output data의 depth가 N이 된다.   Padding   padding이란 필터를 적용시키기 전에 input data의 가장자리를 특정값으로 채우는 것을 의미한다. 예를 들어 아래와 같이 4x4의 사이즈를 갖는 input data 3x3 filter를 적용시키는 경우를 생각해보자. 이 경우 output data의 크기는 2x2가 된다. 이때 input data에 zero-padding=1을 주는 경우, input data가 5x5가 되어, 같은 filter를 적용해도 결과가 4x4가 된다는 것을 알 수 있다.      만일 입력데이터에 padding을 하지 않는다면 출력되는 데이터의 크기는 무조건 입력 데이터보다 작아질 것이다. 레이어가 깊어질 수록 (=filter가 많아질수록) 출력 데이터의 사이즈가 심하게 줄어들어 학습이 불가능한 상태가 되는 것을 방지하기 위해 도입된 개념이 바로 padding이다. 일반적으로 P만큼의 padding을 줄 때, (X_Height,X_Width)의 사이즈를 갖는 input data는 (X_H+2P, X_W+2P)의 크기가 되며, filter의 크기가 (F_Height,F_Width)일 때 output data의 크기 (O_Heigth,O_Width)는 다음의 식을 만족하게 된다. \\((O_H,O_W)=(X_H+2P-F_H+1,X_W+2P-F_W+1)\\)      Q. 채워주는 Padding값이 result에 영향을 주지는 않을까?     padding을 주는 경우 대부분의 상황에서는 zero-padding이 일반적이다. 하지만 gray-padding이라고 해서 밝기값이 0~255인 경우에 128값으로 padding을 주는 경우도 존재한다. 중요한 점은, 이미지의 가장자리값이 조금 바뀌는 것은 전체 이미지에 큰 영향을 주지 못한다는 점이다. 일반적으로 classification에서 주요 인식 대상은 이미지의 가운데에 위치할 뿐만 아니라, padding을 주는 값은 전체 image의 size 중에서 극히 일부이기 때문이다.    Stride   stride란 필터의 이동 간격을 의미한다. stride=1인 경우 filter는 input data에서 1pixel씩 이동하며 연산을 수행한다. 예를 들어 위의 예시에서는 stride=1인 경우이다. 이때 stride에서 주의해야 할 점은 input data, filter의 size와 맞지 않는 경우 output data의 size가 정수가 아니게 되어 에러가 발생할 수 있다는 점이다. 예를 들어, 6x6 input data에 3x3 filter를 적용하는 경우를 생각해보자. stride=2라면 filter가 stride를 이동하는 과정에서 input data를 초과하여 접근하게 된다. 따라서 이러한 경우를 방지하기 위해 아래의 공식을 통해 출력데이터의 사이즈가 자연수값이 되도록 적절히 stride를 조정해주어야 한다. 아래의 식은 입력데이터(X_H, X_W), filter크기(F_H, F_W), Padding P, Stride S인 경우의 출력데이터의 크기(O_H, O_W)를 계산하는 공식이다. \\((O_H,O_W)=(\\frac{X_H+2P-F_H}{S}+1,\\frac{X_W+2P-F_W}{S}+1)\\)      Q. input의 volume이 32x32x3이며 10개의 5x5 filter를 stride=1, pad=2로 줄 때 output의 volume size는 어떻게 될까?     (1)  volume = (32+2x2, 32+2x2, 3) = (36, 36, 3) (∵ input data에 padding, pad=2) (2) volume = (36-5+1, 36-5+1, 3/3) = (32, 32, 1) (∵ 5x5 filter 적용, stride=1) (3) volume = (32, 32, 1*10) (∵ filter 10개 적용)     ⇒ ∴ 32x32x10     Q. 이 layer에 존재하는 parameters의 개수는?     filter size는 5x5x3이며, 10개가 존재한다. 또한 filter별로 bias가 1개씩 존재한다. 따라서 (5x5x3)X10 + 10x1 = 750 + 10 = 760    Pooling Layer   pooling layer는 CNN을 구성하는 layer중 하나이다. 일반적인 경우, convolution layer에서는 입력데이터의 사이즈를 유지하는 반면, pooling layer는 주로 데이터의 사이즈를 줄이기 위해 사용된다. 여기서 주의할 점은, pooling layer는 parameters를 갖고있지 않다는 점이다. pooling layer는 filter와 달리 따로 weight를 가지는 것이 아니라 정해진 연산을 수행할 뿐이다. 대표적인 pooling layer에는 max-pooling과 average pooling이 있다.   Max-pooling   max-pooling이란, 해당하는 영역에서의 최대값(MAX)을 선택하는 방법이다. 이러한 방법은 window 내에서 가장 중요한 요소 정보만을 취하겠다는 철학이 바탕이 된다. 아래 이미지는 이러한 max-pooling의 결과를 잘 보여준다. 대부분의 이미지 인식 모델에서는 주로 max-pooling을 사용한다. 또한 pooling layer에서 pooling의 window size는 stride와 같은 값으로 취하는 경우가 일반적이다.      Average-pooling   average-pooling은 해당하는 영역 내의 평균값을 취하는 방법이다. 아래 그림은 average-pooling을 잘 보여준다. average-pooling은 max-pooling보다 덜 극단적이며, 중요한 요소든 덜 중요한 요소든 모두 혼합하여 사용한다.         경우에 따라서는 pooling layer를 두는 대신 stride를 크게 주어 데이터의 사이즈를 줄이는 경우도 있다.    Normalization Layer   Normalization layer는 입력을 정규화 하는 layer이다.   Fully Connected Layer (FC layer)   CNN의 탄생 배경은, fully connected layer가 입력된 이미지 데이터에 대한 spatial space를 제대로 표현하지 못한다는 점이었다. 그럼에도 불구하고 CNN은 fully connected layer를 필요로 한다. 일반적으로는 아래와 같이 CNN의 마지막 layer단에서 classification을 위해 사용되는 경우가 많다.      CNN Architectures   지금까지 CNN의 구성요소(CONV Layer, Filter, POOL Layer, FC Layer 등)에 대해 살펴보았다. 정리하자면, CNN architectures는 convolution layer(CONV), pooling layer(POOL), fully connected layer(FC) 등으로 구성되며, CONV와 FC는 learnable filter(weights)를 갖는다. 여기서 유의할 점은 언급하지는 않았으나 각 layer를 통과할 때마다 activation function을 지나간다는 점이다. 이 부분에 대해 간단하게 설명하자면, activation function이 없는 경우 아무리 deep하게 layers가 구성되어있을지라도 결국에 linear할 수밖에 없다. 따라서 layer마다 activation function을 두어 non-linear하게 만드는 작업을 해야한다.   이번에는 대표적인 CNN 모델에 대해 살펴보도록 한다. 아래 이미지는 세계적인 규모의 이미지 인식 대회인 ILSVRC에서의 역대 우승자를 나타낸 그림이다.      2012년으로 넘어오면서 error rate(y)가 16.4%로 급격히 낮아짐을 확인할 수 있다. 2012년 우승을 차지한 AlexNet은 최초로 대회에서 CNN 기반 모델을 이용하여 우승을 하였다.   LeNet   LeNet은 최초의 CNN 기반 아키텍처이다. 1990년대에 대부분의 CNN에서의 성공적인 결과는 Yann LeCun가 만들었다. 그의 대표적인 architeture로는 zip code나 숫자를 인식하는 LeNet architeture가 있다.   AlexNet   AlexNet은 최초의 CNN기반 우승자 모델로, 이미지 인식 분야에서 deep learning의 가능성을 보여준 모델이라 평가된다. AlexNet에 대해서 자세히 알고싶다면 AlexNet에 대해 기술한 ImageNet Classification with Deep Convolutional Neural Networks 논문을 참고하면 된다.      위에서 설명한 대부분의 개념은 이 논문에서 더 자세히 확인할 수 있다.    아래 이미지는 AlexNet의 architecture를 보여준다.      위 그림을 text로 풀어서 표현하면 아래와 같다.      눈여겨 봐야할 점은 다음과 같다.      first use of ReLU   used Norm layers (not common anymore)            heavy data augmentation           dropout 0.5   batch size 128   SGD Momentum 0.9   Learning rate 1e-2, reduced by 10 manually when val accruacy plateaus   L2 weight decay 5e-4   7 CNN ensemble: 18.2% -&gt; 15.4%   ZFNet   ZFNet은 AlexNet 다음년도 ILSVRC 우승자로, AlexNet 모델에서 hyperparameters를 향상시켜 더 나은 error rate(16.4%→11.7%)를 도출하였다. AlextNet에서 바뀐 hyperparameters는 다음과 같다.      CONV1: (11×11 stride 4) → (7×7 stride 2)   CONV3, 4, 5: (384, 384, 256 filters) → (512, 1024, 512)      사실상 AlexNet과 큰 차이가 없어서 잘 언급되지는 않는다.    VGGNet   이전 CNN기반 모델(AlextNet, ZFNet)은 8 layers를 사용하였다. 하지만 VGGNet(2014년)를 기점으로 사용되는 layers의 개수가 점점 더 많아지기 시작했다. VGG는 19개의 layers를 이용하여 network를 더욱 깊게 만들 수 있다는 것에 대한 가능성을 열어주었다. 아래 그림은 VGGNet의 architecture를 그림으로 표현한 것이다.      VGGNet는 더 작은 filters로 더 깊은 networks를 구현했다는 특징이 있다.      8 layers(AlexNet) → 16~19 layers(VGGNet) : deeper networks   11×11, 5×5, 3×3과 같이 다양한 size의 CONV layer (AlexNet) → 오직 3×3 size, stride 1, pad 1를 갖는 CONV layer (VGGNet)   3×3 stride 2의 MAX POOL (AlexNet) → 2×2 stride 2의 MAX POOL (VGGNet)   11.7% top-5 error in ILSVRC’13 (ZFNet) → 7.3% top-5 error in ILSVRC’14 (VGGNet)   VGGNet이 더 작은 filters를 사용한 이유는 3개의 3×3 CONV layer (stride 1)가 7×7 CONV layer와 비슷한 receptive field를 갖기 때문이다. 그 이유는 아래의 그림을 보면 더 쉽게 체감할 수 있다.      3개의 3×3 CONV layers가 input data의 7×7 receptive field를 갖는다는 것을 쉽게 알 수 있다. 그렇다면 1 7×7 CONV filter와 3 3×3 CONV filters 중 어느 것이 더 효율적일까? layer 당 channel 개수가 C개라고 할 때 parameters 개수는, 3개의 3×3 CONV layer는 3개*(3*3 kernel size *C kernel depth *C output depth)=27C²개이고, 7×7 CONV layer는 1개*(7*7 kernel size *C kernel depth *C output depth)=49C²개이다. 따라서 차라리 더 작은 size의 CONV layers를 여러 개 사용하는 것이 더 큰 size의 CONV layer 하나를 사용하는 것보다 효율적이다. 한편, VGGNet의 parameters 개수는 아래와 같이 계산된다.      위 이미지에서 눈여겨 볼 점은, 대부분의 memory를 초반 CONV가 차지하고, 대부분의 parameters는 후반 FC가 차지한다는 점이다.   DL 모델에서 메모리는 주로 backpropagation에서 chain rule을 적용하기 위해 모든 중간 변수를 저장하는 데에 사용되거나, mini batch를 사용하여 모델을 학습시킬 때 더 많은 중간 activation을 저장하는 데에 사용된다.   backpropagationi을 할 때 CONV layer의 중간 결과를 저장하는 데에 주로 사용된다. 따라서  ???????????????//   VGGNet의 세부사항은 아래와 같다.      ILSVRC’14 2nd in classification, 1st in localization   Similar training procedure as Krizhevsky 2012   No Local Response Normalisation (LRN)   Use VGG16 or VGG19 (VGG19 only slightly better, more memory)   Use ensembles for best results   FC7 features generalize well to other tasks   References      https://excelsior-cjh.tistory.com/180   ","categories": ["ai-deepLearning"],
        "tags": ["Deep Learning","AIAS"],
        "url": "/ai-deeplearning/DL05/",
        "teaser": null
      },{
        "title": "[Summary] AugMix",
        "excerpt":"AugMix   배경   머신러닝 모델은 학습 데이터의 능력에 크게 의존한다. 따라서 학습 데이터와 테스트 데이터의 분포가 mismatched인 data shift 상황에서 accuracy가 감소되는 문제점이 있다.   이를 해결하려는 기존의 방식들은 크게 세 가지 문제점을 가지고 있다.      Data Memorizing : 테스트 데이터와 유사한 입력 데이터로 모델을 학습시키는 것에 불과해서, 완전히 새로운 데이터를 접하는 경우에는 역시 robust하지 못하다. 즉, corruptions을 학습하는 건 해당 corruptions에서만 robust할 뿐이지 완전히 새로운 corruption에 대해서는 일반화되지 못한다.   Trade-off : clean accuracy*와 robustness는 trade-off 관계이다. 따라서 robust해졌지만 clean accuracy는 감소하는 경우가 많다.            (참고) clean accuracy* : corrupted 되지 않은 원본 입력으로 테스트하여 얻은 정확도           Augmented Data with Degraded Performance : 다양성을 증가시키기위해 augmentation primitives를 chain에 직접적으로 구성한다. 즉, augmented data는 original data의 manifold에서 크게 벗어날 수 있다.       개념   AugMix는 image classifier에서 robustness와 uncertainty estimates를 향상시키는 기술로, 구현이 쉽고 학습 단계에서 본 적 없는 corruptions에도 모델이 강인해질 수 있도록 도와준다.   특징   AugMix는 이전의 data augmentation 기법과 차별되는 다음의 특징을 갖는다.      augmentation operations가 확률적으로 샘플링됨 ※ 이후 “알고리즘” 섹션에서 설명할 “1. Augmentation”과 관련 ※   다양한 augmented images를 생산하기 위해 augmentation chains이 layered됨 ※ 이후 “알고리즘” 섹션에서 설명할 “2. Mixing”과 관련 ※   Jensen-Shannon divergence를 consistency loss로 사용함으로써 다양한 augmentations에서도 일관성을 유지할 수 있음 ※ 이후 “알고리즘” 섹션에서 설명할 “3. Jensen-Shannon Divergence Consistency Loss”와 관련 ※   알고리즘   AugMix는 몇 가지 augmentation chains을 convex combinations하여 사용함으로써, 다양성과 일관성을 보장했다.      AugMix에서 “다양성”과 “일관성”이라는 말은 굉장히 중요하다. 기존이 augmentation 기법은 “다양성”과 “일관성”이 일종의 trade-off 관계였기 때문이다. 가령, 다양성을 주려고 다양한 연산을 적용하다보면 증강된 데이터가 원본 데이터와 크게 멀어지는 경우가 이에 해당한다. AugMix는 “다양성”과 “일관성”을 모두 지켰다는 점에서 의의가 크다.          AugMix 알고리즘은 크게 ①Augmentation과 ②Mixing, 그리고 ③Jensen-Shannon Divergence Consistency Loss로 나눌 수 있다.      ① Augmentation은 “다양성”과 관련이 있고, ②Mixing은 “일관성”과 관련이 있다.        1. Augmentation      Augmentation은 data augmentation의 ‘다양성‘을 보장하기 위한 과정이다.       3:\tFill x_aug with zeros \t\t... 5:\tfor i = 1, ..., k do 6:\t\tSample operations op1, op2, op3 ~ Ο 7:\t\tCompose operations with varying depth op12 = op2·op1 and op123 = op3·op2·op1 8:\t\tSample uniformly from one of these operations chain ~ {op1, op12, op123}   AutoAugment[개념]를 통해 주어진 데이터에 대해 최적의 augmentations 기법을 선택한다. 이후, 테스트 과정에서 사용할 ImageNet-C[개념] 데이터와 겹치는 augmentation 기법은 학습 과정에서 배제한다. (e.g., image noising과 image blurring 연산은 학습 과정에서 배제) 또한 augmented data가 기존 data manifold에서 벗어나지 않도록 각 augmentation의 강도를 적절히 설정한다. (e.g., rotation operations의 회전 강도, like 2º or -15º)   augmentation 과정은 augmentation chain을 랜덤으로 $k$(default:3)개 생성하는데, 이때 각 augmentation chain은 랜덤으로 선택 된 1~3가지 augmentation operations로 구성된다.       2. Mixing      Mixing은 augmented data가 original data의 manifold에서 벗어나지 않도록 ‘일관성‘을 보장하기 위한 과정이다.       4:\tSample mixing weights (w1, w2, ..., wk) ~ Dirichlet(α, α, ..., α) 5:\tfor i = 1, ..., k do \t\t... 9:\t\tx_aug += w_i · chain(x_orig) 10:\tend for 11:\tSample weight m ~ Beta(α, α) 12:\tInterpolate with rule x_augmix = m*x_orig + (1-m)*x_aug 13: return x_augmix 14: end function   Augmentations 과정을 통해 생성된 $k$개의 augmentation chains은 mixing 과정에서 결합된다. 알파 합성(Alpha Compositing)에 의한 mixing을 단순하기 구현하기 위해 여기서는 elementwise convex combinations를 사용한다. convex coefficients의 $k$차원 벡터는 $\\text{Dirichlet}{(\\alpha, \\alpha, ..., \\alpha)}$ 분포[개념]로부터 랜덤하게 샘플링되어 각 augmentations chains의 결과에 곱해진 후 하나의 결과로 합쳐진다. 이후, 하나로 합쳐진 결과는 $\\text{Beta}{(\\alpha, \\alpha)}$ 분포[개념]로부터 샘플링된 두 번째 random convex combination을 통해 original image와 합쳐진다.   즉, 최종적인 이미지는 연산 선택, 이러한 연산의 강도, augmentation chains의 길이, 그리고 mixing weights의 선택에 의해 여러 개의 랜덤한 sources가 통합된 결과이다.       3. Jensen-Shannon Divergence Consistency Loss   위의 과정을 통해 구한 augmentation scheme을 loss에 결합하여 신경망의 응답을 부드럽게 만든다. (mixing 과정을 통해) 이미지의 semantic content를 거의 보존했기 때문에 loss를 계산할 때 $x_{\\text{orig}}, x_{\\text{augmix1}}, x_{\\text{augmix2}}$는 유사한 정도로 모델에 내장된다.      🗣️ “유사한 정도로 모델에 내장된다”는 것은, $x_{\\text{augmix}}$을 $x_{\\text{orig}}$에 비해 특별히 적은 가중치로 반영하지 않겠다는 의미이다.    이를 위해, original sample $x_{\\text{orig}}$와 이것의 augmented 변형들의 posterior distributions에 대해 Jensen-Shannon divergence[개념]를 최소화한다. 즉, $p_{\\text{orig}}=\\hat{p}{(y \\mid x_{\\text{orig}})}$, $p_{\\text{augmix1}}=\\hat{p}{(y \\mid x_{\\text{augmix1}})}$, $p_{\\text{augmix2}}=\\hat{p}{(y \\mid x_{\\text{augmix2}})}$, 그리고 original loss $\\mathcal{L}$에 대해 최종적인 loss는 다음과 같다. \\(\\mathcal{L}(p_{\\text{orig}}, y) + λ\\textbf{JS}(p_{\\text{orig}}; p_{\\text{augmix1}}; p_{\\text{augmix2}}).\\)   \\[\\textbf{JS}(p_{\\text{orig}}, p_{\\text{augmix1}}, p_{\\text{augmix2}}) = \\frac{1}{3}(\\textbf{KL}[p_{\\text{orig}} \\mid\\mid M] + \\textbf{KL}[p_{\\text{augmix1}} \\mid\\mid M] + \\textbf{KL}[p_{\\text{augmix2}} \\mid\\mid M]).\\]  이러한 loss는 세 개의 $p_{\\text{orig}}$, $p_{\\text{augmix1}}$, $p_{\\text{augmix2}}$ 분포 중 어떤 한 샘플이 나타내는 샘플 분포의 동일성에 대한 평균 정보로 이해할 수 있다.      참고            &lt;a name=Jensen-ShannonDivergence&gt;Jensen-Shannon divergence&lt;/a&gt;                    KL Divergence에 비해 upper bounded.           모델을 다양한 입력 범위에 대해 ①stable, ②consistent, ③intensive하게 만들어준다 (Bachman et al., 2014; Zheng et al., 2016; Kannan et al., 2018).                       두 개의 augmentation chains을 사용하는 이유                    $\\textbf{JS}(p_{\\text{orig}}; p_{\\text{augmix1}})$은 성능을 향상시키기에 부족하다.           $\\textbf{JS}(p_{\\text{orig}}; p_{\\text{augmix1}}; p_{\\text{augmix2}}; p_{\\text{augmix3}})$은 성능을 너무 조금 향상시킨다. (‘굳이’인 느낌)                               Experiments   Datasets   실험에 사용되는 데이터셋은 크게 CIFAR-10, CIFAR-100, 그리고 ImageNet으로 나뉜다.      CIFAR (Krizhevsky &amp; Hinton, 2009)            32×32×3 color natural images       50,000 training &amp; 10,000 testing images       CIFAR-$N$ has $N$ categories           ImageNet (Deng et al., 2009)            1,000 classes       approximately 1.2 milion large-scale color images              실험에는 이러한 데이터셋의 변형된 버전도 사용되는데, 논문에 나와있는 용어는 아니지만, 여기서는 편의에 따라 “Dataset-C”와 “Dataset-P”로 나눈다.    Dataset-C      data shift 상황에서의 네트워크 성능을 평가하기 위해 원본 데이터셋에 corruption을 추가한 CIFAR-10-C, CIFAR-100-C, ImageNet-C (Hendrycks &amp; Dietterich, 2019) 데이터셋을 사용한다. 각 데이터셋은 세 가지 타입(blur, weather, and digital corruption), 총 15개의 노이즈가 각각 5가지의 강도로 추가된다.   Dataset-P      classifier의 예측 안정성(prediction stability)을 평가하기 위해 원본 데이터셋에 dataset-C에 비해 작은 약간의 변화를 추가한 CIFAR-10-P, CIFAR-100-P, ImageNet-P 데이터셋을 사용한다. 각 데이터셋은 영상 타입이며, 예를 들어 시간이 지남에 따라 밝기가 점점 증가한다는 식으로 변형되었다. 이를 통해 밝기가 증가함에 따라 영상 프레임간의 일관되지않거나 급격한 예측이 발생하는 지를 확인한다.       Metrics   네트워크 성능을 평가하기 위한 메트릭으로 본 논문에서는 세 가지를 제안한다.   mCE(Mean Corruption Error)      Clean Error : 변형되지 않은 clean data에 대한 일반적인 classification error   Corruption Error : 변형된 corrupted data에 대한 classification error            $E_{c,s}$ : corruption $c$가 $s(1≤s≤5)$의 강도로 주어졌을 때 error       $\\text{u}CE_c = \\sum_{s=1}^5{E_{c,s}}$ : unnormalized corruption error       $CE_c = \\sum_{s=1}^5{E_{c,s}}/\\sum_{s=1}^t{E_{c,s}^{\\text{AlexNet}}}$ : normalized corruption error       $\\text{m}CE$ : 15가지 corruptions에 대한 평균적인 error           mFP(mean Flip Probability), mFR(mean Flip Rate)   영상 프레임간의 예측 안정성과 관련된 Perturbation robustness를 측정하기 위해 flip probability를 계산한다. flip probability는 인접 프레임간에 미묘한 차이가 존재할 때, 그 예측이 달라지는(이를 “flipped”라고 표현한다) 확률을 의미한다. 10가지perturbation 종류에 대해 평균값을 계산한 것을 mean Flip Probability (mFP)라고 한다. ImageNet-C에서는, AlexNet의 flip probabilities를 normalization하여 mean Flip Rate(mFR)을 계산한다.   CE(Calibration Error)   모델의 uncertainty estimates를 평가하기 위해 모델의 miscalibration을 측정한다. 여기서 “calibrated”는 classifiers가 출력한 정확도를 신뢰할 수 있는 능력을 의미한다. 예를 들어, calibrated model이라면, 70%의 정확도로 예측했을 때 그 신뢰도 역시 70%여야 한다.           idealized RMS Calibration Error : $\\sqrt{E_C[(P(Y=\\hat{Y} \\mid C=c) - c)^2]}$              주어진 신뢰도 c에서의 accuracy와 실제 신뢰도 c의 squared difference를 의미한다.                CIFAR-10 and CIFAR-100에 대한 실험   Training Setup      Learning rate : initial learning rate = 0.1, which decays following a cosine learning rate   Input images : All input images are pre-processed with standard random left-right ﬂipping and cropping prior to any augmentations.   AUGMIX parameters : We do not change AUGMIX parameters across CIFAR-10 and CIFAR-100 experiments for consistency.   Epochs : The All Convolutional Network and Wide ResNet train for 100 epochs, and the DenseNet and ResNeXt require 200 epochs for convergence.   Optimization : We optimize with stochastic gradient descent using Nesterov momentum.   Weight decay : We use a weight decay of 0.0001 for Mixup and 0.0005 otherwise.       Result①: Corruption Error   CIFAR-10-C와 CIFAR-100-C에 대해 여러 augmentation 기법 적용에 따른 corruption error 결과는 다음과 같다. Figure 5는 CIFAR-10-C에 대한 Corruption Error를 그래프화한 것이고, Table 1은 결과를 표로 나타낸 것이다.         AUGMIX는 비교한 여러 augmentation 중 가장 낮은 Corruption Error를 달성했다.         Standard에서            비교적 예측이 힘든 corruption 종류 : Gaussian Noise, Glass Blur, Impulse Noise, Shot Noise       비교적 예측이 쉬운 corruption 종류 :Brightness, Fog           AugMix에서            비교적 예측이 힘든 corruption 종류 : Gaussian Noise, Glass Blur, Pixelate, Shot Noise       비교적 예측이 쉬운 corruption 종류 : Brightness, Defocus Blur, Zoom Blur, Motion Blur, Fog              위 그림에서 노란색 막대는 Standard와 AugMix 간의 corruption error 차이를 시각화한 것이다. 이때 특히나 그 차이가 심한 부분을 검은색 윤곽선으로 표현했다.      corruption error에서 AugMix가 비교적 좋은 결과를 야기하는 corruption 종류 : Gaussian Noise, Shot Noise, Impulse Noise, Glass Blur   corruption error에서 AugMix가 성능에 큰 영향을 주지 못하는 corruption 종류 : Fog, Brightness                  전체적으로 AugMix는 corruption error를 대략 11~17.8%정도 감소시킨다.                  (Standard와 AugMix 두 결과에 의하면) Noise corruptions와 Glass Blur, 그리고 Pixelate는 예측이 어렵고, Brightness와 Fog는 예측이 쉽다.         하지만 Standard와 비교했을 때, AugMix에서는 corruption 종류에 따른 Corruption Error 차이가 덜하다. 즉, AugMix일 때 네트워크가 더 robust해졌다.            Result②: Flip Probability &amp; Calibration Error              (Figure 6의 좌측 자료로부터) AugMix는 비교적 perturbation에 robust함을 확인할 수 있다.     (Figure 6의 우측 자료로부터) AugMix가 clean data와 corrupted data간의 RMS Calibration Error 차이를 효과적으로 감소시켰음을 확인할 수 있다.                 CIFAR-10 Clean Error                Adversarial training에서 현저히 높은 에러를 보여준다.         그 외 기법간의 에러 차이는 근소하다.                 CIFAR-10-P mean Flip Probability                Adversarial training과 AugMix에서 낮은 에러를 보여준다.                   ∴ AugMix는 clean error는 최대한 보존하면서도 mean Flip Probability는 낮춰주는 데에 효과적이다.    ImageNet에 대한 실험   Baselines   큰 규모의 classes를 갖는 ImageNet에서의 성능 비교를 위해, 다음의 기준으로 기존의 방법과 AugMix의 성능을 비교할 대상을 선정하였다.      Cutout은 ImageNet 규모에서 효과적이라는 것이 증명되지 않았으므로 ImageNet-C에서 성능이 검증된 Stylized ImageNet을 비교 대상으로 결정했다.   Patch Uniform은 랜덤으로 선택된 이미지 영역에 uniform noise를 주입한다는 것을 제외하고는 Cutout과 유사하므로 비교 대상으로 삼았다. 참고로 원논문에서는 학습시 Gaussian noise를 사용하나, 테스트 대상 데이터와 중복되므로 이를 uniform noise로 바꾸어 성능을 평가했다. 대략 30개가 넘는 hyperparameters를 수정했다.   AutoAugment는 높은 성능을 달성하는 augmentation policy를 찾아준다. 이때 출력 결과로 나온 최적의 augmentations 중 테스트 데이터와 겹치는 corruptions은 제거했으므로 논문에서는 AutoAugment*****로 표기한다.   Random AutoAugment는 AutoAugment*을 사용하여 랜덤으로 샘플링된 augmentation policy를 갖는다. AutoAugment와 비교했을 때 더 적은 계산으로 더 다양한 augmentation를 제공한다는 특징이 있다.   MaxBlurPooling은 최근에 제안된 architectural modification으로, pooling 결과를 smooth하게 만들어준다.   Stylized ImageNet (SIN)은 원본 ImageNet 데이터에 추가적으로 style transfer가 적용된 ImageNet을 사용하여 모델을 학습시키는 기술이다   여기에 추가적으로 SIN과 AugMix를 결합한 것 역시 비교 대상으로 삼았다.   Training Setup   학습에는 Goyal et al. (2017)의 표준 training scheme을 따르는 ResNet-50이 사용된다.      learning rate : we linearly scale the learning rate with the batch size, and use a learning rate warm-up for the ﬁrst 5 epochs.   **epochs **: AutoAugment and AUGMIX train for 180 epochs.   Preprocessing : All input images are ﬁrst pre-processed with standard random cropping horizontal mirroring.   Result①: Corruption Error         AugMix는 standard와 비교했을 때 Clean Error는 23.9→22.4%로 1.5% 감소시킴과 동시에 mean Corruption Error는 80.6→68.4%로 12.2% 감소시켰다. 기존의 augmentation 기법들이 clean error와 corruption error가 trade-off 관계가 있었다는 점을 감안할 때 의미있는 결과이다.   AugMix는 다른 augmentation 기법과 결합하여 사용하기에도 좋게 만들어졌다. 여기서는 SIN(Stylized ImageNet) 기법과 결합하여 사용해 본 결과, AugMix만을 사용했을 때 보다 더 좋은 성능을 거두었음을 보여주었다.   Result②: Flip Rate              Flip Rate는 Perturbation robustness 측정 지표이다. ImageNet-P에 각 augmentation 기법을 테스트한 결과, 가장 낮은 mean Flip Rate를 달성하였음을 확인할 수 있다.            하지만 테스트 데이터에 적용된 corruption의 종류에 따라 다른 Flip Rate를 보여주는데, noise corrruption에 대해서는 Patch Uniform 기법이 더 성능이 좋음을 알 수 있다.              Patch Uniform은 랜덤으로 선택된 이미지 영역에 uniform noise를 주입”하는 augmentation 기법이다. 저자는 모든 종류의 augmentation 기법에서 테스트단에서 마주칠 수 있는 종류의 corruptions은 학습에서 배제했는데, 여기서는 Gaussian이나 Shot noise가 아니라는 이유에서 허용된 것으로 보인다. 사실상, 다른 분포를 갖는 노이즈를 학습한 것이므로 성능이 특히 더 좋아진 게 아닐까 싶다. 하지만 이는 역시 기존의 Corruption Memorizing한다는 문제점에 해당한다.                    참고자료   AutoAugment      지나친 data augmentation은 manifold intrusion으로 이어질 수 있다. 따라서 이전까지는 데이터 도메인에 따라 어떤 augmentation을 적용할지 선택하는 과정이 수동으로 이루어졌다 (e.g., “새의 종류를 판별하는 문제에서는 histogram color swapping과 같은 augmentation을 사용하면 안되겠군”). AutoAugment는 주어진 데이터에서의 효과적인 augmentation을 찾는 과정을 '**자동으로**' 수행하는 것을 목표로 한다. 이를 위해 탐색공간(search space)을 정의하여 최적화된 augmentation 기법을 찾는다. 이때 탐색공간은 하나의 augmentation policy는 5개의 sub-policies로 구성하고, 각 sub-policy는 2개의 image operation으로 구성하여 정의된다. 각 sub-policy가 생성하는 augmentation 기법은 다음과 같다. \\(\\text{1개의 augmentation policy}\\\\ = \\text{2개의 이미지 연산}\\\\ =\\text{이미지 연산}×\\text{이미지 연산}\\\\ =(\\text{augmentation 기법} × \\text{확률} × \\text{강도})^2\\) AutoAugment는 규모가 작은 데이터셋에서 특히나 더 뛰어난 성능을 보여주지만, 높은 계산 복잡도로 인해 일반적인 연구 환경에 적용이 어렵다는 단점이 있다.      AugMix에서는 실험 데이터에 적용할 적절한 augmentation 기법을 뽑아내기위해 AutoAugment를 사용했다. 이후, 뽑아낸 최적화된 augmentation 기법들 중 ImageNet-C와 겹치는 corruptions에 대해서는 배제하고 실험했다.        Dirichlet &amp; Beta Distribution   Dirichlet distribution과 Beta distribution은 주로 image classification에서 사용되는 parametric distribution*이다.      (참고) parametric distribution* : 모수를 가정하여 추정하는 분포. 데이터가 적어도 잘 동작한다는 장점이 있지만, 모수에 영향을 받으므로 데이터에 따른 분포 업데이트 반영이 어렵다는 단점이 있다.   Beta distribution는 univariate 특성을 가지며 매개변수 $\\alpha$, $\\beta$에 대해 [0,1] 범위에서 정의되는 연속확률 분포이다.      \\[f(x; α, β) = \\frac{\\Gamma{(\\alpha+\\beta)}}{\\Gamma{(\\alpha)}\\Gamma{(\\beta)}}x^{\\alpha-1}(1-x)^{\\beta-1}\\\\ \\Gamma{(n)} = (n-1)!\\]  Dirichlet distribution는 multivariate 특성을 가지며 다음의 연속확률 분포이다.   \\[B(\\alpha) = \\frac{\\Pi_{i=1}^k{\\Gamma{(\\alpha_i)}}}{\\Gamma{(\\sum_{i=1}^k{\\alpha_i})}}\\]          References      Hendrycks, Dan, et al. “Augmix: A simple data processing method to improve robustness and uncertainty.” arXiv preprint arXiv:1912.02781 (2019).        “google-research/augmix”, github, 2022년 03월 22일 접속, https://github.com/google-research/augmix       AutoAugment            “Fast AutoAugment/1.데이터 어그먼테이션 연구 동향을 소개합니다.”, kakaobrain 블로그, 2022년 03월 02일 접속, https://www.kakaobrain.com/blog/64           Dirichlet &amp; Beta Distribution            “Dirichlet distribution(Dirichlet prior)를 사용하는 이유”, donghwa-kim.github.io, 2019년 04월 02일 수정, 2022년 03월 02일 접속, https://donghwa-kim.github.io/distributions.html       “이항분포, 다항분포, 베타분포, 디리클레분포”, ratsgo’s blog, 2017년 05월 28일 수정, 2022년 03월 02일 접속, https://ratsgo.github.io/statistics/2017/05/28/binomial/                  ","categories": ["ai-computerVision"],
        "tags": ["AugMix","Augmentation"],
        "url": "/ai-computervision/AugMix/",
        "teaser": null
      }]
