<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dazory blog</title>
    <description>A personal blog to keep track of my research, ideas, and concepts.</description>
    <link>https://dazory.github.io//</link>
    <atom:link href="https://dazory.github.io//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 06 Mar 2022 15:21:21 +0900</pubDate>
    <lastBuildDate>Sun, 06 Mar 2022 15:21:21 +0900</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>[Summary] AugMix</title>
        <description>&lt;h1 id=&quot;augmix&quot;&gt;AugMix&lt;/h1&gt;

&lt;h2 id=&quot;배경&quot;&gt;배경&lt;/h2&gt;

&lt;p&gt;머신러닝 모델은 학습 데이터의 능력에 크게 의존한다. 따라서 학습 데이터와 테스트 데이터의 분포가 mismatched인 data shift 상황에서 accuracy가 감소되는 문제점이 있다.&lt;/p&gt;

&lt;p&gt;이를 해결하려는 기존의 방식들은 크게 세 가지 문제점을 가지고 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Data Memorizing&lt;/strong&gt; : 테스트 데이터와 유사한 입력 데이터로 모델을 학습시키는 것에 불과해서, 완전히 새로운 데이터를 접하는 경우에는 역시 robust하지 못하다. 즉, corruptions을 학습하는 건 해당 corruptions에서만 robust할 뿐이지 완전히 새로운 corruption에 대해서는 일반화되지 못한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Trade-off&lt;/strong&gt; : clean accuracy*와 robustness는 trade-off 관계이다. 따라서 robust해졌지만 clean accuracy는 감소하는 경우가 많다.
    &lt;ul&gt;
      &lt;li&gt;(참고) clean accuracy* : corrupted 되지 않은 원본 입력으로 테스트하여 얻은 정확도&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Augmented Data with Degraded Performance&lt;/strong&gt; : 다양성을 증가시키기위해 augmentation primitives를 chain에 직접적으로 구성한다. 즉, augmented data는 original data의 manifold에서 크게 벗어날 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;개념&quot;&gt;개념&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;AugMix&lt;/strong&gt;는 image classifier에서 robustness와 uncertainty estimates를 향상시키는 기술로, 구현이 쉽고 학습 단계에서 본 적 없는 corruptions에도 모델이 강인해질 수 있도록 도와준다.&lt;/p&gt;

&lt;h3 id=&quot;특징&quot;&gt;특징&lt;/h3&gt;

&lt;p&gt;AugMix는 이전의 data augmentation 기법과 차별되는 다음의 특징을 갖는다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;augmentation operations가 확률적으로 샘플링됨&lt;br /&gt;※ 이후 “알고리즘” 섹션에서 설명할 “1. Augmentation”과 관련 ※&lt;/li&gt;
  &lt;li&gt;다양한 augmented images를 생산하기 위해 augmentation chains이 layered됨&lt;br /&gt;※ 이후 “알고리즘” 섹션에서 설명할 “2. Mixing”과 관련 ※&lt;/li&gt;
  &lt;li&gt;Jensen-Shannon divergence를 consistency loss로 사용함으로써 다양한 augmentations에서도 일관성을 유지할 수 있음&lt;br /&gt;※ 이후 “알고리즘” 섹션에서 설명할 “3. Jensen-Shannon Divergence Consistency Loss”와 관련 ※&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;알고리즘&quot;&gt;알고리즘&lt;/h3&gt;

&lt;p&gt;AugMix는 &lt;u&gt;몇 가지 augmentation chains을 convex combinations&lt;/u&gt;하여 사용함으로써, 다양성과 일관성을 보장했다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;AugMix에서 “다양성”과 “일관성”이라는 말은 굉장히 중요하다. 기존이 augmentation 기법은 “다양성”과 “일관성”이 일종의 trade-off 관계였기 때문이다. 가령, 다양성을 주려고 다양한 연산을 적용하다보면 증강된 데이터가 원본 데이터와 크게 멀어지는 경우가 이에 해당한다. AugMix는 “다양성”과 “일관성”을 모두 지켰다는 점에서 의의가 크다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220302184953251.png&quot; alt=&quot;image-20220302184953251&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220302184841118.png&quot; alt=&quot;image-20220302184841118&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AugMix 알고리즘은 크게 &lt;strong&gt;①Augmentation&lt;/strong&gt;과 &lt;strong&gt;②Mixing&lt;/strong&gt;, 그리고 &lt;strong&gt;③Jensen-Shannon Divergence Consistency Loss&lt;/strong&gt;로 나눌 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;① Augmentation은 “다양성”과 관련이 있고, ②Mixing은 “일관성”과 관련이 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1-augmentation&quot;&gt;1. Augmentation&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;Augmentation은 data augmentation의 ‘&lt;strong&gt;다양성&lt;/strong&gt;‘을 보장하기 위한 과정이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303101035678.png&quot; alt=&quot;image-20220303101035678&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;3:	Fill x_aug with zeros
		...
5:	for i = 1, ..., k do
6:		Sample operations op1, op2, op3 ~ Ο
7:		Compose operations with varying depth op12 = op2·op1 and op123 = op3·op2·op1
8:		Sample uniformly from one of these operations chain ~ {op1, op12, op123}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;AutoAugment&lt;sup&gt;&lt;a href=&quot;#AutoAugment&quot;&gt;[개념]&lt;/a&gt;&lt;/sup&gt;를 통해 주어진 데이터에 대해 &lt;u&gt;최적의 augmentations 기법을 선택&lt;/u&gt;한다. 이후, 테스트 과정에서 사용할 &lt;u&gt;ImageNet-C&lt;/u&gt;&lt;sup&gt;&lt;a href=&quot;#ImageNetC&quot;&gt;[개념]&lt;/a&gt;&lt;/sup&gt;&lt;u&gt; 데이터와 겹치는 augmentation 기법은 학습 과정에서 배제&lt;/u&gt;한다. (e.g., image noising과 image blurring 연산은 학습 과정에서 배제) 또한 augmented data가 기존 data manifold에서 벗어나지 않도록 &lt;u&gt;각 augmentation의 강도를 적절히 설정&lt;/u&gt;한다. (e.g., rotation operations의 회전 강도, like 2º or -15º)&lt;/p&gt;

&lt;p&gt;augmentation 과정은 &lt;u&gt;augmentation chain을 랜덤으로 $k$(default:3)개 생성&lt;/u&gt;하는데, 이때 각 augmentation chain은 랜덤으로 선택 된 1~3가지 augmentation operations로 구성된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;2-mixing&quot;&gt;2. Mixing&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mixing은 augmented data가 original data의 manifold에서 벗어나지 않도록 ‘&lt;strong&gt;일관성&lt;/strong&gt;‘을 보장하기 위한 과정이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303102414162.png&quot; alt=&quot;image-20220303102414162&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;4:	Sample mixing weights (w1, w2, ..., wk) ~ Dirichlet(α, α, ..., α)
5:	for i = 1, ..., k do
		...
9:		x_aug += w_i · chain(x_orig)
10:	end for
11:	Sample weight m ~ Beta(α, α)
12:	Interpolate with rule x_augmix = m*x_orig + (1-m)*x_aug
13: return x_augmix
14: end function
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Augmentations 과정을 통해 &lt;strong&gt;생성된 $k$개의 augmentation chains은 mixing 과정에서 결합된다&lt;/strong&gt;. 알파 합성(Alpha Compositing)에 의한 mixing을 단순하기 구현하기 위해 여기서는 elementwise convex combinations를 사용한다. &lt;u&gt;convex coefficients의 $k$차원 벡터는 $\text{Dirichlet}{(\alpha, \alpha, ..., \alpha)}$ 분포&lt;/u&gt;&lt;sup&gt;&lt;a href=&quot;#Dirichlet_BetaDistribution&quot;&gt;[개념]&lt;/a&gt;&lt;/sup&gt;&lt;u&gt;로부터 랜덤하게 샘플링되어 각 augmentations chains의 결과에 곱해진 후 하나의 결과로 합쳐진다&lt;/u&gt;. 이후, &lt;u&gt;하나로 합쳐진 결과는 $\text{Beta}{(\alpha, \alpha)}$ 분포&lt;/u&gt;&lt;sup&gt;&lt;a href=&quot;#Dirichlet_BetaDistribution&quot;&gt;[개념]&lt;/a&gt;&lt;/sup&gt;&lt;u&gt;로부터 샘플링된 두 번째 random convex combination을 통해 original image와 합쳐진다&lt;/u&gt;.&lt;/p&gt;

&lt;p&gt;즉, 최종적인 이미지는 &lt;u&gt;연산 선택&lt;/u&gt;, 이러한 &lt;u&gt;연산의 강도&lt;/u&gt;, &lt;u&gt;augmentation chains의 길이&lt;/u&gt;, 그리고 &lt;u&gt;mixing weights의 선택&lt;/u&gt;에 의해 여러 개의 랜덤한 sources가 통합된 결과이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;3-jensen-shannon-divergence-consistency-loss&quot;&gt;3. Jensen-Shannon Divergence Consistency Loss&lt;/h4&gt;

&lt;p&gt;위의 과정을 통해 구한 augmentation scheme을 loss에 결합하여 신경망의 응답을 부드럽게 만든다. (mixing 과정을 통해) 이미지의 semantic content를 거의 보존했기 때문에 loss를 계산할 때 $x_{\text{orig}}, x_{\text{augmix1}}, x_{\text{augmix2}}$는 유사한 정도로 모델에 내장된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;🗣️ “유사한 정도로 모델에 내장된다”는 것은, $x_{\text{augmix}}$을 $x_{\text{orig}}$에 비해 특별히 적은 가중치로 반영하지 않겠다는 의미이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이를 위해, original sample $x_{\text{orig}}$와 이것의 augmented 변형들의 posterior distributions에 대해 Jensen-Shannon divergence&lt;sup&gt;&lt;a href=&quot;#Jensen-ShannonDivergence&quot;&gt;[개념]&lt;/a&gt;&lt;/sup&gt;를 최소화한다. 즉, $p_{\text{orig}}=\hat{p}{(y \mid x_{\text{orig}})}$, $p_{\text{augmix1}}=\hat{p}{(y \mid x_{\text{augmix1}})}$, $p_{\text{augmix2}}=\hat{p}{(y \mid x_{\text{augmix2}})}$, 그리고 original loss $\mathcal{L}$에 대해 최종적인 loss는 다음과 같다.
\(\mathcal{L}(p_{\text{orig}}, y) + λ\textbf{JS}(p_{\text{orig}}; p_{\text{augmix1}}; p_{\text{augmix2}}).\)&lt;/p&gt;

\[\textbf{JS}(p_{\text{orig}}, p_{\text{augmix1}}, p_{\text{augmix2}}) = \frac{1}{3}(\textbf{KL}[p_{\text{orig}} \mid\mid M] + \textbf{KL}[p_{\text{augmix1}} \mid\mid M] + \textbf{KL}[p_{\text{augmix2}} \mid\mid M]).\]

&lt;p&gt;이러한 loss는 세 개의 $p_{\text{orig}}$, $p_{\text{augmix1}}$, $p_{\text{augmix2}}$ 분포 중 어떤 한 샘플이 나타내는 샘플 분포의 동일성에 대한 평균 정보로 이해할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;참고
    &lt;ul&gt;
      &lt;li&gt;&amp;lt;a name=Jensen-ShannonDivergence&amp;gt;Jensen-Shannon divergence&amp;lt;/a&amp;gt;
        &lt;ul&gt;
          &lt;li&gt;KL Divergence에 비해 upper bounded.&lt;/li&gt;
          &lt;li&gt;모델을 다양한 입력 범위에 대해 ①stable, ②consistent, ③intensive하게 만들어준다 (Bachman et al., 2014; Zheng et al., 2016; Kannan et al., 2018).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;두 개의 augmentation chains을 사용하는 이유
        &lt;ul&gt;
          &lt;li&gt;$\textbf{JS}(p_{\text{orig}}; p_{\text{augmix1}})$은 성능을 향상시키기에 부족하다.&lt;/li&gt;
          &lt;li&gt;$\textbf{JS}(p_{\text{orig}}; p_{\text{augmix1}}; p_{\text{augmix2}}; p_{\text{augmix3}})$은 성능을 너무 조금 향상시킨다. (‘굳이’인 느낌)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;실험에 사용되는 데이터셋은 크게 CIFAR-10, CIFAR-100, 그리고 ImageNet으로 나뉜다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CIFAR&lt;/strong&gt; (Krizhevsky &amp;amp; Hinton, 2009)
    &lt;ul&gt;
      &lt;li&gt;32×32×3 color natural images&lt;/li&gt;
      &lt;li&gt;50,000 training &amp;amp; 10,000 testing images&lt;/li&gt;
      &lt;li&gt;CIFAR-$N$ has $N$ categories&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ImageNet&lt;/strong&gt; (Deng et al., 2009)
    &lt;ul&gt;
      &lt;li&gt;1,000 classes&lt;/li&gt;
      &lt;li&gt;approximately 1.2 milion large-scale color images&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;실험에는 이러한 데이터셋의 변형된 버전도 사용되는데, 논문에 나와있는 용어는 아니지만, 여기서는 편의에 따라 “Dataset-C”와 “Dataset-P”로 나눈다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;dataset-c&quot;&gt;&lt;a name=&quot;ImageNetC&quot;&gt;Dataset-C&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/imagenet-c.png&quot; alt=&quot;img&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;u&gt;data shift 상황에서의 네트워크 성능을 평가&lt;/u&gt;하기 위해 원본 데이터셋에 corruption을 추가한 &lt;strong&gt;CIFAR-10-C&lt;/strong&gt;, &lt;strong&gt;CIFAR-100-C&lt;/strong&gt;, &lt;strong&gt;ImageNet-C&lt;/strong&gt; (Hendrycks &amp;amp; Dietterich, 2019) 데이터셋을 사용한다. 각 데이터셋은 &lt;u&gt;세 가지 타입(blur, weather, and digital corruption)&lt;/u&gt;, 총 &lt;u&gt;15개의 노이즈&lt;/u&gt;가 각각 &lt;u&gt;5가지의 강도&lt;/u&gt;로 추가된다.&lt;/p&gt;

&lt;h4 id=&quot;dataset-p&quot;&gt;Dataset-P&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/translate.gif&quot; alt=&quot;img&quot; style=&quot;width:200px;&quot; /&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/tilt.gif&quot; alt=&quot;img&quot; style=&quot;width:200px;&quot; /&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/spatter.gif&quot; alt=&quot;img&quot; style=&quot;width:200px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;u&gt;classifier의 예측 안정성(prediction stability)을 평가&lt;/u&gt;하기 위해 원본 데이터셋에 dataset-C에 비해 작은 약간의 변화를 추가한 &lt;strong&gt;CIFAR-10-P&lt;/strong&gt;, &lt;strong&gt;CIFAR-100-P&lt;/strong&gt;, &lt;strong&gt;ImageNet-P&lt;/strong&gt; 데이터셋을 사용한다. 각 데이터셋은 &lt;u&gt;영상&lt;/u&gt; 타입이며, 예를 들어 시간이 지남에 따라 밝기가 점점 증가한다는 식으로 변형되었다. 이를 통해 밝기가 증가함에 따라 &lt;u&gt;영상 프레임간의 일관되지않거나 급격한 예측이 발생하는 지를 확인&lt;/u&gt;한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;p&gt;네트워크 성능을 평가하기 위한 메트릭으로 본 논문에서는 세 가지를 제안한다.&lt;/p&gt;

&lt;h4 id=&quot;mcemean-corruption-error&quot;&gt;mCE(Mean Corruption Error)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Clean Error&lt;/strong&gt; : 변형되지 않은 clean data에 대한 일반적인 classification error&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Corruption Error&lt;/strong&gt; : 변형된 corrupted data에 대한 classification error
    &lt;ul&gt;
      &lt;li&gt;$E_{c,s}$ : corruption $c$가 $s(1≤s≤5)$의 강도로 주어졌을 때 error&lt;/li&gt;
      &lt;li&gt;$\text{u}CE_c = \sum_{s=1}^5{E_{c,s}}$ : unnormalized corruption error&lt;/li&gt;
      &lt;li&gt;$CE_c = \sum_{s=1}^5{E_{c,s}}/\sum_{s=1}^t{E_{c,s}^{\text{AlexNet}}}$ : normalized corruption error&lt;/li&gt;
      &lt;li&gt;$\text{m}CE$ : 15가지 corruptions에 대한 평균적인 error&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;mfpmean-flip-probability-mfrmean-flip-rate&quot;&gt;mFP(mean Flip Probability), mFR(mean Flip Rate)&lt;/h4&gt;

&lt;p&gt;&lt;u&gt;영상 프레임간의 예측 안정성과 관련된 Perturbation robustness를 측정&lt;/u&gt;하기 위해 &lt;strong&gt;flip probability&lt;/strong&gt;를 계산한다. flip probability는 인접 프레임간에 미묘한 차이가 존재할 때, 그 예측이 달라지는(이를 “flipped”라고 표현한다) 확률을 의미한다. 10가지perturbation 종류에 대해 평균값을 계산한 것을 &lt;strong&gt;mean Flip Probability (mFP)&lt;/strong&gt;라고 한다. ImageNet-C에서는, AlexNet의 flip probabilities를 normalization하여 &lt;strong&gt;mean Flip Rate(mFR)&lt;/strong&gt;을 계산한다.&lt;/p&gt;

&lt;h4 id=&quot;cecalibration-error&quot;&gt;CE(Calibration Error)&lt;/h4&gt;

&lt;p&gt;&lt;u&gt;모델의 uncertainty estimates를 평가&lt;/u&gt;하기 위해 모델의 &lt;u&gt;miscalibration&lt;/u&gt;을 측정한다. 여기서 “calibrated”는 classifiers가 출력한 정확도를 신뢰할 수 있는 능력을 의미한다. 예를 들어, calibrated model이라면, 70%의 정확도로 예측했을 때 그 신뢰도 역시 70%여야 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;idealized RMS Calibration Error : $\sqrt{E_C[(P(Y=\hat{Y} \mid C=c) - c)^2]}$&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;주어진 신뢰도 c에서의 accuracy와 실제 신뢰도 c의 squared difference를 의미한다.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cifar-10-and-cifar-100에-대한-실험&quot;&gt;CIFAR-10 and CIFAR-100에 대한 실험&lt;/h3&gt;

&lt;h4 id=&quot;training-setup&quot;&gt;Training Setup&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Learning rate&lt;/strong&gt; : &lt;u&gt;initial learning rate = 0.1&lt;/u&gt;, which decays following &lt;u&gt;a cosine learning rate&lt;/u&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Input images&lt;/strong&gt; : All input images are pre-processed with &lt;u&gt;standard random left-right ﬂipping and cropping prior to any augmentations&lt;/u&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AUGMIX parameters&lt;/strong&gt; : We &lt;u&gt;do not change AUGMIX parameters&lt;/u&gt; across CIFAR-10 and CIFAR-100 experiments for consistency.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Epochs&lt;/strong&gt; : The All Convolutional Network and Wide ResNet train for &lt;u&gt;100 epochs&lt;/u&gt;, and the DenseNet and ResNeXt require &lt;u&gt;200 epochs&lt;/u&gt; for convergence.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt; : We optimize with &lt;u&gt;stochastic gradient descent using Nesterov momentum&lt;/u&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Weight decay&lt;/strong&gt; : We use &lt;u&gt;a weight decay of 0.0001 for Mixup and 0.0005 otherwise&lt;/u&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;result-corruption-error&quot;&gt;Result①: Corruption Error&lt;/h4&gt;

&lt;p&gt;CIFAR-10-C와 CIFAR-100-C에 대해 여러 augmentation 기법 적용에 따른 corruption error 결과는 다음과 같다. Figure 5는 CIFAR-10-C에 대한 Corruption Error를 그래프화한 것이고, Table 1은 결과를 표로 나타낸 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303145123150.png&quot; alt=&quot;image-20220303145123150&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303145916172.png&quot; alt=&quot;image-20220303145916172&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AUGMIX는 비교한 여러 augmentation 중 가장 낮은 Corruption Error를 달성했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303151502989.png&quot; alt=&quot;image-20220303151502989&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Standard에서
    &lt;ul&gt;
      &lt;li&gt;비교적 예측이 힘든 corruption 종류 : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gaussian Noise&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Glass Blur&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Impulse Noise&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Shot Noise&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;비교적 예측이 쉬운 corruption 종류 :&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Brightness&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fog&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AugMix에서
    &lt;ul&gt;
      &lt;li&gt;비교적 예측이 힘든 corruption 종류 : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gaussian Noise&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Glass Blur&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pixelate&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Shot Noise&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;비교적 예측이 쉬운 corruption 종류 : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Brightness&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Defocus Blur&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Zoom Blur&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Motion Blur&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fog&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303151553607.png&quot; alt=&quot;image-20220303151553607&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 노란색 막대는 Standard와 AugMix 간의 corruption error 차이를 시각화한 것이다. 이때 특히나 그 차이가 심한 부분을 검은색 윤곽선으로 표현했다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;corruption error에서 AugMix가 비교적 좋은 결과를 야기하는 corruption 종류 : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gaussian Noise&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Shot Noise&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Impulse Noise&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Glass Blur&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;corruption error에서 AugMix가 성능에 큰 영향을 주지 못하는 corruption 종류 : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fog&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Brightness&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;전체적으로 AugMix는 corruption error를 대략 11~17.8%정도 감소시킨다.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;(Standard와 AugMix 두 결과에 의하면) Noise corruptions와 Glass Blur, 그리고 Pixelate는 예측이 어렵고, Brightness와 Fog는 예측이 쉽다.&lt;/p&gt;

      &lt;p&gt;하지만 Standard와 비교했을 때, AugMix에서는 corruption 종류에 따른 Corruption Error 차이가 덜하다. 즉, AugMix일 때 네트워크가 더 robust해졌다.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;result-flip-probability--calibration-error&quot;&gt;Result②: Flip Probability &amp;amp; Calibration Error&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303154039743.png&quot; alt=&quot;image-20220303154039743&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;(Figure 6의 좌측 자료로부터) AugMix는 비교적 perturbation에 robust함을 확인할 수 있다.&lt;/li&gt;
    &lt;li&gt;(Figure 6의 우측 자료로부터) AugMix가 clean data와 corrupted data간의 RMS Calibration Error 차이를 효과적으로 감소시켰음을 확인할 수 있다.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220303155419448.png&quot; alt=&quot;image-20220303155419448&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;CIFAR-10 Clean Error&lt;/strong&gt;
      &lt;ul&gt;
        &lt;li&gt;Adversarial training에서 현저히 높은 에러를 보여준다.&lt;/li&gt;
        &lt;li&gt;그 외 기법간의 에러 차이는 근소하다.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;CIFAR-10-P mean Flip Probability&lt;/strong&gt;
      &lt;ul&gt;
        &lt;li&gt;Adversarial training과 AugMix에서 낮은 에러를 보여준다.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;∴ AugMix는 clean error는 최대한 보존하면서도 mean Flip Probability는 낮춰주는 데에 효과적이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;imagenet에-대한-실험&quot;&gt;ImageNet에 대한 실험&lt;/h3&gt;

&lt;h4 id=&quot;baselines&quot;&gt;Baselines&lt;/h4&gt;

&lt;p&gt;큰 규모의 classes를 갖는 ImageNet에서의 성능 비교를 위해, 다음의 기준으로 기존의 방법과 AugMix의 성능을 비교할 대상을 선정하였다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cutout은 ImageNet 규모에서 효과적이라는 것이 증명되지 않았으므로 ImageNet-C에서 성능이 검증된 &lt;strong&gt;Stylized ImageNet&lt;/strong&gt;을 비교 대상으로 결정했다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Patch Uniform&lt;/strong&gt;은 랜덤으로 선택된 이미지 영역에 uniform noise를 주입한다는 것을 제외하고는 Cutout과 유사하므로 비교 대상으로 삼았다. 참고로 원논문에서는 학습시 Gaussian noise를 사용하나, 테스트 대상 데이터와 중복되므로 이를 uniform noise로 바꾸어 성능을 평가했다. 대략 30개가 넘는 hyperparameters를 수정했다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AutoAugment&lt;/strong&gt;는 높은 성능을 달성하는 augmentation policy를 찾아준다. 이때 출력 결과로 나온 최적의 augmentations 중 테스트 데이터와 겹치는 corruptions은 제거했으므로 논문에서는 AutoAugment*****로 표기한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Random AutoAugment&lt;/strong&gt;는 AutoAugment*을 사용하여 랜덤으로 샘플링된 augmentation policy를 갖는다. AutoAugment와 비교했을 때 더 적은 계산으로 더 다양한 augmentation를 제공한다는 특징이 있다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MaxBlurPooling&lt;/strong&gt;은 최근에 제안된 architectural modification으로, pooling 결과를 smooth하게 만들어준다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stylized ImageNet (SIN)&lt;/strong&gt;은 원본 ImageNet 데이터에 추가적으로 style transfer가 적용된 ImageNet을 사용하여 모델을 학습시키는 기술이다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여기에 추가적으로 &lt;strong&gt;SIN과 AugMix를 결합&lt;/strong&gt;한 것 역시 비교 대상으로 삼았다.&lt;/p&gt;

&lt;h4 id=&quot;training-setup-1&quot;&gt;Training Setup&lt;/h4&gt;

&lt;p&gt;학습에는 Goyal et al. (2017)의 표준 training scheme을 따르는 ResNet-50이 사용된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;learning rate&lt;/strong&gt; : we linearly scale the learning rate with the batch size, and use a learning rate warm-up for the ﬁrst 5 epochs.&lt;/li&gt;
  &lt;li&gt;**epochs **: AutoAugment and AUGMIX train for 180 epochs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt; : All input images are ﬁrst pre-processed with standard random cropping horizontal mirroring.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;result-corruption-error-1&quot;&gt;Result①: Corruption Error&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220305184335714.png&quot; alt=&quot;image-20220305184335714&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AugMix는 standard와 비교했을 때 Clean Error는 23.9→22.4%로 1.5% 감소시킴과 동시에 mean Corruption Error는 80.6→68.4%로 12.2% 감소시켰다. 기존의 augmentation 기법들이 clean error와 corruption error가 trade-off 관계가 있었다는 점을 감안할 때 의미있는 결과이다.&lt;/li&gt;
  &lt;li&gt;AugMix는 다른 augmentation 기법과 결합하여 사용하기에도 좋게 만들어졌다. 여기서는 SIN(Stylized ImageNet) 기법과 결합하여 사용해 본 결과, AugMix만을 사용했을 때 보다 더 좋은 성능을 거두었음을 보여주었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;result-flip-rate&quot;&gt;Result②: Flip Rate&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-03-04-ai-ComputerVision-AugMix/image-20220305184649885.png&quot; alt=&quot;image-20220305184649885&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Flip Rate는 Perturbation robustness 측정 지표이다. ImageNet-P에 각 augmentation 기법을 테스트한 결과, 가장 낮은 mean Flip Rate를 달성하였음을 확인할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;하지만 테스트 데이터에 적용된 corruption의 종류에 따라 다른 Flip Rate를 보여주는데, noise corrruption에 대해서는 Patch Uniform 기법이 더 성능이 좋음을 알 수 있다.&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;Patch Uniform은 랜덤으로 선택된 이미지 영역에 uniform noise를 주입”하는 augmentation 기법이다. 저자는 모든 종류의 augmentation 기법에서 테스트단에서 마주칠 수 있는 종류의 corruptions은 학습에서 배제했는데, 여기서는 Gaussian이나 Shot noise가 아니라는 이유에서 허용된 것으로 보인다. 사실상, 다른 분포를 갖는 노이즈를 학습한 것이므로 성능이 특히 더 좋아진 게 아닐까 싶다. 하지만 이는 역시 기존의 Corruption Memorizing한다는 문제점에 해당한다.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;참고자료&quot;&gt;참고자료&lt;/h2&gt;

&lt;h3 id=&quot;autoaugment&quot;&gt;&lt;a name=&quot;AutoAugment&quot;&gt;AutoAugment&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://qph.fs.quoracdn.net/main-qimg-28a78ec3843c310c9218ec697721411d&quot; alt=&quot;What is the difference between 6 and 9? - Quora&quot; style=&quot;width:200px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지나친 data augmentation은 manifold intrusion으로 이어질 수 있다. 따라서 이전까지는 데이터 도메인에 따라 어떤 augmentation을 적용할지 선택하는 과정이 수동으로 이루어졌다 (e.g., “새의 종류를 판별하는 문제에서는 histogram color swapping과 같은 augmentation을 사용하면 안되겠군”). &lt;strong&gt;AutoAugment&lt;/strong&gt;는 &lt;u&gt;주어진 데이터에서의 효과적인 augmentation을 찾는 과정을 '**자동으로**' 수행하는 것을 목표로 한다&lt;/u&gt;. 이를 위해 탐색공간(search space)을 정의하여 최적화된 augmentation 기법을 찾는다. 이때 탐색공간은 하나의 augmentation policy는 5개의 sub-policies로 구성하고, 각 sub-policy는 2개의 image operation으로 구성하여 정의된다. 각 sub-policy가 생성하는 augmentation 기법은 다음과 같다.
\(\text{1개의 augmentation policy}\\
= \text{2개의 이미지 연산}\\
=\text{이미지 연산}×\text{이미지 연산}\\
=(\text{augmentation 기법} × \text{확률} × \text{강도})^2\)
AutoAugment는 규모가 작은 데이터셋에서 특히나 더 뛰어난 성능을 보여주지만, 높은 계산 복잡도로 인해 일반적인 연구 환경에 적용이 어렵다는 단점이 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;AugMix에서는 실험 데이터에 적용할 적절한 augmentation 기법을 뽑아내기위해 AutoAugment를 사용했다. 이후, 뽑아낸 최적화된 augmentation 기법들 중 ImageNet-C와 겹치는 corruptions에 대해서는 배제하고 실험했다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;dirichlet--beta-distribution&quot;&gt;&lt;a name=&quot;Dirichlet_BetaDistribution&quot;&gt;Dirichlet &amp;amp; Beta Distribution&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Dirichlet distribution&lt;/strong&gt;과 &lt;strong&gt;Beta distribution&lt;/strong&gt;은 주로 image classification에서 사용되는 parametric distribution*이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(참고) parametric distribution* : 모수를 가정하여 추정하는 분포. 데이터가 적어도 잘 동작한다는 장점이 있지만, 모수에 영향을 받으므로 데이터에 따른 분포 업데이트 반영이 어렵다는 단점이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Beta distribution&lt;/strong&gt;는 univariate 특성을 가지며 &lt;u&gt;매개변수 $\alpha$, $\beta$에 대해 [0,1] 범위에서 정의되는 연속확률 분포&lt;/u&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/lna2sdm.jpg&quot; alt=&quot;img&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

\[f(x; α, β) = \frac{\Gamma{(\alpha+\beta)}}{\Gamma{(\alpha)}\Gamma{(\beta)}}x^{\alpha-1}(1-x)^{\beta-1}\\
\Gamma{(n)} = (n-1)!\]

&lt;p&gt;&lt;strong&gt;Dirichlet distribution&lt;/strong&gt;는 multivariate 특성을 가지며 다음의 연속확률 분포이다.&lt;/p&gt;

\[B(\alpha) = \frac{\Pi_{i=1}^k{\Gamma{(\alpha_i)}}}{\Gamma{(\sum_{i=1}^k{\alpha_i})}}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1912.02781.pdf&quot;&gt;Hendrycks, Dan, et al. “Augmix: A simple data processing method to improve robustness and uncertainty.” &lt;em&gt;arXiv preprint arXiv:1912.02781&lt;/em&gt; (2019).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/google-research/augmix&quot;&gt;“google-research/augmix”, github, 2022년 03월 22일 접속, https://github.com/google-research/augmix&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;AutoAugment
    &lt;ul&gt;
      &lt;li&gt;“Fast AutoAugment/1.데이터 어그먼테이션 연구 동향을 소개합니다.”, kakaobrain 블로그, 2022년 03월 02일 접속, &lt;a href=&quot;https://www.kakaobrain.com/blog/64&quot;&gt;https://www.kakaobrain.com/blog/64&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dirichlet &amp;amp; Beta Distribution
    &lt;ul&gt;
      &lt;li&gt;“Dirichlet distribution(Dirichlet prior)를 사용하는 이유”, donghwa-kim.github.io, 2019년 04월 02일 수정, 2022년 03월 02일 접속, &lt;a href=&quot;https://donghwa-kim.github.io/distributions.html&quot;&gt;https://donghwa-kim.github.io/distributions.html&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;“이항분포, 다항분포, 베타분포, 디리클레분포”, ratsgo’s blog, 2017년 05월 28일 수정, 2022년 03월 02일 접속, &lt;a href=&quot;https://ratsgo.github.io/statistics/2017/05/28/binomial/&quot;&gt;https://ratsgo.github.io/statistics/2017/05/28/binomial/&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 05 Mar 2022 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//ai-computervision/AugMix/</link>
        <guid isPermaLink="true">https://dazory.github.io//ai-computervision/AugMix/</guid>
        
        <category>AugMix</category>
        
        <category>Augmentation</category>
        
        
        <category>ai-computerVision</category>
        
      </item>
    
      <item>
        <title>[DL-05] Lect6. CNN-Convolution Neural Network</title>
        <description>&lt;p&gt;AIAS-Lect6_CNN: Convolutional Neural Networks(1)&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;이번 장에서는 Convolutional Neural Networks에 대해서 알아본다.&lt;/p&gt;

&lt;h2 id=&quot;cnn의-등장-배경&quot;&gt;CNN의 등장 배경&lt;/h2&gt;

\[Linear\ Score\ Function:\ f=Wx\\
2-layer\ Neural\ Network:\ f=W_2max()\]

&lt;p&gt;CNN 이전까지는 위와 같은 &lt;a href=&quot;C:\Users\dazol\Desktop\0-Typora\AIAS\Multi-Layer Perceptron.md&quot;&gt;fully-connected layer(=MLP, multilayer perceptron)&lt;/a&gt; 방식이 주로 사용되었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-27-DL05/image-20210411153543775.png&quot; alt=&quot;image-20210411153543775&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 이러한 알고리즘을 사용하는 경우 이미지 학습이 제대로 이루어지지 않는다는 단점이 있었다. 예를 들어 이미지의 경우 (채널을 무시한다고 할 때) 2차원 array의 형태로 구성되어 있다. 이때 학습시키기위해 image의 pixel value를 &lt;u&gt;flatten시킨다면 본래 이미지의 공간상의 영역을 제대로 표현하지 못해 좋은 구조라고 할 수 없다&lt;/u&gt;.&lt;/p&gt;

&lt;p&gt;이러한 니즈에서 나타난 구조가 바로 &lt;strong&gt;convolutional neural networks(CNNs)&lt;/strong&gt;구조이다. &lt;u&gt;공간상의 정보를 유지하면서 hidden vector를 표현한다는 것이 특징&lt;/u&gt;이다.&lt;/p&gt;

&lt;h1 id=&quot;convolutional-neural-networkscnn&quot;&gt;Convolutional Neural Networks(CNN)&lt;/h1&gt;

&lt;h2 id=&quot;fully-connected-layer&quot;&gt;Fully Connected Layer&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-27-DL05/image-20210413234950744.png&quot; alt=&quot;image-20210413234950744&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fully connected layer&lt;/strong&gt;란 하나의 layer의 모든 뉴런이 그 다음 layer의 모든 뉴런과 연결된 상태를 의미한다. 이때 이 layer는 &lt;strong&gt;flatten&lt;/strong&gt;되어 &lt;u&gt;1차원 배열&lt;/u&gt;의 형태라는 특징을 갖는다. fully connected layer는 기존의 image 인식에 주로 사용되었던 구조인데, 몇 가지 한계가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;image가 고해상도인 경우, input layer의 뉴런의 개수가 증가하므로 전체 &lt;u&gt;parameters의 수&lt;/u&gt;가 급격히 증가한다.&lt;/li&gt;
  &lt;li&gt;이미지 pixel을 flatten한 형태이므로 영상 전체의 관계(&lt;strong&gt;topology&lt;/strong&gt;)를 고려하지 못하여 &lt;u&gt;입력 데이터의 변형에 취약&lt;/u&gt;하다. (따라서 굉장히 &lt;u&gt;많은 학습 데이터를 필요&lt;/u&gt;로한다.)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;convolution-layer&quot;&gt;Convolution Layer&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Convolution layer(CL)&lt;/strong&gt;은 fully connected layer(FCL)의 단점을 보완하여 이미지 데이터에 대해 보다 나은 성능을 도출하기 위해 고안된 layer 모델이다. FCL은 flatten된 1차원 배열의 형태이지만 CL은 &lt;u&gt;입력 데이터의 형태를 유지한 3차원 배열의 구조&lt;/u&gt;를 갖는다. 한편 CL은 입력 데이터의 모든 뉴런에 연결되는 FCL와 달리, &lt;strong&gt;filter&lt;/strong&gt;내에 존재하는 뉴런에만 연결된다. 아래의 이미지에서 파란색은 입력 데이터를, 짙은 파란색은 filter 영역을, 하얀색은 연산 결과를 의미한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/9989933E5BC97E652B&quot; alt=&quot;img&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;한편 CL의 이러한 구조는 Hubel &amp;amp; Wiesel의 실험에서 보여준 아이디어를 잘 설명한다. 아래 그림은 layer별 결과를 이미지화 한 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/990A613B5BC97E7D1B&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞쪽 layer일 수록 저수준의 특성(edge, blob)을 판별하는 데에 유용하고 뒤로 갈 수록 점점 고수준의 특성으로 조합해나가는 것을 확인할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;filter&quot;&gt;Filter&lt;/h2&gt;

&lt;p&gt;위에서 설명한 바와 같이 input layer에 filter(or kernel)을 연산하여 output layer의 결과를 도출한다. filter의 구조는 아래와 같이 생겼다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99DF403C5BC97E912A&quot; alt=&quot;img&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;filter 영역에는 weight parameter(W)가 존재한다. 입력 데이터 X에 이러한 W를 연산하여 feature map을 도출한다.&lt;/p&gt;

&lt;h3 id=&quot;filter-연산&quot;&gt;Filter 연산&lt;/h3&gt;

&lt;p&gt;일반적인 영상처리 관점에서 봤을때, 이미지에 필터로 연산을 할 때에는 convolution 연산을 사용한다. Convolution layer라는 이름처럼 convolution layer도 convolution연산을 사용할 것 같지만 실은 그렇지 않다. &lt;strong&gt;convolution&lt;/strong&gt;은 “하나의 함수를 reverse &amp;amp; shift한 결과를 다른 함수에 곱하는 것”이다. 즉, reverse의 과정이 존재한다. 반면 &lt;strong&gt;cross-correlation&lt;/strong&gt;은 convolution에서 ‘&lt;u&gt;reverse&lt;/u&gt;‘만 하지 않는다. 즉 수식으로 풀어쓰면 아래와 같다.
\(convolution\ (f*g)(t)={\int^{∞}_{-∞}}{f(τ)g(t-τ)}dτ\\
cross-correlation\ (f*g)(t)={\int^{∞}_{-∞}}{f(τ)g(t+τ)}dτ\\\)&lt;/p&gt;

&lt;p&gt;이때 deep learning관점에서 봤을 때 filter의 weights는 정해진 값이 아니라 학습시키는 값이다. 따라서 result가 올바른 결과를 도출하게끔 개선된다. 따라서 convolution연산이나 cross-correlation이나 결과적으로 같은 결과를 도출한다. 따라서 tensorflow와 같은 deep learning framework에서는 &lt;u&gt;convolution이 아닌 cross-correlation으로 CNN이 구현&lt;/u&gt;된다. cross-correlation은 입력데이터와 필터의 weights의 대응되는 원소간 곱의 합계이므로 &lt;strong&gt;fused multiply-add(FMA)&lt;/strong&gt;라고도 부른다. 이러한 filter 연산을 통해 dimensional descent/expansion의 효과를 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;한편, Filter의 특성을 결정짓는 요소는 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Input layer의 사이즈 ⇒ filter size를 결정&lt;/li&gt;
  &lt;li&gt;filter의 개수 ⇒ 출력 데이터의 depth를 결정&lt;/li&gt;
  &lt;li&gt;Padding ⇒ 출력 데이터의 width, height를 결정&lt;/li&gt;
  &lt;li&gt;Stride ⇒ 출력 데이터의 width, height를 결정&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;input-layer의-사이즈&quot;&gt;Input layer의 사이즈&lt;/h3&gt;

&lt;p&gt;filter는 input layer size의 영향을 받는다. 먼저 input layer를 살펴보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-27-DL05/image-20210414001507332.png&quot; alt=&quot;image-20210414001507332&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에서 언급했듯 input layer는 3차원 데이터(height, width, depth)의 크기를 갖는다. 이때 depth가 3개인 RGB image data를 생각해보자. 이미지에서 R channel, G channel, B channel은 각각 동시성을 가지며 같은 공간 영역을 나타낸다. 따라서 spatial structure를 유지한다는 convolution layer의 철학에 맞게 filter 또한 이러한 성질을 반영해야 한다. 따라서 &lt;u&gt;filter의 depth는 input layer의 depth와 같은 크기를 갖는다&lt;/u&gt;.&lt;/p&gt;

&lt;h3 id=&quot;filter의-개수&quot;&gt;Filter의 개수&lt;/h3&gt;

&lt;p&gt;filter를 간단하게 표현하자면, &lt;u&gt;input data의 어떠한 특성을 도출하기 위한 값&lt;/u&gt;이다. 예를 들어, filter1은 input data의 edge특성을 나타낼 수 있고, filter2는 input data의 검정색 영역의 분포 특성을 잘 나타낼 수 있다. 한편 하나의 이미지에서 이러한 특성은 동시에 발생한다. 따라서 convolution layer를 통과한 결과의 depth로 특성을 동시에 표현할 수 있다. 기본적으로 filter의 depth는 input data의 depth와 같으므로 연산 결과의 depth는 1의 크기를 갖는다. 이러한 결과를 얼마나 겹치느냐에 따라 output data의 depth가 결정되는 것이다. 예를 들어, filter를 N개 사용하는 경우에는 output data의 depth가 N이 된다.&lt;/p&gt;

&lt;h3 id=&quot;padding&quot;&gt;Padding&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;padding&lt;/strong&gt;이란 필터를 적용시키기 전에 input data의 가장자리를 특정값으로 채우는 것을 의미한다. 예를 들어 아래와 같이 4x4의 사이즈를 갖는 input data 3x3 filter를 적용시키는 경우를 생각해보자. 이 경우 output data의 크기는 2x2가 된다. 이때 input data에 zero-padding=1을 주는 경우, input data가 5x5가 되어, 같은 filter를 적용해도 결과가 4x4가 된다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/999408355BC97F062C&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만일 입력데이터에 padding을 하지 않는다면 출력되는 데이터의 크기는 무조건 입력 데이터보다 작아질 것이다. 레이어가 깊어질 수록 (=filter가 많아질수록) 출력 데이터의 사이즈가 심하게 줄어들어 학습이 불가능한 상태가 되는 것을 방지하기 위해 도입된 개념이 바로 padding이다. 일반적으로 P만큼의 padding을 줄 때, (X_Height,X_Width)의 사이즈를 갖는 input data는 (X_H+2P, X_W+2P)의 크기가 되며, filter의 크기가 (F_Height,F_Width)일 때 output data의 크기 (O_Heigth,O_Width)는 다음의 식을 만족하게 된다.
\((O_H,O_W)=(X_H+2P-F_H+1,X_W+2P-F_W+1)\)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Q. 채워주는 Padding값이 result에 영향을 주지는 않을까?&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;padding을 주는 경우 대부분의 상황에서는 zero-padding이 일반적이다. 하지만 gray-padding이라고 해서 밝기값이 0~255인 경우에 128값으로 padding을 주는 경우도 존재한다. 중요한 점은, 이미지의 가장자리값이 조금 바뀌는 것은 전체 이미지에 큰 영향을 주지 못한다는 점이다. 일반적으로 classification에서 주요 인식 대상은 이미지의 가운데에 위치할 뿐만 아니라, padding을 주는 값은 전체 image의 size 중에서 극히 일부이기 때문이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;stride&quot;&gt;Stride&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;stride&lt;/strong&gt;란 필터의 &lt;u&gt;이동 간격&lt;/u&gt;을 의미한다. stride=1인 경우 filter는 input data에서 1pixel씩 이동하며 연산을 수행한다. 예를 들어 위의 예시에서는 stride=1인 경우이다. 이때 stride에서 주의해야 할 점은 input data, filter의 size와 맞지 않는 경우 output data의 size가 정수가 아니게 되어 에러가 발생할 수 있다는 점이다. 예를 들어, 6x6 input data에 3x3 filter를 적용하는 경우를 생각해보자. stride=2라면 filter가 stride를 이동하는 과정에서 input data를 초과하여 접근하게 된다. 따라서 이러한 경우를 방지하기 위해 아래의 공식을 통해 &lt;u&gt;출력데이터의 사이즈가 자연수값이 되도록 적절히 stride를 조정해주어야 한다.&lt;/u&gt; 아래의 식은 입력데이터(X_H, X_W), filter크기(F_H, F_W), Padding P, Stride S인 경우의 출력데이터의 크기(O_H, O_W)를 계산하는 공식이다.
\((O_H,O_W)=(\frac{X_H+2P-F_H}{S}+1,\frac{X_W+2P-F_W}{S}+1)\)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Q. input의 volume이 32x32x3이며 10개의 5x5 filter를 stride=1, pad=2로 줄 때 output의 volume size는 어떻게 될까?&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;(1)  volume = (32+2x2, 32+2x2, 3) = (36, 36, 3) (∵ input data에 padding, pad=2)
(2) volume = (36-5+1, 36-5+1, 3/3) = (32, 32, 1) (∵ 5x5 filter 적용, stride=1)
(3) volume = (32, 32, 1*10) (∵ filter 10개 적용)&lt;/p&gt;

  &lt;p&gt;⇒ ∴ 32x32x10&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Q. 이 layer에 존재하는 parameters의 개수는?&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;filter size는 5x5x3이며, 10개가 존재한다. 또한 filter별로 bias가 1개씩 존재한다. 따라서 (5x5x3)X10 + 10x1 = 750 + 10 = 760&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;pooling-layer&quot;&gt;Pooling Layer&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;pooling layer&lt;/strong&gt;는 CNN을 구성하는 layer중 하나이다. 일반적인 경우, convolution layer에서는 입력데이터의 사이즈를 유지하는 반면, pooling layer는 주로 데이터의 사이즈를 줄이기 위해 사용된다. 여기서 주의할 점은, pooling layer는 &lt;u&gt;parameters를 갖고있지 않다&lt;/u&gt;는 점이다. pooling layer는 filter와 달리 따로 weight를 가지는 것이 아니라 정해진 연산을 수행할 뿐이다. 대표적인 pooling layer에는 max-pooling과 average pooling이 있다.&lt;/p&gt;

&lt;h3 id=&quot;max-pooling&quot;&gt;Max-pooling&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;max-pooling&lt;/strong&gt;이란, 해당하는 영역에서의 최대값(MAX)을 선택하는 방법이다. 이러한 방법은 window 내에서 가장 중요한 요소 정보만을 취하겠다는 철학이 바탕이 된다. 아래 이미지는 이러한 max-pooling의 결과를 잘 보여준다. 대부분의 이미지 인식 모델에서는 주로 max-pooling을 사용한다. 또한 pooling layer에서 pooling의 window size는 stride와 같은 값으로 취하는 경우가 일반적이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/2366B34458F47D7808&quot; alt=&quot;img&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;average-pooling&quot;&gt;Average-pooling&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;average-pooling&lt;/strong&gt;은 해당하는 영역 내의 평균값을 취하는 방법이다. 아래 그림은 average-pooling을 잘 보여준다. average-pooling은 max-pooling보다 덜 극단적이며, 중요한 요소든 덜 중요한 요소든 모두 혼합하여 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.machinecurve.com/wp-content/uploads/2020/01/Average-Pooling-1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;경우에 따라서는 pooling layer를 두는 대신 stride를 크게 주어 데이터의 사이즈를 줄이는 경우도 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;normalization-layer&quot;&gt;Normalization Layer&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Normalization layer&lt;/strong&gt;는 입력을 정규화 하는 layer이다.&lt;/p&gt;

&lt;h2 id=&quot;fully-connected-layer-fc-layer&quot;&gt;Fully Connected Layer (FC layer)&lt;/h2&gt;

&lt;p&gt;CNN의 탄생 배경은, fully connected layer가 입력된 이미지 데이터에 대한 spatial space를 제대로 표현하지 못한다는 점이었다. 그럼에도 불구하고 CNN은 &lt;strong&gt;fully connected layer&lt;/strong&gt;를 필요로 한다. 일반적으로는 아래와 같이 &lt;u&gt;CNN의 마지막 layer단에서 classification을 위해 사용&lt;/u&gt;되는 경우가 많다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-27-DL05/image-20210414220839151.png&quot; alt=&quot;image-20210414220839151&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;cnn-architectures&quot;&gt;CNN Architectures&lt;/h1&gt;

&lt;p&gt;지금까지 CNN의 구성요소(CONV Layer, Filter, POOL Layer, FC Layer 등)에 대해 살펴보았다. 정리하자면, CNN architectures는 convolution layer(CONV), pooling layer(POOL), fully connected layer(FC) 등으로 구성되며, CONV와 FC는 learnable filter(weights)를 갖는다. 여기서 유의할 점은 언급하지는 않았으나 각 layer를 통과할 때마다 activation function을 지나간다는 점이다. 이 부분에 대해 간단하게 설명하자면, activation function이 없는 경우 아무리 deep하게 layers가 구성되어있을지라도 결국에 linear할 수밖에 없다. 따라서 layer마다 activation function을 두어 non-linear하게 만드는 작업을 해야한다.&lt;/p&gt;

&lt;p&gt;이번에는 대표적인 CNN 모델에 대해 살펴보도록 한다. 아래 이미지는 세계적인 규모의 이미지 인식 대회인 ILSVRC에서의 역대 우승자를 나타낸 그림이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-27-DL05/image-20210414221154855.png&quot; alt=&quot;image-20210414221154855&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2012년으로 넘어오면서 error rate(y)가 16.4%로 급격히 낮아짐을 확인할 수 있다. 2012년 우승을 차지한 &lt;strong&gt;AlexNet&lt;/strong&gt;은 최초로 대회에서 CNN 기반 모델을 이용하여 우승을 하였다.&lt;/p&gt;

&lt;h2 id=&quot;lenet&quot;&gt;LeNet&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;LeNet&lt;/strong&gt;은 최초의 CNN 기반 아키텍처이다. 1990년대에 대부분의 CNN에서의 성공적인 결과는 Yann LeCun가 만들었다. 그의 대표적인 architeture로는 zip code나 숫자를 인식하는 LeNet architeture가 있다.&lt;/p&gt;

&lt;h2 id=&quot;alexnet&quot;&gt;AlexNet&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;AlexNet&lt;/strong&gt;은 최초의 CNN기반 우승자 모델로, 이미지 인식 분야에서 deep learning의 가능성을 보여준 모델이라 평가된다. AlexNet에 대해서 자세히 알고싶다면 AlexNet에 대해 기술한 ImageNet Classification with Deep Convolutional Neural Networks 논문을 참고하면 된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;위에서 설명한 대부분의 개념은 이 논문에서 더 자세히 확인할 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;아래 이미지는 AlexNet의 architecture를 보여준다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-27-DL05/image-20210414221658635.png&quot; alt=&quot;image-20210414221658635&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림을 text로 풀어서 표현하면 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-27-DL05/image-20210414225431139.png&quot; alt=&quot;image-20210414225431139&quot; style=&quot;zoom: 67%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;눈여겨 봐야할 점은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;first use of ReLU&lt;/li&gt;
  &lt;li&gt;used Norm layers (not common anymore)
    &lt;ol&gt;
      &lt;li&gt;heavy data augmentation&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;dropout 0.5&lt;/li&gt;
  &lt;li&gt;batch size 128&lt;/li&gt;
  &lt;li&gt;SGD Momentum 0.9&lt;/li&gt;
  &lt;li&gt;Learning rate 1e-2, reduced by 10 manually when val accruacy plateaus&lt;/li&gt;
  &lt;li&gt;L2 weight decay 5e-4&lt;/li&gt;
  &lt;li&gt;7 CNN ensemble: 18.2% -&amp;gt; 15.4%&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;zfnet&quot;&gt;ZFNet&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ZFNet&lt;/strong&gt;은 AlexNet 다음년도 ILSVRC 우승자로, AlexNet 모델에서 hyperparameters를 향상시켜 더 나은 error rate(16.4%→11.7%)를 도출하였다. AlextNet에서 바뀐 hyperparameters는 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CONV1: (11×11 stride 4) → (7×7 stride 2)&lt;/li&gt;
  &lt;li&gt;CONV3, 4, 5: (384, 384, 256 filters) → (512, 1024, 512)&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;사실상 AlexNet과 큰 차이가 없어서 잘 언급되지는 않는다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;vggnet&quot;&gt;VGGNet&lt;/h2&gt;

&lt;p&gt;이전 CNN기반 모델(AlextNet, ZFNet)은 8 layers를 사용하였다. 하지만 &lt;strong&gt;VGGNet&lt;/strong&gt;(2014년)를 기점으로 사용되는 layers의 개수가 점점 더 많아지기 시작했다. VGG는 19개의 layers를 이용하여 network를 더욱 깊게 만들 수 있다는 것에 대한 가능성을 열어주었다. 아래 그림은 VGGNet의 architecture를 그림으로 표현한 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-27-DL05/image-20210414225553476.png&quot; alt=&quot;image-20210414225553476&quot; style=&quot;zoom:67%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;VGGNet는 더 작은 filters로 더 깊은 networks를 구현했다는 특징이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;8 layers(AlexNet) → 16~19 layers(VGGNet) : deeper networks&lt;/li&gt;
  &lt;li&gt;11×11, 5×5, 3×3과 같이 다양한 size의 CONV layer (AlexNet)
→ 오직 3×3 size, stride 1, pad 1를 갖는 CONV layer (VGGNet)&lt;/li&gt;
  &lt;li&gt;3×3 stride 2의 MAX POOL (AlexNet)
→ 2×2 stride 2의 MAX POOL (VGGNet)&lt;/li&gt;
  &lt;li&gt;11.7% top-5 error in ILSVRC’13 (ZFNet)
→ 7.3% top-5 error in ILSVRC’14 (VGGNet)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;VGGNet이 더 작은 filters를 사용한 이유는 3개의 3×3 CONV layer (stride 1)가 7×7 CONV layer와 비슷한 receptive field를 갖기 때문이다. 그 이유는 아래의 그림을 보면 더 쉽게 체감할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-27-DL05/image-20210414230418863.png&quot; alt=&quot;image-20210414230418863&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3개의 3×3 CONV layers가 input data의 7×7 receptive field를 갖는다는 것을 쉽게 알 수 있다. 그렇다면 1 7×7 CONV filter와 3 3×3 CONV filters 중 어느 것이 더 효율적일까? layer 당 channel 개수가 C개라고 할 때 parameters 개수는, 3개의 3×3 CONV layer는 3개*(3*3 kernel size *C kernel depth *C output depth)=27C²개이고, 7×7 CONV layer는 1개*(7*7 kernel size *C kernel depth *C output depth)=49C²개이다. 따라서 차라리 더 작은 size의 CONV layers를 여러 개 사용하는 것이 더 큰 size의 CONV layer 하나를 사용하는 것보다 효율적이다. 한편, VGGNet의 parameters 개수는 아래와 같이 계산된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-27-DL05/image-20210414231938865.png&quot; alt=&quot;image-20210414231938865&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 이미지에서 눈여겨 볼 점은, 대부분의 memory를 초반 CONV가 차지하고, 대부분의 parameters는 후반 FC가 차지한다는 점이다.&lt;/p&gt;

&lt;p&gt;DL 모델에서 메모리는 주로 backpropagation에서 chain rule을 적용하기 위해 모든 중간 변수를 저장하는 데에 사용되거나, mini batch를 사용하여 모델을 학습시킬 때 더 많은 중간 activation을 저장하는 데에 사용된다.&lt;/p&gt;

&lt;p&gt;backpropagationi을 할 때 CONV layer의 중간 결과를 저장하는 데에 주로 사용된다. 따라서  ???????????????//&lt;/p&gt;

&lt;p&gt;VGGNet의 세부사항은 아래와 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ILSVRC’14 2nd in classification, 1st in localization&lt;/li&gt;
  &lt;li&gt;Similar training procedure as Krizhevsky 2012&lt;/li&gt;
  &lt;li&gt;No Local Response Normalisation (LRN)&lt;/li&gt;
  &lt;li&gt;Use VGG16 or VGG19 (VGG19 only slightly better, more memory)&lt;/li&gt;
  &lt;li&gt;Use ensembles for best results&lt;/li&gt;
  &lt;li&gt;FC7 features generalize well to other tasks&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;https://excelsior-cjh.tistory.com/180&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 27 Feb 2022 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//ai-deeplearning/DL05/</link>
        <guid isPermaLink="true">https://dazory.github.io//ai-deeplearning/DL05/</guid>
        
        <category>Deep Learning</category>
        
        <category>AIAS</category>
        
        
        <category>ai-deepLearning</category>
        
      </item>
    
      <item>
        <title>[DL-04] Lect5. Deep Learning Programming</title>
        <description>&lt;p&gt;AIAS-Lect5_DL Programming&lt;/p&gt;

&lt;h1 id=&quot;deep-learning-platform&quot;&gt;Deep Learning Platform&lt;/h1&gt;

&lt;h2 id=&quot;소소한-팁&quot;&gt;소소한 팁&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;u&gt;데이터의 양&lt;/u&gt;보다는 &lt;u&gt;데이터의 퀄리티&lt;/u&gt;가 더 중요하다.
예를 들어 얼굴인식 모델을 만들 때, 너무 고정적인 환경에서의 데이터만 주어진다면 데이터가 10만장이어도 중복성때문에 사실상 유용성은 떨어질 수 있다. 반면, 30개의 데이터를 사용하더라도 성별, 옷차림, 헤어스타일, 악세사리, 안경 등의 practical 환경에서 가능한 여러가지 변화를 준 데이터를 사용한다면 훨씬 성능이 나아질 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;model을 만드는 것보다 더 중요한 것은 &lt;u&gt;goal을 정의&lt;/u&gt;하는 것이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;ai의-역사&quot;&gt;AI의 역사&lt;/h2&gt;

&lt;p&gt;초기 인공지능 분야는 비주류였다. 하지만 빅데이터, HW의 발전, SW의 발전 등의 요인으로 짧은 기간동안 큰 발전을 이룰 수 있었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Big Data&lt;/strong&gt; : Datasets이 더욱 커졌으며 수집 및 저장이 용이해졌다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt; : Graphics Procesisng Units(GPUs)기반 딥러닝 등장과 대규모 병렬 컴퓨팅(Massively Parallel Computer, MPP)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Software&lt;/strong&gt; : 향상된 기술, 새로운 models과 toolboxs&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ml-pipeline&quot;&gt;ML Pipeline&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ML Pipeline이란?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ML pipeline은 아래 diagram과 같이 몇 가지 components로 구성되어 있다.&lt;sup&gt;&lt;a href=&quot;#mFoot&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 
&lt;img src=&quot;https://developers.google.com/machine-learning/testing-debugging/images/ml-pipeline-schematic.svg?hl=ko&quot; alt=&quot;ex_screenshot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이처럼 model은 일부이고 다양한 components를 거쳐 최종적인 ML이 완성된다. 예를 들어, 일반적인 ML은 다음의 과정을 거친다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data &amp;amp; Streaming&lt;/strong&gt; : 분산 데이터 저장 및 스트리밍. 심하게 noisy한 data는 삭제된다. &lt;a href=&quot;https://cloud.google.com/storage/?utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=japac-AU-all-en-dr-bkws-all-super-trial-e-dr-1009882&amp;amp;utm_content=ims_text-ad-none-none-DEV_c-CRE_504957317166-ADGP_Hybrid%20%7C%20BKWS%20-%20EXA%20%7C%20Txt%20~%20Storage%20~%20Cloud%20Storage_Travel-cloud%20storage-google%20cloud%20storage-KWID_43700060418824876-kwd-11642151515&amp;amp;userloc_1009871-network_g&amp;amp;utm_term=KW_google%20cloud%20storage&amp;amp;gclid=CjwKCAjw07qDBhBxEiwA6pPbHvW8gJx31p4HrSsIStKUdZuiMT2ynoiVC07q18m8XW_iA_AeF0xPAxoCCWAQAvD_BwE&amp;amp;gclsrc=aw.ds&quot;&gt;Google Cloud Storage&lt;/a&gt;, &lt;a href=&quot;http://cassandra.apache.org/&quot;&gt;아파치 카산드라&lt;/a&gt;, &lt;a href=&quot;https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html&quot;&gt;아파치 하둡&lt;/a&gt; 등.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Users&lt;/strong&gt; : 데이터 준비 및 분석의 단계. &lt;a href=&quot;https://jupyter.org/hub&quot;&gt;Jupyterhub&lt;/a&gt;, &lt;a href=&quot;https://zeppelin.apache.org/&quot;&gt;아파치 제플린&lt;/a&gt;, &lt;a href=&quot;http://spark.apache.org/&quot;&gt;아파치 스파크&lt;/a&gt; 등.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Frameworks &amp;amp; Cluster&lt;/strong&gt; : Deep Learning Tools와 분산 호스팅 사용의 단계. 엔지니어링 영역. &lt;a href=&quot;https://www.tensorflow.org/?hl=ko&quot;&gt;Tensorflow&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; 등.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Models&lt;/strong&gt; : Machine learning model을 빌딩. 사이언티스트의 영역.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Model Serving&lt;/strong&gt; : Model을 clients에게 전송&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;deep-learning-hardware&quot;&gt;Deep Learning Hardware&lt;/h1&gt;

&lt;h2 id=&quot;cpu와-gpu의-차이점&quot;&gt;CPU와 GPU의 차이점&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;재밌게 봤던 &lt;a href=&quot;https://www.youtube.com/watch?v=-P28LKWTzrI&quot;&gt;영상: Mythbusturs Demo GPU versus CPU - NVIDIA&lt;/a&gt;. CPU와 GPU의 차이점을 시각적으로 재미있게 설명해준다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;dl&gt;
      &lt;dt&gt;&lt;strong&gt;CPU(Central Processing Unit)&lt;sup&gt;&lt;a href=&quot;#mFoot&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt;&lt;/dt&gt;
      &lt;dd&gt;개수는 적지만 각 코어가 빠르고 훨씬 capable하다. 순차적인 tasks 처리에 유용하다.&lt;/dd&gt;
    &lt;/dl&gt;
    &lt;ul&gt;
      &lt;li&gt;number of cores = 4 
⇒ 직렬처리에 최적화된 몇 개의 코어로 구성됨&lt;/li&gt;
      &lt;li&gt;Memory = System RAM 
⇒ cache가 매우 작으므로 대부분의 메모리를 RAM에서 가져다 사용한다. RAM은 보통 8, 12, 16, 32GB의 크기를 갖는다. 이는 GPU의 메모리 크기와 비교했을 때 매우 적은 양.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;dl&gt;
      &lt;dt&gt;&lt;strong&gt;GPU(Graphics Processing Unit)&lt;sup&gt;&lt;a href=&quot;#mFoot&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt;&lt;/dt&gt;
      &lt;dd&gt;개수는 많지만 각 코어가 비교적 느리고 단순한 작업만을 수행한다. 병렬처리에 유용한다.&lt;/dd&gt;
    &lt;/dl&gt;
    &lt;ul&gt;
      &lt;li&gt;number of cores = 4 
⇒ 다수의 소형 코어로 구성됨. 이를 통해 병렬처리를 효율적으로 처리 가능.&lt;/li&gt;
      &lt;li&gt;Memory = 11GB GDDR6
⇒ 칩 안에 RAM이 내장되어있는 내장 메모리를 사용한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cpu와-gpu의-communication&quot;&gt;CPU와 GPU의 Communication&lt;/h2&gt;

&lt;p&gt;하드웨어가 균형적으로 구성되지 않는 경우, PC의 성능이 저하되는 &lt;u&gt;병목현상&lt;/u&gt;이 나타날 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Bottle Neck이란?&lt;/strong&gt;
bottle neck은 이름 그대로 해석할 때 “병(bottle)의 목(neck) 현상”이다. 병에 들어있는 물체가 병의 주둥아리(bottle neck)으로 인해 한 번에 나오지 못하고 지연이 발생하는 현상과 관련한 용어이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;CPU Bottle Neck&lt;/strong&gt;
CPU가 여러 복잡한 프로세스를 처리할 때, CPU의 처리 속도가 데이터의 전송속도를 따라가지 못하는 경우 발생한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;실제 딥러닝 모델을 학습시킬 때 구간별로 소요시간을 측정해볼 것! 병목현상이 어디에서 발생하는지를 확인할 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;해결방법&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CPU와 GPU의 사양을 일치시킨다.&lt;/li&gt;
  &lt;li&gt;CPU와 GPU에 걸리는 처리 부하의 균형을 맞춘다.&lt;/li&gt;
  &lt;li&gt;HDD보다는 SSD를 사용한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;deep-learning-software&quot;&gt;Deep Learning Software&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;언어는 주로 PyTorch(Facebook), TensorFlow(Google)를 사용한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Microsoft Azure, amazon web services, Google Cloud Platform, Alibaba Cloud 등에서 좋은 GPU를 대여해주기도 한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;deep-learning-frameworks에서의-주요-포인트&quot;&gt;Deep Learning Frameworks에서의 주요 포인트&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;실행력&lt;/strong&gt; : 새로운 idea가 있을 때 빠르게 실행하는 것이 중요하다. ppt 100슬라이드보담 빠르게 데모 보여주는게 더 임팩트가 있다. 그런 측면에서 DL framework를 활용하면 실제와 모델을 맵핑하기 쉬워진다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;autograd&lt;/strong&gt; : pytorch등은 기본적인 함수의 gradient는 자동으로 구해준다. 하지만 ReLU같은 간단한 module에 대해서 forward와 backward정도는 구해본다면 이해도가 높아질 것이다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt; : 머신러닝 모델은 GPU 내에서 효율적으로 돌아간다. (cuDNN, cuBLAS, OpenGL 등)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;numpy-vs-pytorch&quot;&gt;Numpy vs. PyTorch&lt;/h2&gt;

&lt;p&gt;PyTorch를 사용하면 보다 간단하고 빠르게 머신러닝 코드를 구현할 수 있다. Numpy와 PyTorch로 각각 모델을 구현해본 뒤 차이점을 살펴보자.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Numpy&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
np.random.seed(0)

N,D = 3, 4 # N=batch개수, D=차원

x = np.random.randn(N,D)
y = np.random.randn(N,D)
z = np.random.randn(N,D)

a = x * y
b = a + z
c = np.sum(b)

# 각 node별로 gradient 계산하기&amp;lt;sup&amp;gt;[4](#mFoot)&amp;lt;/sup&amp;gt;
grad_c = 1.0
grad_b = grad_c * np.ones((N,D))
grad_a = grad_b.copy()
grad_z = grad_b.copy()
grad_x = grad_a * y
grad_y = grad_q * x
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Numpy로 구현한 코드는 API가 깔끔하고 숫자를 다루기 쉽다는 장점이 있으나, &lt;u&gt;gradients 수식을 직접 계산&lt;/u&gt;해야므로 번거롭고, &lt;u&gt;GPU 위에서 돌릴 수 없다&lt;/u&gt;는 단점이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch

N, D = 3, 4
x = torch.randn(N,D, required_grad=True)
y = torch.randn(N,D)
z = torch.randn(N,D)

a = x * y
b = a + z
c = torch.sum(b)

c.backward() # 위에서 required_grad값을 True해주었으므로 backward()를 통해 바로 gradient를 계산할 수 있다.
print(x.grad) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!-- 주석 --&gt;
&lt;p&gt;&lt;a name=&quot;mFoot&quot;&gt;1&lt;/a&gt;: Overview of ML Pipelines (https://developers.google.com/machine-learning/testing-debugging/pipeline/overview?hl=ko)&lt;br /&gt;
&lt;a name=&quot;mFoot&quot;&gt;2&lt;/a&gt;: Intel Core i7-7700k 기준&lt;br /&gt;
&lt;a name=&quot;mFoot&quot;&gt;3&lt;/a&gt;: (NVIDIA RTX 2080 Ti 기준)&lt;br /&gt;
&lt;a name=&quot;mFoot&quot;&gt;4&lt;/a&gt;: gradient 계산 관련 포스팅 참고 &lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;pytorch-기본-개념&quot;&gt;PyTorch: 기본 개념&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Tensor&lt;/strong&gt; : numpy array와 유사한 개념으로, numpy와 달리 GPU위에서도 돌아간다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Autograd&lt;/strong&gt; : tensors로 구성된 computational graphs를 만드는 package로, 자동으로 gradients를 계산해준다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Module&lt;/strong&gt; : 하나의 neural network layer로, state와 learnable weigths를 저장할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위의 내용을 코드와 함께 하나씩 살펴보자.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;해당 내용은 &lt;a href=&quot;https://pytorch.org/tutorials/beginner/pytorch_with_examples.html&quot;&gt;PyTorch 공식 홈페이지의 Learning PyTorch with Examples&lt;/a&gt;를 참고했습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;PyTorch 문법과 관련된 내용은 &lt;a href=&quot;https://pytorch.org/docs/stable/search.html?q=&amp;amp;check_keywords=yes&amp;amp;area=default#&quot;&gt;PyTorch 공식 홈페이지의 Docs&lt;/a&gt;에서 확인할 수 있습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;pytorch-running-example&quot;&gt;PyTorch: Running Example&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Running example&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A fully-connected ReLU network with one hidden layer and no biases&lt;/li&gt;
  &lt;li&gt;trained to predict y from x by minimizing squared Euclidean distance.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-20-DL04/img1.JPG&quot; alt=&quot;img1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 그림을 많이 봤을 것이다. 2개의 layer로 구성되었다는 것을 알 수 있다. 위 그림에서 생략된 부분을 자세히 나타내면 아래 그림과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-20-DL04/img2.JPG&quot; alt=&quot;img2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;layer별로 weigth parameter가 존재하며, 첫 번째 layer에는 ReLU activation function도 존재함을 확인할 수 있다. 이와 같이 모델을 구성하고자 할때 PyTorch를 이용하여 코드를 작성해보자.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-tensors&quot;&gt;PyTorch: Tensors&lt;/h2&gt;

&lt;p&gt;PyTorch는 Tensors를 지원한다. 이를 통해 모델을 디자인할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tensor란?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;전체 코드 보기 [&lt;a href=&quot;https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html#pytorch-tensors&quot;&gt;참고&lt;/a&gt;]&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cpu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  
    &lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_h_relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;we&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
		&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_w1&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_w2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Create random tensors for data and weights&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-02-20-DL04/img3.JPG&quot; alt=&quot;img3&quot; /&gt;&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cpu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;먼저 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D_in&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D_out&lt;/code&gt;과 같은 hyperparameters를 정의한다. 여기서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt;은 batch size를 의미하며, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D_in&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D_out&lt;/code&gt;은 각각 input layer, hidden layer, output layer의 크기를 의미한다.&lt;/p&gt;

    &lt;p&gt;hyperparameters(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D_in&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D_out&lt;/code&gt;)을 결정했다면 자동적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w1&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w2&lt;/code&gt;의 사이즈가 결정된다. 결정된 사이즈를 갖는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w1&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w2&lt;/code&gt;에 대해 random tensors를 생성하여 초기화 해 준다.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;learning_rate&lt;/code&gt;도 마찬가지로 초기화해준다.&lt;/p&gt;

    &lt;p&gt;한편, device는 코드가 돌아갈 장치를 지정해준다. 값을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;'cpu'&lt;/code&gt;가 아닌 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;'cuda:0'&lt;/code&gt;와 같이 GPU로 지정하면 학습 속도가 향상된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Forward pass: compute predictions and loss&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-02-20-DL04/img4.JPG&quot; alt=&quot;img4&quot; /&gt;&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;이 예제에서는 activation function으로 ReLU함수를 사용한다. 구성된 텐서 수식을 통해 예측값(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_pred&lt;/code&gt;)와 예측값에 따른 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss&lt;/code&gt;를 계산한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Backward pass: manually compute gradients&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-02-20-DL04/img5.JPG&quot; alt=&quot;img5&quot; /&gt;&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_h_relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;we&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;gradient는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss&lt;/code&gt;가 줄어드는 방향을 알려준다. gradients를 구하는 수식을 계산해준다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradient descent step on weights&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-02-20-DL04/img6.JPG&quot; alt=&quot;img6&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;계산한 gradients값을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w1&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w2&lt;/code&gt;에 적용해준다. 이를 통해 다음 반복부터는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss&lt;/code&gt;를 줄일 수 있게 될 것이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;pytorch-autograd&quot;&gt;PyTorch: Autograd&lt;/h2&gt;

&lt;p&gt;PyTorch는 autograd 기능을 지원한다. 이를 통해 gradients를 손수 계산하는 대신 코드상에서 자동으로 계산되게 만들 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;전체 코드 보기&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  
		&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_w1&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_w2&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-20-DL04/img7.JPG&quot; alt=&quot;img7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽은 Autograd를 사용하기 이전 코드이고 오른쪽은 PyTorch에서 지원하는 Autograd를 사용한 코드이다. 주황색 박스를 보면, 기존의 수식을 직접 대입하는 방식에서 loss.backward()와 같이 자동으로 계산하게 하는 코드로 바뀐 것을 확인할 수 있다. 이때 autograd를 수행하는 parameter는 requires_grad=True와 같이 명시해주어야 한다(빨간 박스부분). 해당 인자를 통해 PyTorch가 computational graph를 만들기 때문이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;x와 y는 parameters가 아니라 data다. 따라서 학습가능하지 않고 불변하므로 gradients를 계산할 수 없다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;torch.no_grad()는 이 부분에 대해서는 computational graph를 생성하지 말라는 것을 의미한다. 한편, 다음 iteration에서는 새로운 data가 들어오므로 메모리를 들고있을 필요가 없다. 따라서 w.grad.zero_()를 통해 gradient를 초기화해준다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;TIP!&lt;/strong&gt;
gradient초기화해주는 부분(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w.grad.zero_()&lt;/code&gt;)에서 함수 끝에 언더바(_)가 있는 걸 확인할 수 있다. 이것은 결과값이 return으로 반환되는 게 아니라, 그 자체를 변형시킨다는 것을 의미한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Autograd함수 직접 정의하는 방법&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;forward와 backward함수 작성을 통해 자유롭게 autograd함수를 만들 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;직접정의한 Autograd 함수: MyReLU()&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;staticmethod&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_for_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;staticmethod&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saved_tensors&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_input&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MyReLU()를 사용한 전체 코드&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  
		&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_w1&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_w2&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Forward pass: compute predictions and loss
Backward pass: compute gradients&lt;/p&gt;

&lt;p&gt;forward()는 predictions와 loss를 계산하는 역할을 한다. 위 예시에서는 x.clamp(min=0)을 통해 ReLU결과를 반환한다. 한편 여기서 ctx는 일종의 cache memory역할을 한다.&lt;/p&gt;

&lt;p&gt;backward()는 gradients를 계산하는 역할을 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;일반적인 경우에는 backward()함수를 정의할 필요가 없다. backward가 필요없는 경우에는 ctx도 필요없으므로 autograd를 하지 않는 경우는 해당 부분을 삭제함으로써 memory증가를 막아줘야 한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;pytorch-nn&quot;&gt;PyTorch: nn&lt;/h2&gt;

&lt;p&gt;Computational graph와 autograd는 복잡한 연산을 정의하거나 자동으로 derivatives를 구하는 데에 효과적이다. 하지만 매우 큰 neural networks에 대해서는 autograd가 너무 low-level일 수 있다. 따라서 PyTorch는 nn package를 통해 이러한 문제를 해결한다.&lt;/p&gt;

&lt;p&gt;nn package는 neural network layers와 거의 동등한 modules 세트를 정의한다. Module은 input Tensors를 받고, output Tensors를 계산한다 (물론 learnable parameters를 포함하는 tensors같은 internal state도 보유할 수 있다.). nn package는 또한 유용한 loss functions 세트도 정의하고 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-02-20-DL04/img8.jpg&quot; alt=&quot;img8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;주요 코드 비교&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;전체 코드보기&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-2&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  
	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;PyTorch:Autograd 코드(일부)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;PyTorch:nn 코드(일부)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-2&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;PyTorch 문법 helper&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.Sequential(*args)&lt;/code&gt; : A sequential container. Modules will be added to it in the order they are passed in the constructor.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.Linear(in_features, out_features, bias=True)&lt;/code&gt; : Applies a linear transformation to the incoming data: $y=xA^T+b$&lt;/p&gt;

    &lt;p&gt;parameters&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;in_features&lt;/strong&gt; – size of each input&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;sampleout_features&lt;/strong&gt; – size of each output&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;samplebias –&lt;/strong&gt; If set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;False&lt;/code&gt;, the layer will not learn an additive bias. Default: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.ReLU(inplace=False)&lt;/code&gt; : Applies the rectified linear unit function element-wise: $ReLU(x)=(x)^+=max(0,x)$&lt;/p&gt;

    &lt;p&gt;parameters&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;inplace&lt;/strong&gt; – can optionally do the operation in-place. Default: False&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위와 같이 model을 정의하면, for문에서 위와 같이 사용할 수 있다. 이처럼 PyTorch.nn class의 장점을 활용하면 코드를 좀 더 간결하고 유연하게 만들 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;새로운 modules 직접 정의하는 방법&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;__init__과 forward함수 작성을 통해 자유롭게 모듈을 정의할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;직접 정의한 module: TwoLayerNet()&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
  
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;blockquote&gt;
      &lt;p&gt;autograd가 backward를 핸들링하므로 여기서는 backward()함수를 정의할 필요가 없다.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TwoLayerNet을 사용한 전체 코드&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  	
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;새로운 module을 정의할 때 nn의 module을 사용할 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;pytorch-optim&quot;&gt;PyTorch: optim&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://tutorials.pytorch.kr/beginner/examples_nn/two_layer_net_optim.html?highlight=optim&quot;&gt;[PyTorch: Tutorials &amp;gt; 예제로 배우는 파이토치(PyTorch) &amp;gt; PyTorch: optim]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이전에 PyTorch:nn package를 이용해서 신경망을 구현한 코드에서 더 나아가 모델의 가중치(loss)를 직접 갱신하는 대신 optim package를 이용하여 가중치를 갱신할 optimizer를 정의해보자. optim package는 일반적으로 딥러닝에 사용되는 SGD+momentum, RMSProp, Adam과 같은 다양한 최적화(Optimization) 알고리즘을 정의한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;주요 코드 비교&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;전체 코드보기&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
															&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  	
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  	
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;PyTorch:nn 코드(일부)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;PyTorch:optim 코드(일부)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
															&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;PyTorch 문법 helper&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)&lt;/code&gt; : Implements Adam algorithm.&lt;/p&gt;

    &lt;p&gt;parameters&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;params (&lt;em&gt;iterable&lt;/em&gt;)&lt;/strong&gt; – iterable of parameters to optimize or dicts defining parameter groups&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;lr (&lt;em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;, optional&lt;/em&gt;)&lt;/strong&gt; – learning rate (default: 1e-3)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;betas (&lt;em&gt;Tuple[&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;, &lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;], optional&lt;/em&gt;)&lt;/strong&gt; – coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;eps (&lt;em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;, optional&lt;/em&gt;)&lt;/strong&gt; – term added to the denominator to improve numerical stability (default: 1e-8)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;weight_decay (&lt;em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;, optional&lt;/em&gt;)&lt;/strong&gt; – weight decay (L2 penalty) (default: 0)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;amsgrad (&lt;em&gt;boolean, optional&lt;/em&gt;)&lt;/strong&gt; – whether to use the AMSGrad variant of this algorithm from the paper &lt;a href=&quot;https://openreview.net/forum?id=ryQu7f-RZ&quot;&gt;On the Convergence of Adam and Beyond&lt;/a&gt; (default: False)&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)&lt;/code&gt; : Implements Adam algorithm.&lt;/p&gt;

    &lt;p&gt;parameters&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;params (&lt;em&gt;iterable&lt;/em&gt;)&lt;/strong&gt; – iterable of parameters to optimize or dicts defining parameter groups&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;lr (&lt;em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;, optional&lt;/em&gt;)&lt;/strong&gt; – learning rate (default: 1e-3)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;betas (&lt;em&gt;Tuple[&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;, &lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;], optional&lt;/em&gt;)&lt;/strong&gt; – coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;eps (&lt;em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;, optional&lt;/em&gt;)&lt;/strong&gt; – term added to the denominator to improve numerical stability (default: 1e-8)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;weight_decay (&lt;em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;, optional&lt;/em&gt;)&lt;/strong&gt; – weight decay (L2 penalty) (default: 0)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;amsgrad (&lt;em&gt;boolean, optional&lt;/em&gt;)&lt;/strong&gt; – whether to use the AMSGrad variant of this algorithm from the paper &lt;a href=&quot;https://openreview.net/forum?id=ryQu7f-RZ&quot;&gt;On the Convergence of Adam and Beyond&lt;/a&gt; (default: False)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 코드는 optimizer를 Adam으로 정의하고, 이를 이용하여 for문에서 parameters와 zero gradients를 업데이트해주었다.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-pretrained-models&quot;&gt;PyTorch: Pretrained Models&lt;/h2&gt;

&lt;p&gt;torchvision을 이용하면 pretrained models를 쉽게 사용할 수 있다. &lt;a href=&quot;https://github.com/pytorch/vision&quot;&gt;pytorch/vision 깃허브&lt;/a&gt;에 들어가면 자세한 내용을 확인할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-torchutilstensorboard&quot;&gt;PyTorch: torch.utils.tensorboard&lt;/h2&gt;

&lt;p&gt;TensorBoard utility를 이용하여 TensorBoard UI내에서 PyTorch 결과를 시각화 할 수 있다. TensorBoard에 대한 자세한 내용은 &lt;a href=&quot;https://www.tensorflow.org/tensorboard/&quot;&gt;TensorFlow-TensorBoard 페이지&lt;/a&gt;에서 확인 가능하다.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-dynamic-computational-graphs&quot;&gt;PyTorch: &lt;strong&gt;Dynamic&lt;/strong&gt; Computational Graphs&lt;/h2&gt;

&lt;p&gt;코드의 실행 단계에 따라 computational graph가 동적으로 생성된다.&lt;/p&gt;

&lt;p&gt;⇒ 비효율적&lt;/p&gt;

&lt;h2 id=&quot;pytorch-static-computational-graphs&quot;&gt;PyTorch: Static Computational Graphs&lt;/h2&gt;

&lt;p&gt;먼저 computational graph를 생성한 뒤, 해당 graph를 매 iteration마다 재사용한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;run_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Sun, 20 Feb 2022 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//ai-deeplearning/DL04/</link>
        <guid isPermaLink="true">https://dazory.github.io//ai-deeplearning/DL04/</guid>
        
        <category>Deep Learning</category>
        
        <category>AIAS</category>
        
        
        <category>ai-deepLearning</category>
        
      </item>
    
      <item>
        <title>[DL-03] Lect4. Neural Networks (part2)</title>
        <description>&lt;p&gt;AIAS-Lect4_Neural Networks (part2)&lt;/p&gt;

&lt;h1 id=&quot;data-preprocessing&quot;&gt;Data Preprocessing&lt;/h1&gt;

&lt;p&gt;일반적인 경우 우리가 가진 original data를 바로 학습에 사용하면 성능이 기대에 다소 못 미칠 수 있다. original data가 weight matrix에 너무 민감하여 optimize하기 어려운 경우가 그에 해당한다. 따라서 이런 경우 적절한 &lt;strong&gt;data preprocessing(전처리)&lt;/strong&gt;를 통해 학습에 사용하기 좋은 형태로 변환해주어야 한다.&lt;/p&gt;

&lt;h2 id=&quot;normalization&quot;&gt;Normalization&lt;/h2&gt;

&lt;p&gt;아래 그림은 데이터가 편향되거나 너무 분산된 경우 zero-centering과 normalization을 통해 preprocessing하는 것을 보여준다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418133156030.png&quot; alt=&quot;image-20210418133156030&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;zero-centering&lt;/strong&gt;이란 &lt;u&gt;data의 평균값을 0으로 맞춰주는 과정&lt;/u&gt;이다. 앞서 sigmoid의 단점으로 bias shift가 있다고 언급한 바 있다. 데이터에서도 마찬가지로 전체적으로 어느 방향으로 치우친 데이터보단 평균이 0인 데이터가 좋다는 입장에서 zero-centering을 한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;normalization&lt;/strong&gt;은 &lt;u&gt;data feature의 스케일을 동일한 정도로 맞춰주는 과정&lt;/u&gt;이다. unnormalized data의 경우, 최적화하는 과정에서 큰 폭으로 단계를 거쳐 최적값에 도달한다. 반면 normalized data의 경우는 시작점과 무관하게 일정한 폭으로 최적값이 도달할 수 있다. 이러한 특징은 학습을 보다 빠르게 만들어준다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;normalization은 feature의 scale이 동일할 때 학습이 더 잘 될 것이라는 가정 하에 사용해야 한다. data 분석 없이 마구잡이로 사용하는 것이 아니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;pca-and-whitening&quot;&gt;PCA and Whitening&lt;/h2&gt;

&lt;p&gt;아래 그림은 데이터가 편향되거나 너무 분산된 경우 decorrelation과 whitening을 통해 preprocessing하는 것을 보여준다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418134232817.png&quot; alt=&quot;image-20210418134232817&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;decorrelation&lt;/strong&gt;은&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Whitening&lt;/strong&gt;은 eigenbasis data(기저벡터)를 eigenvalue(고유값)로 나누어 정규화 하는 방법이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;일반적으로 이미지 데이터에 대해서는 PCA나 whitening을 잘 사용하지 않는다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;weight-initialization&quot;&gt;Weight Initialization&lt;/h1&gt;

&lt;p&gt;이전에 배웠듯 머신러닝 학습 과정의 큰 틀은 다음과 같다.&lt;/p&gt;

&lt;p&gt;feed-forward → loss function → backpropagation(optimization) → update model parameters → feed-forward → …&lt;/p&gt;

&lt;p&gt;그렇다면 학습 가장 초반에 model parameters는 어떻게 초기화하는 것이 좋을까?&lt;/p&gt;

&lt;h2 id=&quot;weights-and-activation-function&quot;&gt;Weights and Activation function&lt;/h2&gt;

&lt;p&gt;학습 초기 initialization 과정은 매우 중요하다. model parameters가 어떻게 initialization되었는가에 따라 결과가 달라지기 때문이다. 또한 weigths와 activation function은 연관성이 깊은데, network가 deep해질 수록 activation functions에 의해 model parameters(weights)가 전혀 update가 안될 수도 있기 때문이다. 아래에서 사례를 통해 연관성을 살펴보자.&lt;/p&gt;

&lt;h3 id=&quot;small-random-numbers-with-tanh&quot;&gt;Small random numbers with tanh&lt;/h3&gt;

\[W-N(0,(1e-2)^2)\]

&lt;p&gt;위와 같이 평균이 0, 표준편차가 1e-2인 가우시안 분포를 따르는 random변수로 초기화 할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이런 경우 small networks에는 나쁘지 않게 작동하지만 network가 깊어질 수록 문제가 발생한다. 가령 activationo functions으로 tanh()함수를 사용한 아래의 경우를 살펴보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Forward pass for a 6-layer net with hidden size 4096
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dout&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'['&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'layers]  mean:%.2f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'std:%.2f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
[ 1 layers]  mean:-0.00 std:0.49
[ 2 layers]  mean:0.00 std:0.29
[ 3 layers]  mean:-0.00 std:0.18
[ 4 layers]  mean:0.00 std:0.11
[ 5 layers]  mean:-0.00 std:0.07
[ 6 layers]  mean:-0.00 std:0.05
&quot;&quot;&quot;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418170041457.png&quot; alt=&quot;image-20210418170041457&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 코드는에서 W값은 mean=0, std=1e-2인 가우시안 분포를 따르는 랜덤변수이다.  각 tanh activation function을 통과한 결과에 대하여 평균과 분산을 출력한 결과는 위와 같다. 이를 통해 layer를 통과할 수록 분산이 매우 작아져 결국 출력값의 대부분이 0으로 수렴한다는 것을 알 수 있다. 출력값이 0에 수렴하는 이유는 tanh activation function의 꼴을 보면 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418170359983.png&quot; alt=&quot;image-20210418170359983&quot; style=&quot;zoom:67%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;tanh()함수는 입력의 절댓값이 작은 경우 출력값이 0에 가까워지는 특징을 가졌다. 이러한 함수 특징에 의해 layer를 통과할 수록 출력값이 0에 수렴하는 것이다. layer의 출력값이 0인 경우 학습이 전혀 되지 않는다. 그 이유는 chain rule에 의해 쉽게 설명 가능하다.
\(\frac{∂L}{∂w_i^{l+1}}=\frac{∂L}{∂f_{l+1}}*\frac{∂f_{l+1}}{∂w_i^{l+1}}=\frac{∂L}{∂f_{l+1}}*x_i^{l+1}≈0\\
(∵\ f_{l+1}(x_i^{l+1},w_i^{l+1})=W^{l+1}*X^{l+1}\ )\)
downstream gradient는 upstream gradient와 local gradient의 곱으로 계산할 수 있다. 이때 layers를 통과할 수록 x값이 0에 수렴하므로 결국 downstream gradient도 0으로 수렴하여 backpropagation이 전혀 진행되지 않음을 상상할 수 있다.&lt;/p&gt;

&lt;p&gt;### Increase std for initial weights from 0.01 to 0.05&lt;/p&gt;

&lt;p&gt;weights의 초기값이 너무 작은 경우 위와 같이 deep network에서 layer를 통과할 수록 출력값이 0에 수렴해 backpropagation이 전혀 진행되지 않았음을 확인했다. 그렇다면 weights 표준편차의 초기값을 0.01에서 0.05로 늘려보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Forward pass for a 6-layer net with hidden size 4096
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dout&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#★
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'['&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'layers]  mean:%.2f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'std:%.2f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
[ 1 layers]  mean:0.00 std:0.87
[ 2 layers]  mean:0.00 std:0.85
[ 3 layers]  mean:-0.00 std:0.85
[ 4 layers]  mean:-0.00 std:0.85
[ 5 layers]  mean:-0.01 std:0.85
[ 6 layers]  mean:-0.00 std:0.85
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418170515596.png&quot; alt=&quot;image-20210418170515596&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 코드는에서 W값은 mean=0, std=0.05인 가우시안 분포를 따르는 랜덤변수이다.  각 tanh activation function을 통과한 결과에 대하여 평균과 분산을 출력한 결과는 위와 같다. 한편, 이번에는 출력값의 대부분이 -1또는 1인것을 histogram을 통해 확인할 수 있다. 이 또한 tanh activation function의 특징을 통해 설명 가능하다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418170634722.png&quot; alt=&quot;image-20210418170634722&quot; style=&quot;zoom:67%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;tanh()함수는 입력의 절댓값이 큰 경우 출력값이 -1 또는 1에 가까워지는 특징을 가졌다. 이러한 함수 특징에 의해 layer를 통과할 수록 출력값이 -1 또는 1에 수렴하는 것이다. 이러한 경우를 saturated activation function이라고 하는데, 이 경우 또한 학습이 전혀 되지 않는다. 그 이유는 chain rule에 의해 쉽게 설명 가능하다.
\(\frac{∂L}{∂w_i}=\frac{∂L}{∂f}*\frac{∂f}{∂w_i}=\frac{∂L}{∂f}*\frac{∂f}{∂s}*\frac{∂s}{∂w_i}≈0\\
(∵\ s=\sum_i{w_ix_i}+b,\ f:activation\ function일\ 때,\ \frac{∂f}{∂s}≈0\ )\)
s를 loss function, f를 activation function이라고 하자. 출력이 -1또는 1인 경우의 activation function (tanh)의 미분값은 위 tanh그래프를 통해 0에 수렴함을 알 수 있다. 따라서 local gradient이 0으로 수렴하므로 downstream gradient도 0으로 수렴하여 backpropagation이 전혀 진행되지 않음을 상상할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;xavier-with-tanh-initialization&quot;&gt;Xavier (with tanh) Initialization&lt;/h3&gt;

&lt;p&gt;위의 사례를 통해 activation function이 tanh인 경우, weights가 너무 작거나 크면 backpropagation이 안되는 문제를 확인했다. 출력값의 분포를 적당하게 맞춰주는 작업이 필요해졌고, 그에 따라 Xavier initialization 방법이 제안되었다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Forward pass for a 6-layer net with hidden size 4096
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dout&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#★
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'['&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'layers]  mean:%.2f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'std:%.2f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
[ 1 layers]  mean:0.00 std:0.63
[ 2 layers]  mean:0.00 std:0.49
[ 3 layers]  mean:-0.00 std:0.41
[ 4 layers]  mean:-0.00 std:0.36
[ 5 layers]  mean:0.00 std:0.32
[ 6 layers]  mean:0.00 std:0.29
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418171907614.png&quot; alt=&quot;image-20210418171907614&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 코드는에서 W값은 mean=0, std=1/sqrt(Din)인 가우시안 분포를 따르는 랜덤변수이다. 이때 입력의 루트값으로 나눠준 값을 표준편차로 정의하는 경우, 위와 같이 출력의 분산이 -1과 1 사이에 골고루 퍼지는 것을 확인할 수 있다. 이 원리에 대해서는 학부생의 수준을 넘어서므로 여기서는 논의하지 않는다. 다만 어떠한 작업(1/sqrt(Din))을 통해 출력의 분산을 적절히 scaling해주었다는 정도만 알면 된다. 이 경우 모든 layers에 대해서 적절한 입력이 주어지므로 학습이 잘 될 것임을 기대할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;xavier-with-relu-initialization&quot;&gt;Xavier (with ReLU) Initialization&lt;/h3&gt;

&lt;p&gt;이번에는 Xavier initialization에 activation function으로 ReLU함수를 사용해보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Forward pass for a 6-layer net with hidden size 4096
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dout&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#★
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'['&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'layers]  mean:%.2f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'std:%.2f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
[ 1 layers]  mean:0.40 std:0.58
[ 2 layers]  mean:0.28 std:0.41
[ 3 layers]  mean:0.19 std:0.29
[ 4 layers]  mean:0.14 std:0.20
[ 5 layers]  mean:0.10 std:0.14
[ 6 layers]  mean:0.07 std:0.10
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418172843174.png&quot; alt=&quot;image-20210418172843174&quot; /&gt;&lt;/p&gt;

&lt;p&gt;activation function을 ReLU함수로 바꾸니 위와 같이 출력이 다시 0으로 수렴하는 현상이 발생했다. 이러한 결과는 음수 입력값에 대해 항상 0을 출력하는 ReLU함수의 특징 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;kaimingmsra-with-relu-initialization&quot;&gt;Kaiming/MSRA (with ReLU) Initialization&lt;/h3&gt;

&lt;p&gt;위의 예시를 통해 Xavier initialization에 ReLU함수를 사용하면 출력이 0으로 수렴하는 문제가 발생함을 확인했다. 다음과 같이 weights 부분을 고침으로써 이러한 문제를 해결할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Forward pass for a 6-layer net with hidden size 4096
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dout&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Din&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#★
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'['&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'layers]  mean:%.2f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'std:%.2f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
[ 1 layers]  mean:0.56 std:0.83
[ 2 layers]  mean:0.58 std:0.83
[ 3 layers]  mean:0.58 std:0.83
[ 4 layers]  mean:0.56 std:0.83
[ 5 layers]  mean:0.55 std:0.80
[ 6 layers]  mean:0.55 std:0.80
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418191214260.png&quot; alt=&quot;image-20210418191214260&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ReLU함수는 음수입력값이 0으로 출력된다는 문제로 인해, 입력의 절반 정도가 사라진다는 이유로 학습이 제대로 이루어지지 않았다. 따라서 Xavier initialization에서의 아이디어에 2로 나누어 입력의 절반이 없어진다는 점을 반영한다. 그 결과 위와 같이 좋은 분포를 출력함을 확인할 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;stochastic-gradient-descent-sgd&quot;&gt;Stochastic Gradient Descent (SGD)&lt;/h1&gt;

&lt;h2 id=&quot;gradient-steps&quot;&gt;Gradient Steps&lt;/h2&gt;

&lt;p&gt;모델은 gradient를 통해 loss가 줄어드는 방향으로 model parameters가 업데이트 되며 학습이 이루어진다. 이때 model parameters는 다음과 같이 업데이트된다.
\(x'=x-α▽_xf(x)\\
α: learning\ rate\\
▽_x: gradient\)
이때 learning rate(α)가 작으면 아래 그림에서 화살표 방향(gradient)으로 조금 이동하고, learning rate가 큰 경우, 화살표 방향으로 크게 이동한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418191925234.png&quot; alt=&quot;image-20210418191925234&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가장 최적의 point를 &lt;strong&gt;global optimum&lt;/strong&gt;이라 하고, 일부 영역에서만 놓고 봤을 때 최적의 point는 &lt;strong&gt;local mimum&lt;/strong&gt;이라고 한다. 이때 learning rate가 너무 작은 경우, global minimum에 도달하는데 까지 걸리는 오랜 시간이 소요될 수 있다. 반면 learning rate가 너무 작은 경우, 섬세한 이동을 하지 못해 global minimum를 정확히 찾지 못할 수 있다. 이러한 learning rate는 아래 그림과 같이 gradient descent의 convergence(수렴) 여부에 영향을 미친다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418192259743.png&quot; alt=&quot;image-20210418192259743&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 big learning rate인 경우 발산할 위험성이 있음을 보여준다. 따라서 적절한 learning rate를 결정하는 것이 학습에 있어서 매우 중요하다.&lt;/p&gt;

&lt;h2 id=&quot;sgd-탄생-배경&quot;&gt;SGD 탄생 배경&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Single Training Sample의 경우&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;다음과 같이 주어진 조건에서 최적의 model parameters θ={W,b}을 찾아보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418200053743.png&quot; alt=&quot;image-20210418200053743&quot; /&gt;&lt;/p&gt;

&lt;p&gt;single training sample에 대해서 학습하는 방법은 아래와 같다.
\(θ^{k+1}=θ^k-α▽_θL_i(θ^k,x_i,y_i)\)
즉, 현재 parameters θ에서 gradient(▽)와 Loss function(L)의 곱을 learning rate(α)만큼 빼주면 새로운 parameters가 된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multiple Training Samples의 경우&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이번에는 single training sample이 아닌 n개의 multiple training samples {xi, yi}에 대해 최적의 model parameters를 구한다고 생각해보자. 이때의 cost는 아래와 같이 계산할 수 있다.
\(Cost\ L=\frac{1}{n}\sum_{i=1}^nL_i(θ, x_i, y_i)\)
n개의 single training sample 각각에 대한 loss의 평균을 구하면 multiple training samples에 대한 cost를 구할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Too Many Training Samples의 경우&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 아주 training samples가 너무 많은 경우는 어떻게 될까? 모든 training samples 각각에 대한 loss를 계산한 뒤 평균을 내는 방법은 연산량이 너무 많아 비효율적이다. 이러한 배경에서 &lt;strong&gt;stochastic gradient descent(SGD)&lt;/strong&gt;가 탄생했다.&lt;/p&gt;

&lt;h2 id=&quot;sgd의-개념&quot;&gt;SGD의 개념&lt;/h2&gt;

&lt;p&gt;n개의 training samples에 대해 gradient를 계산하려면 O(n)만큼의 연산이 필요하다. 확률에 근거하면 전체 training samples에 대한 loss는 전체 training samples의 loss 기댓값으로 나타낼 수 있다.
\(\frac{1}{n}(\sum_{i=1}^n{L_i(θ, x_i, y_i)})= \mathbb{E}_{i~[1,...,n]}[L_i(θ,x_i,y_i)]\)
이때 loss의 기댓값은 일부 training samples만으로 아래와 같이 추정할 수 있다.
\(\mathbb{E}_{i~[1,...,n]}[L_i(θ,x_i,y_i)] ≈ \frac{1}{|S|}\sum_{j∈S}(L_j(θ,x_j,y_j))\ with\ S⊆{1,...n}\)
이러한 전체 training samples의 subset을 &lt;strong&gt;minibatch&lt;/strong&gt;라고 하며 아래와 같이 수학적 기호로 표현할 수 있다.
\(B_i=\{\{x_1,y_1\},\{x_2,y_2\}, ... ,\{x_m,y_m\}\}=\{B_1, B_2,...,B_{n/m}\}\)&lt;/p&gt;

&lt;h2 id=&quot;용어-정리&quot;&gt;용어 정리&lt;/h2&gt;

&lt;p&gt;SGD에서 새롭게 알게 될 용어들에 대해 간단히 살펴보자.&lt;/p&gt;

&lt;h3 id=&quot;epoch&quot;&gt;Epoch&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Epoch&lt;/strong&gt;은 한국말로 “시대”를 뜻한다. AI에서 epoch은 &lt;u&gt;전체 dataset에 대하여 한 번 학습을 완료한 상태&lt;/u&gt;(forward &amp;amp; backward pass과정을 거친 상태)를 의미한다.&lt;/p&gt;

&lt;h3 id=&quot;batch&quot;&gt;Batch&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;batch&lt;/strong&gt;는 &lt;u&gt;나누어진 dataset&lt;/u&gt;을 의미한다. gradient descent에서는 전체 dataset을 한꺼번에 학습시키는데, 이로인해 발생하는 비효율 문제를 해소하고자 나온 알고리즘이 바로 stochastic gradient descent이었다. 여기서 SGD는 전체 dataset을 잘게 쪼개어 batch 단위로 학습시킨다고 볼 수 있다. batch를 &lt;strong&gt;mini-batch&lt;/strong&gt;라고도 부르며, 일반적으로 사이즈는 32, 64, 128개를 주로 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;iteration&quot;&gt;Iteration&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;iteration&lt;/strong&gt;은 한국말로 “되풀이”라는 뜻이다. 전체 dataset을 쪼개어 batch 단위로 학습을 수행할 때, 1 epoch을 달성하기 위해서는 여러 번의 실행이 필요하다. 이러한 &lt;u&gt;실행 반복&lt;/u&gt;을 iteration이라고 한다. 전체 data의 양이 N개라고 할 때 batch size가 n이면, iteration은 N/n이 되는 관계이다.&lt;/p&gt;

&lt;h2 id=&quot;학습-과정&quot;&gt;학습 과정&lt;/h2&gt;

&lt;p&gt;Stochastic Gradient Descent(SGD)는 다음과 같은 학습 과정을 거친다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;Loop:
	1. Sample a batch of data
	2. Forward prop it through the graph(network), get loss
	3. Backprop to calculate the gradients
	4. Update the parameters using the gradient
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;samples의 개수가 많은 경우 일반적인 gradient descent보다 stochastic gradient descent 알고리즘을 더 많이 사용하는데, 그 이유는 아래의 그림으로 설명할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418202625500.png&quot; alt=&quot;image-20210418202625500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;gradient descent의 경우 한 step을 이동할 때 아주 정확한 방향으로 최적화를 수행한다. 반면 stochastic gradient descent는 한 step을 이동할때 방향적 정확도는 gradient descent보다 떨어짐을 확인할 수 있다. 하지만 그럼에도 불구하고 대량의 데이터에 대해 stochastic gradient descent가 더 좋은 성능을 보이는 이유는, 빠르기 때문이다. gradient descent가 최적의 한 step을 계산하는 동안 stochastic gradient descent는 이미 몇 step 이동해있기 때문이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;비유하자면, gradient descent는 완벽주의자, stochastic gradient descent는 행동주의자이다. &lt;del&gt;여기서 완벽한 것도 좋지만 조금 틀리더라도 일단 해보는 게 중요하다는 교훈을 얻을 수 있다.&lt;/del&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;fancier-optimizers&quot;&gt;(Fancier) Optimizers&lt;/h1&gt;

&lt;p&gt;Gradient descent의 학습속도 문제를 해결하고자 stochastic gradient descent가 제안되었다. 하지만 SGD는 gradient 기반 optimizer이므로 local minimum에 갇힐 수 있다는 문제점을 가지고 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418203310926.png&quot; alt=&quot;image-20210418203310926&quot; style=&quot;zoom:67%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 이미지는 local minimum(혹은 saddle point)에 갇히는 경우를 나타낸다. gradient값이 일시적으로 0인 경우, gradient descent가 최적화를 중지한다는 문제를 해결하고자 여러가지 optimizers가 제안되었다.&lt;/p&gt;

&lt;h2 id=&quot;sgd--momentum&quot;&gt;SGD + Momentum&lt;/h2&gt;

&lt;p&gt;먼저 SGD의 학습은 아래와 같이 이루어졌다.
\(x_{t+1}=x_t-α▽f(x_t)\)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;여기에 momentum의 개념을 더한 SGD+Momentum optimizer는 아래와 같다.
\(v_{t+1}=ρv_t+▽f(x_t)\\
x_{t+1}=x_t-αv_{t+1}\)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rho&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;momentum&lt;/strong&gt;이란 “현재 가고있는 방향을 유지하려는 성질”이다. 자연계의 대부분의 현상은 갑작스럽게 변하지 않고 서서히 변한다는 가정 아래 momentum의 개념을 추가할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418204552073.png&quot; alt=&quot;image-20210418204552073&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 rho(ρ)값은 일반적으로 0.9 혹은 0.99를 사용하며, 관성을 유지하는 정도를 나타낸다.&lt;/p&gt;

&lt;h2 id=&quot;adagrad&quot;&gt;AdaGrad&lt;/h2&gt;

&lt;p&gt;한편, SGD+momentum optimizer는 &lt;u&gt;learning rate가 너무 작으면 학습시간이 너무 길고, learning rate가 너무 크면 발산한다는 문제&lt;/u&gt;가 있었다. 이러한 문제를 해결하고자 AdaGrad optimizer가 등장했다. AdaGrad는 “시간이 지남에 따라 optimum에 가까워진다”라는 가정 하에 learning rate를 계속 감소시키며 학습을 진행한다.
\(h←h+▽f(x)^2\\
x←x-α\frac{1}{\sqrt{h}}▽f(x)\)
이처럼 h에 이전 기울기의 제곱값을 누적해서 더한 뒤, model parameter(x)를 업데이트 할 때 gradient와 learning rate의 곱에 √h를 나누어 준다. h값은 학습이 진행됨에 따라 값이 커지므로, x값의 learning rate(α)가 학습이 진행됨에 따라 감소하게 된다. 이를 코드로 나타내면 아래와 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;그-외의-optimizers&quot;&gt;그 외의 optimizers&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418210542140.png&quot; alt=&quot;출처: 자습해도 모르겠던 딥러닝, 머리속에 인스톨 시켜드립니다.&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;잘 모르겠다면 일단 Adam을 써라!!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;regularization&quot;&gt;Regularization&lt;/h1&gt;

&lt;p&gt;이전에 잠시 loss가 최소가 되는 model parameters는 unique하지 않다는 것을 언급한 바 있다. 따라서 최적의 model parameters를 찾기 위해 regularization의 개념이 필요하다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;혹시 까먹었을까봐&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Regularization&lt;/strong&gt;은 &lt;u&gt;model이 training data에 대하여 너무 잘 맞는 **overfitting**을 방지하기 위해 추가되는 항&lt;/u&gt;이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;model-ensembles&quot;&gt;Model Ensembles&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Model ensembles&lt;/strong&gt;란 독립적인 모델 여러 개를 함께 사용하여 overfitting을 방지하고 성능을 향상시키는 방법이다. 각각의 모델을 따로 학습시킨 뒤 test time때 각각의 결과를 평균내는 방식으로 진행된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;하지만 model ensembles을 하기 위해서는 좋은 컴퓨팅 성능이 필수적이다. 따라서 개인 차원에서는 잘 안하고 회사 차원에서 시도해볼 수 있는 기법이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;model ensembles에서 변형을 가하는 방법은 다음과 같이 매우 다양하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Same model but different &lt;strong&gt;initializations&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Same model but different &lt;strong&gt;optimization/objective function&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Same model but different &lt;strong&gt;datasets&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Top models&lt;/strong&gt; discovered during cross-validation&lt;/li&gt;
  &lt;li&gt;Different &lt;strong&gt;checkpoints&lt;/strong&gt; (i.e. iteration) of a single model&lt;/li&gt;
  &lt;li&gt;Running &lt;strong&gt;average of parameters&lt;/strong&gt; during training&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이러한 model ensembles의 종류에 대해 살펴보자.&lt;/p&gt;

&lt;h2 id=&quot;dropout&quot;&gt;Dropout&lt;/h2&gt;

&lt;p&gt;각 node에서 forward pass를 할 때, &lt;u&gt;랜덤한 확률로 일부 neurons을 zero로 만드는 기법&lt;/u&gt;을 &lt;strong&gt;dropout&lt;/strong&gt;이라고 한다. dropping 확률은 hyperparameter로 일반적인 경우 0.5를 많이 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418212357585.png&quot; alt=&quot;image-20210418212357585&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래와 같이 코드로 구현할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# probability of keeping a unit active. higher = less dropout
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; X contains the data &quot;&quot;&quot;&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# forward pass for example 3-layer neural network
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# first dropout mask
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# drop!
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# second dropout mask
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# drop!
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# backward pass: compute gradients ... (not shown)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# perform parameter update ... (not shown)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;dropout은 다음과 같은 가정 하에 탄생했다. 예를 들어, 고양이 이미지를 판별하는 모델을 만들어보자. 모델은 뾰족한 귀가 있고, 길쭉한 꼬리가 있고, 털이 있으면 고양이라고 판단한다고 하자. 이때 스코티쉬폴드(귀가 접힌 고양이 종류)는 귀가 뾰족하지 않아서 고양이가 아니고, 스핑크스 고양이(털이 없는 고양이)는 털이 없어서 고양이가 아니라고 판별하는 사태가 벌어질 수 있다. 이러한 overfitting을 방지하기 위해 일부 node를 의도적으로 차단시켜 “귀가 납작하지만 길쭉한 꼬리와 털을 가졌으니 고양이”라고 판단하도록 하는 게 바로 dropout 방식이다.&lt;/p&gt;

&lt;p&gt;dropout을 model ensemble관점에서 보면 다음과 같이 해석할 수 있다. 각각의 binary mask를 하나의 model이라 보면, 같은 model parameters를 공유하는 여러 개의 model의 조합으로 볼 수 있다.&lt;/p&gt;

&lt;p&gt;한편, train time때는 확률(p)을 통해 일부 nodes를 zero로 만들어 학습하지만, test time때는 모든 nodes를 사용한다. 이때, 각 node가 켜질 확률을 반영하여 각 layer를 통과할 때마다 확률 p를 곱해주어야 한다. 예를 들어 이러한 동작을 수행하는 코드는 아래와 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ensembled forward pass
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# NOTE: scale the activations
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# NOTE: scale the activations
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;dropout을 수행하는 전체 코드(train+test)는 아래와 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Vanilla Dropout: Not recommended implementation (see notes below)&quot;&quot;&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# probability of keeping a unit active. higher = less dropout
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; X contains the data &quot;&quot;&quot;&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# forward pass for example 3-layer neural network
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# first dropout mask
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# drop!
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# second dropout mask
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# drop!
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# backward pass: compute gradients ... (not shown)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# perform parameter update ... (not shown)
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ensembled forward pass
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# NOTE: scale the activations
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# NOTE: scale the activations
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;train time에는 확률 p를 이용하여 일부 node를 drop시키고, test time에는 모든 nodes를 키되, 이러한 확률을 반영하여 activation function을 지날 때 마다 확률 p를 곱해준다. 이때 model parameters를 학습시킬 때 애초에 확률 p가 곱해진 형태로 나오도록 하는 트릭을 통해 아래와 같이 test time의 코드는 기존과 같이(확률 p를 반영하지 않도록) 만들 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Vanilla Dropout: Not recommended implementation (see notes below)&quot;&quot;&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# probability of keeping a unit active. higher = less dropout
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; X contains the data &quot;&quot;&quot;&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# forward pass for example 3-layer neural network
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ★ first dropout mask
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# drop!
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ★ second dropout mask
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# drop!
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# backward pass: compute gradients ... (not shown)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# perform parameter update ... (not shown)
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ensembled forward pass
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ★ no scaling necessary
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ★ no scaling necessary
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Data augmentation&lt;/strong&gt;은 입력 데이터에 약간의 변형을 가하여 데이터의 양을 늘리는 일종의 트릭이다. 데이터 변형은 “실제 상황에서 발생할 수 있을 정도”를 지키는 한에서 다음과 같이 다양하게 변형을 가할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;horizontal-flips&quot;&gt;Horizontal Flips&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418214849448.png&quot; alt=&quot;image-20210418214849448&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;random-crops-and-scales&quot;&gt;Random crops and scales&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418215158878.png&quot; alt=&quot;image-20210418215158878&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;일반적인 경우, 5개(양쪽 모서리와 정 가운데)로 쪼갠다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;color-jitter&quot;&gt;Color Jitter&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418215255521.png&quot; alt=&quot;image-20210418215255521&quot; /&gt;&lt;/p&gt;

&lt;p&gt;밝기나 대조값을 조정할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;그-외&quot;&gt;그 외&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418215602212.png&quot; alt=&quot;image-20210418215602212&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 외에 translation, rotation, stretching, shearing, lens distortions 등 다양한 변형 방법을 조합하여 나만의 data augmentation을 수행할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;dropconnect&quot;&gt;DropConnect&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;Training: Drop connections between neurons (set weights to 0)
Testing: Use all the connections
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;DropConnect&lt;/strong&gt;는 &lt;u&gt;뉴런간의 connections을 dropping하는 방식&lt;/u&gt;이다. nodes를 random하게 dropping하는 dropout과는 달리 nodes는 모두 살리고 nodes간의 connections을 random하게 dropping한다는 점에서 차이가 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418215812864.png&quot; alt=&quot;image-20210418215812864&quot; /&gt;&lt;/p&gt;

&lt;p&gt;dropconnect의 동작을 간단하게 표현하면 다음과 같다.&lt;/p&gt;

&lt;h2 id=&quot;cutout&quot;&gt;Cutout&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;Training: Set random image regions to zero
Testing: Use full image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418220059365.png&quot; alt=&quot;image-20210418220059365&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;train time에 cutout은 이미지의 일부 영역의 data값을 zero로 만듦으로써 수행된다.&lt;/p&gt;

&lt;h2 id=&quot;mixup&quot;&gt;Mixup&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;Training: Train on random blends of images
Testing: Use original images
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418220210576.png&quot; alt=&quot;image-20210418220210576&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Mixup은  train time에 서로 다른 두 개의 이미지를 섞고, 그 비율에 따라 target label을 정의함으로써 수행된다.&lt;/p&gt;

&lt;h2 id=&quot;cutmix&quot;&gt;Cutmix&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;Training: Train on random blends of images
Testing: Use original images
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418222009055.png&quot; alt=&quot;image-20210418222009055&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cutmix&lt;/strong&gt;는 train time에 서로 다른 두 개의 이미지를 잘라 붙이고, 그 비율에 따라 target label을 정의함으로써 수행된다.&lt;/p&gt;

&lt;h1 id=&quot;hyperparameter-tuning&quot;&gt;Hyperparameter Tuning&lt;/h1&gt;

&lt;p&gt;이전에 parameters는 모델이 학습하는 model parameters와 사람이 정의하는 hyperparameters로 나눠진다고 설명한 바 있다. 이러한 hyperparameters의 종류에는 다음의 것들이 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Network architecture (e.g., layers 개수, weights 개수)&lt;/li&gt;
  &lt;li&gt;Number of itarations&lt;/li&gt;
  &lt;li&gt;Learning rate(s) (i.e., solver parameters, decay, etc.)&lt;/li&gt;
  &lt;li&gt;Regularization (more later next lecture)&lt;/li&gt;
  &lt;li&gt;Batch size&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;grid-search-vs-random-search&quot;&gt;Grid Search vs. Random Search&lt;/h2&gt;

&lt;p&gt;아래는 hyperparameter를 선택하는 방법 중 grid search와 random search를 비교한 그림이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418222618991.png&quot; alt=&quot;image-20210418222618991&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;세로축 방향의 hyperparameter는 어떤 값이든 크게 중요치 않은 값이고, 가로축 방향의 hyperparameter는 특정 값(볼록 튀어나온 부분)에서 성능이 크게 향상되는 중요한 값이라고 하자. grid search의 경우 왼쪽 그림에서 확인할 수 있듯이 생각보다 성능이 나쁘다. 반면 오른쪽 그림과 같이 random search를 하는 경우, 성능이 좋은 hyperparameters를 발견할 가능성이 더 높다.&lt;/p&gt;

&lt;p&gt;이러한 random search를 통해 rough한 공간 내에서의 좋은 hyperparameters 영역을 찾은 뒤, 이후 세부적인 영역 내에서 더 좋은 hyperparameters를 찾는 방식을 적용하면 더 좋은 성능을 끌어올릴 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;example-run-coarse-search-for-5-epochs&quot;&gt;Example: run coarse search for 5 epochs&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;max_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ClassifierTrainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_two_layer_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# input size, hidden size, number of classes
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ClassifierTrainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;best_model_local&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;two_layer_net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'momentum'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate_decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_batchs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;실행결과:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418223443065.png&quot; alt=&quot;image-20210418223443065&quot; style=&quot;zoom:67%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일부 hyperparameters에 대하여 val_acc값이 대략 0.4 이상으로 좋게 나오는 것을 확인할 수 있다. 이러한 영역 내에서 더 좋은 hyperparameters를 찾기 위해 아래와 같이 수행할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;max_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ★
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ★
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;실행결과:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418223647345.png&quot; alt=&quot;image-20210418223647345&quot; style=&quot;zoom:62%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 영역 좁히기 과정을 통해 더 나은 hyperparameters를 결정할 수 있다. 이때 가장 성능이 좋은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;val_acc:0.53100, lr: 9471549e-04, reg: 1.433895e-03, (14/100)&lt;/code&gt; 을 살펴보면, 위에서 조정해준 영역 reg(-4,0), lr(-3,-4)의 경계면에 맞닿아있는 것을 확인할 수 있다. 이러한 경우 설정해준 경계면 바깥에 더 좋은 hyperparameters가 있을 수 있다는 합리적 의심을 해볼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;guideline&quot;&gt;Guideline&lt;/h2&gt;

&lt;h3 id=&quot;step1-check-initial-loss&quot;&gt;Step1: Check initial loss&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418224058971.png&quot; alt=&quot;image-20210418224058971&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 학습 초기에는 overfitting을 걱정할 필요가 없다 (오히려 overfitting으로 향해 나아가야 하는 단계이므로). 따라서 regularization loss(a.k.a. weight decay) 없이 학습 과정을 수행한다.&lt;/p&gt;

&lt;h3 id=&quot;step2-overfit-a-small-sample&quot;&gt;Step2: Overfit a small sample&lt;/h3&gt;

&lt;p&gt;전체 training data는 너무 크므로 small training data로 sampling을 한 뒤, 해당 samples에 대해서만 일단 최대한 100% training accuracy를 가지도록 architecture도 조금 수정하고, learning rate도 조금 수정하고, weigth initialization도 조금 손 봐 가며 학습을 수행한다 (일단은 overfitting 걱정하지 말고, 경향성을 살펴보자는 의미).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;예를 들어 loss가 줄어들지 않는다면 learning rate를 조금 키울 수 있고, loss가 발산하면 learning rate를 조금 줄이는 식으로 hyperparameters를 rough하게 결정할 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;step3-find-lr-that-makes-loss-go-down&quot;&gt;Step3: Find LR that makes loss go down&lt;/h3&gt;

&lt;p&gt;이후 전체 training data에 대하여 이전의 단계 (overfitting)를 수행해본다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;일반적으로 초기 learning rate의 값으로 1e-1, 1e-2, 1e-3, 1e-4를 주로 사용한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;step4-coarse-grid-train-for-1-5-epochs&quot;&gt;Step4: Coarse grid, train for ~1-5 epochs&lt;/h3&gt;

&lt;p&gt;step3으로 부터 확인한 영역 내에서 learning rate나 weight decay(regularization loss)등의 값을 선택한 뒤, 몇 개의 models에 대해 1~5 epochs만큼 train을 수행해 본다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;일반적으로 초기 weight decay값으로 1e-4, 1e-5, 0를 선택할 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;step5-refine-grid-train-longer&quot;&gt;Step5: Refine grid, train longer&lt;/h3&gt;

&lt;p&gt;step4에서 성능이 좋았던 models를 선택하여 learning rate decay 없이 좀 더 길게(10~20 epochs) training을 수행해본다.&lt;/p&gt;

&lt;h3 id=&quot;step6-look-at-loss-curves&quot;&gt;Step6: Look at loss curves&lt;/h3&gt;

&lt;p&gt;이때 loss curves를 확인해본다. loss curves는 일반적으로 다음의 경우로 나타날 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;training이 더 필요한 경우&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418225233564.png&quot; alt=&quot;image-20210418225233564&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;training accuracy와 validation accuracy가 모두 계속해서 증가하고 있다면 아직 학습이 더 필요한 상태라는 뜻이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;overfitting이 발생한 경우&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418225347502.png&quot; alt=&quot;image-20210418225347502&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;training accuracy는 증가하지만 validation accuracy는 오히려 감소한다면, overfitting이 발생한 것이다. 이런 경우 regularization을 증가시키거나 더 많은 data를 입력하는 방안을 택할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;underfitting이 발생한 경우&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\files\2022-02-06-ai-DeepLearning-DL03\image-20210418225510471.png&quot; alt=&quot;image-20210418225510471&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;training accuracy와 validation accuracy간에 gap이 매우 작은 경우는 underfitting되었음을 의미한다. 이런 경우 더 길게 training을 시키거나, 더 큰 model을 사용하는 방안을 택할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;step7-goto-step5&quot;&gt;Step7: GOTO step5&lt;/h3&gt;

&lt;p&gt;loss curve를 확인한 후 다시 step5로 돌아갈 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;내가 풀려는 문제와 유사한 모델을 가져와서 사용된 hyperparameters에서부터 시작해본다면 여러가지 시행착오를 줄일 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Sun, 06 Feb 2022 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//ai-deeplearning/DL03/</link>
        <guid isPermaLink="true">https://dazory.github.io//ai-deeplearning/DL03/</guid>
        
        <category>Deep Learning</category>
        
        <category>AIAS</category>
        
        
        <category>ai-deepLearning</category>
        
      </item>
    
      <item>
        <title>[I2MR-01] 01. Robot Control Paradigms</title>
        <description>&lt;p&gt;로봇의 제어 패러다임의 종류에 대해 간략하게 살펴본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;robot-control-paradigms&quot;&gt;Robot Control Paradigms&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;로봇 패러다임(&lt;em&gt;robot paradigms&lt;/em&gt;)&lt;/strong&gt;은 로봇이 작동하는 방식에 대한 정신적 모델을 의미한다. 여기서는 대표적인 로봇의 제어 패러다임 몇 가지에 대해 간략하게 살펴본다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;고전/계층 패러다임(&lt;em&gt;Classical/Hierarchical Paradigm&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;[Sense → Plan → Act → Sense … ]&lt;/li&gt;
      &lt;li&gt;센싱이 플랜을 거쳐 액션으로 매핑되는 패러다임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;반응적/행동-기반 패러다임(&lt;em&gt;Reactive/Behavior-based Paradigm&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Plan을 behavior로 대체&lt;/p&gt;

        &lt;p&gt;⇒ [Sense ↔ Act]&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;플랜의 과정이 사라지고, 센싱이 직접적으로 액션에 영향을 주는 패러다임&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;하이브리드 숙고/반응적 패러다임(&lt;em&gt;Hybrid Deliberative/Reactive Paradigm&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;앞선 패러다임의 장점들을 결합한 패러다임&lt;/li&gt;
      &lt;li&gt;[Sense ↔ Act]에 Plan이 영향을 줌&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;classicalhierarchical-paradigm&quot;&gt;Classical/Hierarchical Paradigm&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220115155508748.png&quot; alt=&quot;image-20220115155508748&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;u&gt;70년대&lt;/u&gt;, 로봇공학 초기에 사용되던 모델&lt;/li&gt;
  &lt;li&gt;&lt;u&gt;계획(planning)&lt;/u&gt;에 중점을 둔 패러다임으로, 세계를 센싱하고 다음 액션을 계획한 뒤 행동한다.&lt;/li&gt;
  &lt;li&gt;센싱 데이터는 &lt;u&gt;한 개의 글로벌 월드 모델&lt;/u&gt;로 모인다.&lt;/li&gt;
  &lt;li&gt;월드 모델은 &lt;u&gt;프레임 문제(*frame problem*)&lt;/u&gt;와 &lt;u&gt;폐쇄 월드 가정(*closed world assumption*)&lt;/u&gt;으로 인해 유지 관리가 어렵다는 한계가 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;예시&quot;&gt;예시&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220118002523615.png&quot; alt=&quot;image-20220118002523615&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;STRIPS (Stanford Research Institute Problem Solver)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;박스들을 찾아서 디자인된 위치로 옮기는 작업을 수행&lt;/li&gt;
      &lt;li&gt;Perfect world model : 완벽한 세계를 가정 (취득된 정보를 100% 신뢰가능)&lt;/li&gt;
      &lt;li&gt;Closed world assumption&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stanford CART (1973), Stanford AI Laboratory/CMU(Moravec)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;classifcal pradigm of stanford CART&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220115164006182.png&quot; alt=&quot;image-20220115164006182&quot; style=&quot;width:800px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reactivebehavior-based-paradigm&quot;&gt;Reactive/Behavior-based Paradigm&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;반응적/행동-기반 패러다임(&lt;em&gt;Reactive/Behavior-based Paradigm&lt;/em&gt;)&lt;/strong&gt;은 어떠한 모델 없이 감지한 정보가 곧장 행동으로 반영되는 패러다임이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220115164512394.png&quot; alt=&quot;image-20220115164512394&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Planning을 극단적으로 버려버린 패러다임&lt;/u&gt;이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;센싱된 정보들은 직접적으로 액션에 결합되며, 이는 각각 &lt;u&gt;행동(behavior)&lt;/u&gt;이라 불린다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이러한 Behavior 기반 접근은 planning의 몇 가지 레벨을 제공한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;지능의 살아있는 예시들을 조사하는 &lt;u&gt;생물학 및 인지 심리학에서 비롯&lt;/u&gt;되어, 로봇의 저비용 및 컴퓨팅 성능을 향상시켜준다.&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;&lt;strong&gt;내 생각 :&lt;/strong&gt;&lt;/p&gt;

      &lt;p&gt;생물학적 시스템을 모방한 뉴로모픽 비전(&lt;em&gt;Neuromorphic Vision&lt;/em&gt;) 역시 에너지 효율적으로 동작한다. 자연적으로 존재하는 구조가 가장 에너지 효율적일 수 있겠다(그야말로 자연적인 것이니깐).&lt;/p&gt;

      &lt;p&gt;참고1. &lt;a href=&quot;https://dazory.github.io/ai-computervision/neuromorphicVision01/#%ED%8A%B9%EC%A7%95&quot;&gt;[NeuromorphicVision-001] 뉴로모픽 비전(Neuromorphic Vision)의 배경 및 개념, Dazory Blog&lt;/a&gt;&lt;br /&gt;
참고2. &lt;a href=&quot;https://dazory.github.io/ai-computervision/neuromorphicVision02/&quot;&gt;[NeuromorphicVision-002] 뉴로모픽 비전(Neuromorphic Vision)의 현황(1), Dazory Blog&lt;/a&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Behavior(e.g., exploer, wander, avoid obstacles, …) 기반 접근은 다음과 같이 몇 가지 &lt;u&gt;planning 레벨&lt;/u&gt;을 제공한다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220116151310136.png&quot; alt=&quot;image-20220116151310136&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;특징&quot;&gt;특징&lt;/h3&gt;

&lt;p&gt;반응적 패러다임의 특징은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;에이전트가 위치한 로봇은 위치한 환경의 필수적인 부분이다.&lt;/li&gt;
  &lt;li&gt;&lt;u&gt;behavior&lt;/u&gt;은 로봇의 actions에 대한 &lt;u&gt;기초적인 빌딩 블록 역할&lt;/u&gt;을 한다.
    &lt;ul&gt;
      &lt;li&gt;로봇의 활동(action)은 시퀀스 혹은 동시적으로 동작하는 behaviors의 결과로서 나타난다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;u&gt;모듈화&lt;/u&gt; 가능한 behavior의 특성은 좋은 소프트웨어 디자인 원리를 제공한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;예시-1&quot;&gt;예시&lt;/h3&gt;

&lt;p&gt;반응적/행동-기반 패러다임을 구현한 대표적인 예시로 아래의 구조를 살펴본다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;포함 아키텍처(&lt;em&gt;Subsumption Architecture&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;포텐셜 필드 방법론(&lt;em&gt;Potential Fields Methodologies&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;subsumption-architecture&quot;&gt;Subsumption Architecture&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220118001651171.png&quot; alt=&quot;image-20220118001651171&quot; style=&quot;width:150px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;MIT의 Genghis는 subsumption 아키텍처로 개발되었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Rodney Brooks에 의해 86년 소개되었다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“Behaviors are networks of sensing and acting modules.”&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;AFSM(Augmented Finite State Machines)&lt;/p&gt;

        &lt;blockquote&gt;
          &lt;p&gt;유한 상태 기계(&lt;em&gt;Finite State Machine&lt;/em&gt;)는 컴퓨터 프로그램과 전자 논리 회로를 설계하는 데에 쓰이는 수학적 모델로, 유한한 개수의 상태를 가질 수 있는 오토마타, 즉 추상기계라고 할 수 있다. 이러한 기계는 한 번에 오로지 하나의 상태만을 가지게 되며, 현재 상태(Current State)란 임의의 주어진 시간의 상태를 칭한다.특정한 유한 오토마톤은 현재 상태로부터 가능한 전이 상태와, 이러한 전이를 유발하는 조건들의 집합으로서 정의된다. (위키백과)&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;내부적인 상태(internal state)가 존재하지 않는다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;포함 아키텍처(&lt;em&gt;subsumption architecture&lt;/em&gt;)&lt;/strong&gt;는 역량 계층들(&lt;em&gt;layers of competence&lt;/em&gt;)로 그룹화되며, 각 레이어들은 낮은 레벨의 레이어들을 포함하는(&lt;em&gt;subsume&lt;/em&gt;) 구조로 되어있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Level 0 : Avoid(회피)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220117233426833.png&quot; alt=&quot;image-20220117233426833&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Level 1 : Wander(배회)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220117233527033.png&quot; alt=&quot;image-20220117233527033&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Level 2 : Follow Corridor(복도를 따라감)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220117233643860.png&quot; alt=&quot;image-20220117233643860&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Subsumption architecture의 장점과 단점은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;장점
    &lt;ul&gt;
      &lt;li&gt;타겟 도메인에서의 실시간 시스템의 반복적인 개발 및 테스트에 중점을 둔다.&lt;/li&gt;
      &lt;li&gt;제한된 작업별 인식(&lt;em&gt;perception&lt;/em&gt;)을, 그것을 필요로하는 표현된 액션에 직접적으로 연결하는 것에 중점을 둔다.&lt;/li&gt;
      &lt;li&gt;분배 및 병렬 제어에 중점을 두어, 인식(&lt;em&gt;perception&lt;/em&gt;)과 제어(&lt;em&gt;control&lt;/em&gt;) 그리고 액션 시스템을 동물과 유사한 방식으로 통합한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;단점
    &lt;ul&gt;
      &lt;li&gt;고도로 분산된 방해 및 억제 시스템을 통해 적응 가능한 액션 선택을 설계하는 것이 어렵다.&lt;/li&gt;
      &lt;li&gt;대용량 메모리 및 상징적 표현이 부족하다. (아키텍처가 언어를 이해하는 것을 제한)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;potential-field-methods&quot;&gt;Potential Field Methods&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220118001748806.png&quot; alt=&quot;image-20220118001748806&quot; style=&quot;width:150px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GATech의 Callisto는 potential field 방법으로 개발되었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;로봇을 마치 포텐셜 필드의 영향 하에 있는 &lt;u&gt;입자(*particle*)&lt;/u&gt;처럼 다룬다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;로봇은 &lt;u&gt;포텐셜의 미분&lt;/u&gt;을 따라 운행한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;필드는 장애물, 원하는 운행 방향 및 타겟에 의해 형성&lt;/u&gt;된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;결과 필드(vector)는 기본 필드(&lt;em&gt;primitive fields&lt;/em&gt;)의 합으로 제공된다.&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;&lt;strong&gt;기본 포텐셜 필드(Primitive Potential Fields)의 예시&lt;/strong&gt;&lt;/p&gt;

      &lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220117235107763.png&quot; alt=&quot;image-20220117235107763&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;필드의 세기는 장애물/타겟까지의 거리에 따라 변화한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;로봇은 포텐셜 필드를 이용하여 복도를 따라갈 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Level 0 : Collision Avoidance&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;레벨 0에서는 감지된 장애물의 반발 필드(repulsive fields)에 의해 충돌 회피(&lt;em&gt;collision avoidance&lt;/em&gt;)가 수행된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Level 1 : Wander&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;레벨 1은 단일 필드(&lt;em&gt;uniform field&lt;/em&gt;)를 추가한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Level 2 : Corridor following&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;레벨 2에서는 세 개의 필드들(수직(&lt;em&gt;perpendicular&lt;/em&gt;) 2 + 단일(&lt;em&gt;uniform&lt;/em&gt;) 1)에 의해 배회 필드(&lt;em&gt;wander field&lt;/em&gt;)가 대체되어, 복도를 따라 가기 시작한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;포텐셜 필드의 특징은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;로컬 미니마(&lt;em&gt;local minima&lt;/em&gt;, 극소값)에 빠질 수 있다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220117235643302.png&quot; alt=&quot;image-20220117235643302&quot; style=&quot;width:200px;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;이에 대한 해결책으로 다음의 것들이 존재한다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;역추적(&lt;em&gt;Backtracking&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;로컬 미니멈에서 탈출하기 위한 랜덤 모션&lt;/li&gt;
      &lt;li&gt;벽을 따라가기 위한 절차 계획자(&lt;em&gt;procedural planner&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;방문한 지역들의 포텐셜을 증가&lt;/li&gt;
      &lt;li&gt;고조파 기능(&lt;em&gt;harmonic funcitons&lt;/em&gt;)을 통해 로컬 미니마를 회피&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;레이어 간 선호도가 존재하지 않는다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;시각화하기 용이하다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;다른 필드들과 합치기에 용이하다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;높은 업데이트 속도를 필요로 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;파라미터 튜닝이 중요하다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;hybrid-deliberativereactive-paradigm&quot;&gt;Hybrid Deliberative/Reactive Paradigm&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-23-I2MR-02robotControlParadigms/image-20220118000209760.png&quot; alt=&quot;image-20220118000209760&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;고전/계층적 패러다임(&lt;em&gt;Classical/Hierarchical Paradigm&lt;/em&gt;)과 반응적/행동-기반 패러다임(&lt;em&gt;Reactive/Behavior-based Paradigm&lt;/em&gt;)의 장점을 결합한 패러다임이다.
    &lt;ul&gt;
      &lt;li&gt;계획(planning)에 사용되는 월드 모델&lt;/li&gt;
      &lt;li&gt;폐쇄 루프, 반응적 컨트롤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;1990년부터 현재(2017)까지 주를 이루는 패러다임이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;“02. Robot Control Paradigms”, Introduction to Mobile Robotics - SS 2021, &lt;a href=&quot;http://ais.informatik.uni-freiburg.de/teaching/ss21/robotics/0&quot;&gt;http://ais.informatik.uni-freiburg.de/teaching/ss21/robotics/&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;“Subsumption architecture”, WIKIPEDIA, 2021년 11월 20일 수정, 2022년 01월 18일 접속, https://en.wikipedia.org/wiki/Subsumption_architecture](https://en.wikipedia.org/wiki/Subsumption_architecture).&lt;/li&gt;
  &lt;li&gt;“Hierarchical Control”, CS521: Robotics Winter 2017, Dr.Fang Tang, &lt;a href=&quot;https://www.cpp.edu/~ftang/courses/CS521/notes/hierarchical%20control.pdf&quot;&gt;https://www.cpp.edu/~ftang/courses/CS521/notes/hierarchical%20control.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;“Reactive Architecture”, CS521: Robotics Winter 2017, Dr.Fang Tang, &lt;a href=&quot;https://www.cpp.edu/~ftang/courses/CS521/notes/reactive%20architecture.pdf&quot;&gt;https://www.cpp.edu/~ftang/courses/CS521/notes/reactive%20architecture.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 23 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//robotics-mobilerobotics/I2MR01/</link>
        <guid isPermaLink="true">https://dazory.github.io//robotics-mobilerobotics/I2MR01/</guid>
        
        <category>robotics</category>
        
        <category>mobile robotics</category>
        
        <category>control</category>
        
        <category>paradigm</category>
        
        
        <category>robotics-mobileRobotics</category>
        
      </item>
    
      <item>
        <title>[LA-00] 선형대수 개요</title>
        <description>&lt;h1 id=&quot;linear-algebra&quot;&gt;Linear Algebra&lt;/h1&gt;

&lt;p&gt;여기서는 선형대수(&lt;em&gt;Linear Algebra&lt;/em&gt;)의 대략적인 개념을 살펴본다. 보다 자세한 개념은 이후의 포스팅을 통해 찬찬히 살펴보도록 한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;representations&quot;&gt;Representations&lt;/h2&gt;

&lt;p&gt;선형대수(&lt;em&gt;linear algebra&lt;/em&gt;)는 선형 방정식과 선형 변환에 관한 수학의 한 분야이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;선형 방정식 (&lt;em&gt;linear equation&lt;/em&gt;)&lt;/p&gt;

\[a_1x_1 + ... + a_nx_n = b\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;선형 변환 (&lt;em&gt;linear map&lt;/em&gt;)&lt;/p&gt;

\[(x_1, ..., x_n) \mapsto a_1x_1 + ... + a_nx_n\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이때 선형 방정식과 선형 변환은 &lt;u&gt;벡터 공간(vector space)&lt;/u&gt;과 &lt;u&gt;행렬(matrices)&lt;/u&gt;을 통해 표현된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;벡터 (&lt;em&gt;Vector&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;행렬 (&lt;em&gt;Matrix&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;vector&quot;&gt;Vector&lt;/h3&gt;

\[\textbf{a} = (a_{1}, a_{2}, ..., a_{n})\]

&lt;p&gt;&lt;strong&gt;벡터(&lt;em&gt;Vectors&lt;/em&gt;)&lt;/strong&gt;는 &lt;u&gt;숫자들의 배열&lt;/u&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;operations&quot;&gt;Operations&lt;/h4&gt;

&lt;h5 id=&quot;sum&quot;&gt;Sum&lt;/h5&gt;

&lt;p&gt;: $\textbf{a}_1 + \textbf{a}_2$&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h5 id=&quot;product&quot;&gt;Product&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;dl&gt;
      &lt;dt&gt;&lt;strong&gt;Scalar product&lt;/strong&gt;&lt;/dt&gt;
      &lt;dd&gt;
        &lt;p&gt;$k\textbf{a}$&lt;/p&gt;
      &lt;/dd&gt;
    &lt;/dl&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;dl&gt;
      &lt;dt&gt;&lt;strong&gt;Dot product&lt;/strong&gt;&lt;/dt&gt;
      &lt;dd&gt;
        &lt;p&gt;$\textbf{a}·\textbf{b} = \sum_{i}{a_i b_i}$ (Scalar)&lt;/p&gt;
      &lt;/dd&gt;
    &lt;/dl&gt;

    &lt;ul&gt;
      &lt;li&gt;If $|\textbf{a}| = 1$, then $\textbf{a}·\textbf{b}$ 는 $\textbf{a}$의 방향을 따라 $\textbf{b}$를 투영한 것의 길이&lt;/li&gt;
      &lt;li&gt;If $\textbf{a}·\textbf{b}=0$, then  $\textbf{a}$와 $\textbf{b}$는 orthogonal&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;properties&quot;&gt;Properties&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;$\textbf{b}=\sum_{i}{k_ia_i}$를 만족하는 ${k_i}$가…
    &lt;ul&gt;
      &lt;li&gt;존재 ⇒ Linearly Dependence&lt;/li&gt;
      &lt;li&gt;존재하지 않음 ⇒ Linearly Independence&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;n-차원 공간의 한 점으로 표현됨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;matrices&quot;&gt;Matrices&lt;/h3&gt;

\[\textbf{A}^{n×m} = 
\begin{bmatrix}
	a_{11} &amp;amp; a_{12} &amp;amp; ... &amp;amp; a_{1m} \\
	a_{21} &amp;amp; a_{22} &amp;amp; ... &amp;amp; a_{2m} \\
	\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
	a_{n1} &amp;amp; a_{n2} &amp;amp; ... &amp;amp; a_{nm}
\end{bmatrix}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;properties-1&quot;&gt;Properties&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Matrices &amp;amp; Vectors&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$d$-차원 vector는 $d×1$ matrix로 표현 가능&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Column vectors&lt;/strong&gt;&lt;/p&gt;

\[\textbf{a}_{*} = (\textbf{a}_{*1}, \textbf{a}_{*2}, ..., \textbf{a}_{*m})\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Row vectors&lt;/strong&gt;&lt;/p&gt;

\[\textbf{a}_* = (\textbf{a}^T_{1*}; \textbf{a}^T_{2*}; ...; \textbf{a}^T_{n*};)\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;operations-1&quot;&gt;Operations&lt;/h2&gt;

&lt;p&gt;행렬의 다양한 연산에 대해 알아보자&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sum-1&quot;&gt;Sum&lt;/h3&gt;

&lt;h4 id=&quot;the-sum-of-two-vectors&quot;&gt;The sum of two vectors&lt;/h4&gt;

\[\textbf{a}+\textbf{b} =
\begin{bmatrix} a_1\\ a_2\\ \vdots\\ a_n \end{bmatrix}
+
\begin{bmatrix} b_1\\ b_2\\ \vdots\\ b_n \end{bmatrix}
=
\begin{bmatrix} a_1+b_1\\ a_2+b_2\\ \vdots\\ a_n+b_n \end{bmatrix}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;the-sum-of-two-matrices&quot;&gt;The sum of two matrices&lt;/h4&gt;

\[\displaylines{
{\textbf{A}+\textbf{B}} =

{\begin{bmatrix}
	a_{11} &amp;amp; a_{12} &amp;amp; ... &amp;amp; a_{1m} \\
	a_{21} &amp;amp; a_{22} &amp;amp; ... &amp;amp; a_{2m} \\
	\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
	a_{n1} &amp;amp; a_{n2} &amp;amp; ... &amp;amp; a_{nm}
\end{bmatrix}
+
\begin{bmatrix}
	b_{11} &amp;amp; b_{12} &amp;amp; ... &amp;amp; b_{1m} \\
	b_{21} &amp;amp; b_{22} &amp;amp; ... &amp;amp; b_{2m} \\
	\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
	b_{n1} &amp;amp; b_{n2} &amp;amp; ... &amp;amp; b_{nm}
\end{bmatrix}}
\\
= 
{\begin{bmatrix}
	a_{11}+b_{11} &amp;amp; a_{12}+b_{12} &amp;amp; ... &amp;amp; a_{1m}+b_{1m} \\
	a_{21}+b_{21} &amp;amp; a_{22}+b_{22} &amp;amp; ... &amp;amp; a_{2m}+b_{2m} \\
	\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
	a_{n1}+b_{n1} &amp;amp; a_{n2}+b_{n2} &amp;amp; ... &amp;amp; a_{nm}+b_{nm}
\end{bmatrix}}
}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;multiplication&quot;&gt;Multiplication&lt;/h3&gt;

&lt;h4 id=&quot;scalar-multiplication&quot;&gt;Scalar Multiplication&lt;/h4&gt;

\[\displaylines{
k\textbf{A} = 
\begin{bmatrix}
	ka_{11} &amp;amp; ka_{12} &amp;amp; ... &amp;amp; ka_{1m} \\
	ka_{21} &amp;amp; ka_{22} &amp;amp; ... &amp;amp; ka_{2m} \\
	\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
	ka_{n1} &amp;amp; ka_{n2} &amp;amp; ... &amp;amp; ka_{nm}
\end{bmatrix}
}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;matrix-vector-product&quot;&gt;Matrix Vector Product&lt;/h4&gt;

\[\displaylines{
\textbf{A}\textbf{b} =
\begin{bmatrix} \textbf{a}^T_{1*} \\ \textbf{a}^T_{2*} \\ \vdots\\ \textbf{a}^T_{n*} \end{bmatrix} · \textbf{b}\\
=
\begin{bmatrix} \textbf{a}^T_{1*}·\textbf{b} \\ \textbf{a}^T_{2*}·\textbf{b} \\ \vdots\\ \textbf{a}^T_{n*}·\textbf{b} \end{bmatrix} \\
=
\sum_k{a_{*k}·b_{k}}
}\]

&lt;blockquote&gt;
  &lt;p&gt;e.g.,
\(\begin{bmatrix} 1&amp;amp;2&amp;amp;2\\2&amp;amp;3&amp;amp;1\\1&amp;amp;0&amp;amp;2 \end{bmatrix}
\begin{bmatrix} 1\\ 3\\ 5 \end{bmatrix}
=\begin{bmatrix} 
	\begin{bmatrix} 1&amp;amp;2&amp;amp;2 \end{bmatrix}
    \begin{bmatrix} 1\\ 3\\5 \end{bmatrix} \\ 
    \begin{bmatrix} 2&amp;amp;3&amp;amp;1 \end{bmatrix}
	\begin{bmatrix} 1\\ 3\\5 \end{bmatrix} \\ 
    \begin{bmatrix} 1&amp;amp;0&amp;amp;2 \end{bmatrix}
    \begin{bmatrix} 1\\ 3\\5 \end{bmatrix} \\ 
\end{bmatrix}
=
\begin{bmatrix}
	1+6+10\\
	2+9+5\\
	1+0+10
\end{bmatrix}
=
\begin{bmatrix} 17\\ 16\\ 11 \end{bmatrix}\)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;$\textbf{A}$의 column vectors가 reference system을 나타낸다면, $\textbf{A}\textbf{b}$는 ${a_{*i}}$에 따른 vector $\textbf{b}$의 global transformation이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;matrix-matrix-product&quot;&gt;Matrix Matrix Product&lt;/h4&gt;

\[\displaylines{
\textbf{C} = \textbf{A}\textbf{B} \\
=
\begin{bmatrix}
	a_{11} &amp;amp; a_{12} &amp;amp; ... &amp;amp; a_{1l} \\
	a_{21} &amp;amp; a_{22} &amp;amp; ... &amp;amp; a_{2l} \\
	\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
	a_{n1} &amp;amp; a_{n2} &amp;amp; ... &amp;amp; a_{nl}
\end{bmatrix}
\begin{bmatrix}
	b_{11} &amp;amp; b_{12} &amp;amp; ... &amp;amp; b_{1m} \\
	b_{21} &amp;amp; b_{22} &amp;amp; ... &amp;amp; b_{2m} \\
	\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
	b_{l1} &amp;amp; b_{l2} &amp;amp; ... &amp;amp; b_{lm}
\end{bmatrix}\\
=
\begin{bmatrix} 
	\textbf{a}^T_{1*}\\ \textbf{a}^T_{2*}\\ \vdots\\ \textbf{a}^T_{n*}
\end{bmatrix}
\begin{bmatrix}
	\textbf{b}^T_{*1} &amp;amp; \textbf{b}^T_{*2} &amp;amp; ... &amp;amp; \textbf{b}^T_{*m}
\end{bmatrix} \\
=

\begin{bmatrix} 
	\textbf{A}\textbf{b}^T_{*1} &amp;amp; \textbf{A}\textbf{b}^T_{*2} &amp;amp; ... &amp;amp; \textbf{A}\textbf{b}^T_{*m}
\end{bmatrix}
}\]

&lt;p&gt;: row vectors와 column vector의 dot product로 계산 가능하다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The columns of $\textbf{C}$ : the transformation of the columns of $\textbf{B}$ through $\textbf{A}$.&lt;/p&gt;

\[\textbf{C} = \textbf{A}\textbf{B} = 
\begin{bmatrix}
	\textbf{A}\textbf{b}_{*1} &amp;amp; \textbf{A}\textbf{b}_{*2} &amp;amp; ... &amp;amp; \textbf{A}\textbf{b}_{*m}
\end{bmatrix}\]

\[\textbf{c}_{*i} = \textbf{A}\textbf{b}_{*i}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rank&quot;&gt;Rank&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;랭크(&lt;em&gt;Rank&lt;/em&gt;)&lt;/strong&gt; 는 &lt;u&gt;선형 독립인 행(또는 열)의 최대 개수&lt;/u&gt; 혹은 &lt;u&gt;dimension of the image of the transformation $f(\textbf{x}) = \textbf{A}\textbf{x}$&lt;/u&gt;을 의미한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;e.g., $rank(\textbf{A}) = 2$ when …&lt;/p&gt;

\[A={\begin{bmatrix}1&amp;amp;2&amp;amp;0&amp;amp;1\\0&amp;amp;0&amp;amp;1&amp;amp;1\\0&amp;amp;0&amp;amp;0&amp;amp;0\\0&amp;amp;0&amp;amp;0&amp;amp;0\\\end{bmatrix}}\]
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Properties&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;$rank(\textbf{A})≥0$   ⇔   $\textbf{A}^{m×n}$ : null matrix&lt;/li&gt;
      &lt;li&gt;$rank(\textbf{A})≤ \min(m,n)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Computations&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Gaussian elimination on the matrix&lt;/li&gt;
      &lt;li&gt;Counting the number of non-zero rows&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;inverse&quot;&gt;Inverse&lt;/h3&gt;

\[\textbf{A}\textbf{B} = \textbf{I}\\
\textbf{B}=\textbf{A}^{-1} (\text{inverse matrix of $\textbf{A}$)}\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;조건&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$\textbf{A}$ : a square matrix of full rank&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;특징&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;역행렬이 존재한다면, 그 역행렬은 유일(unique)하다.&lt;/li&gt;
      &lt;li&gt;$\textbf{A}\textbf{A}^{-1} = \textbf{I} = \textbf{A}^{-1}\textbf{A}$&lt;/li&gt;
      &lt;li&gt;$(\textbf{A}\textbf{B})^{-1} = \textbf{B}^{-1}\textbf{A}^{-1}$&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$(\textbf{A}+\textbf{B})^{-1} ≠ \textbf{A}^{-1} + \textbf{B}^{-1}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$\textbf{A}$의 $i$th row와 $\textbf{A}^{-1}$의 $j$th column은..&lt;/p&gt;

\[\begin{cases}
	\text{orthogonal} &amp;amp; \text{if $i≠j$} \\
	\text{their dot product is $1$} &amp;amp; \text{if $i=j$}
\end{cases}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;응용&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;선형방정식의 해 구하기 : $\textbf{x} = \textbf{A}^{-1}\textbf{b}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;determinant&quot;&gt;Determinant&lt;/h3&gt;

\[\displaylines{
\det(\textbf{A}) =
a_{11}\textbf{C}_{11} + a_{12}\textbf{C}_{12} + ... + a_{1n}\textbf{C}_{1n} \\
= \sum_{j=1}^n{a_{1j}\textbf{C}_{1j}}
}\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;계산&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Cofactor expansion&lt;/li&gt;
      &lt;li&gt;Gauss elimination&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;특징&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;$\textbf{A}^{n×n}$이라고 할 때 …&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$\det(\textbf{A})≠0$ ⇔ $\textbf{A}$의 역행렬이 존재&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Row operations&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;$\textbf{B}$ : $\textbf{A}$의 두 행을 interchanging한 결과라면 ⇒ $\det(\textbf{B}) = -\det(\textbf{A})$&lt;/li&gt;
          &lt;li&gt;$\textbf{B}$ : $\textbf{A}$의 한 행에 숫자 $c$를 곱한 결과하면 ⇒ $\det(\textbf{B}) = c·\det(\textbf{A})$&lt;/li&gt;
          &lt;li&gt;$\textbf{B}$ : $\textbf{A}$의 한 행의 배수를 다른 행에 더한 결과라면 ⇒ $\det(\textbf{B}) = \det(\textbf{A})$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$\det(\textbf{A}^T) = \det(\textbf{A})$&lt;/li&gt;
      &lt;li&gt;$\det(\textbf{A}^{-1}) = \frac{1}{\det(\textbf{A})}$&lt;/li&gt;
      &lt;li&gt;$\det(\textbf{P}\textbf{A}\textbf{P}^{-1}) = \det(\textbf{A})$&lt;/li&gt;
      &lt;li&gt;$\det(c\textbf{A}) = c^n \det(\textbf{A})$&lt;/li&gt;
      &lt;li&gt;
\[\det(\text{diag}(a_{11},...,a_{nn})) 
    = \begin{vmatrix} 
        a_{11}&amp;amp;*&amp;amp;0\\
        *&amp;amp;\ddots&amp;amp;*\\
        0&amp;amp;*&amp;amp;a_{nn}
    \end{vmatrix} = a_{11}a_{22}···a_{nn}\]
      &lt;/li&gt;
      &lt;li&gt;
\[\begin{vmatrix}
        a_{11}&amp;amp;&amp;amp;0\\
        \vdots&amp;amp;\ddots&amp;amp;&amp;amp;\\
        a_{n1}&amp;amp;...&amp;amp;a_{nn}
    \end{vmatrix}
    =a_{11}a_{22}···a_{nn}\]
      &lt;/li&gt;
      &lt;li&gt;$\det(\textbf{A}·\textbf{B}) = \det(\textbf{A}) · \det(\textbf{B})$&lt;/li&gt;
      &lt;li&gt;$\det(\textbf{A}+\textbf{B}) ≠ \det(\textbf{A}) + \det(\textbf{B})$&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;응용&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;기하적 분석
        &lt;ul&gt;
          &lt;li&gt;역행렬의 존재 판별 : $\det(\textbf{A})≠0$ ⇒ 역행렬이 존재&lt;/li&gt;
          &lt;li&gt;선형변환의 스케일 성분 : $\text{area, volume, …} = \lvert \det(\textbf{A}) \rvert $&lt;/li&gt;
          &lt;li&gt;도형의 방향 보존 : $\det(\textbf{A})&amp;gt;0$ ⇒ 방향이 보존&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Eigenvalues 계산 : $D(\lambda) = \det(\textbf{A}-\lambda \textbf{I})=0$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;특수행렬&quot;&gt;특수행렬&lt;/h3&gt;

&lt;h4 id=&quot;orthogonal-matrix&quot;&gt;Orthogonal Matrix&lt;/h4&gt;

&lt;p&gt;모든 column(or row) vectors가 직교(&lt;em&gt;orthonormal&lt;/em&gt;)하는 $\textbf{Q}$를 &lt;strong&gt;직교 행렬(&lt;em&gt;orthogonal&lt;/em&gt;)&lt;/strong&gt;이라고 한다.&lt;/p&gt;

\[q^T_{*i}·q_{*j} = 
\begin{cases}
	1 &amp;amp; \text{if $i=j$}\\
	0 &amp;amp; \text{if $i≠j$}
\end{cases},
 ∀i,j\]

&lt;p&gt;※ i.e., 본인과 같은 column(row) vector와의 내적은 1이고, 그 외는 0 ※&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;특징&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;선형 변환으로서 norm은 보존된다. (norm = 1)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$\textbf{Q}\textbf{Q}^T = \textbf{Q}^T\textbf{Q} = \textbf{I}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;행렬식은 unity norm(±1)을 갖는다 :&lt;/p&gt;

\[1 = \det(\textbf{I}) = \det(\textbf{Q}^T\textbf{Q})=\det(\textbf{Q})\det(\textbf{Q}^T)=\det(\textbf{Q})^2\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;symmetric-matrix&quot;&gt;Symmetric Matrix&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;대칭행렬(&lt;em&gt;symmetric matrix&lt;/em&gt;)&lt;/strong&gt;은 정방행렬에서 대각선에 대칭인 두 원소가 같은 행렬을 의미한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;e.g., 대칭행렬의 예시&lt;/p&gt;

\[\begin{bmatrix}
	2&amp;amp;1\\ 1&amp;amp;5
\end{bmatrix},
\begin{bmatrix}
	3&amp;amp;6&amp;amp;1\\ 6&amp;amp;2&amp;amp;2\\ 1&amp;amp;2&amp;amp;4
\end{bmatrix}\]
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;특징&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;실수인 고유값을 갖는다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;positive-definite-matrix&quot;&gt;Positive Definite Matrix&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;양의 정부호 행렬(&lt;em&gt;Positive Definite Matrix&lt;/em&gt;)&lt;/strong&gt; 은 모든 고유값(&lt;em&gt;eigenvalues&lt;/em&gt;)이 양수인 대칭행렬(&lt;em&gt;symmetric matrix&lt;/em&gt;)이다. (i.e., 대칭 행렬의 특수한 케이스)&lt;/p&gt;

\[\textbf{M} &amp;gt; 0 \\ 
\text{iff } \textbf{z}^T\textbf{M}\textbf{z}&amp;gt;0, ∀z≠0\\\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;예시&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;양의 정부호행렬의 예시1&lt;/p&gt;

\[\textbf{M}_1 = \begin{bmatrix} 1&amp;amp;0\\0&amp;amp;1\end{bmatrix} \text{일 때, }\\
\begin{bmatrix}z_1 &amp;amp; z_2\end{bmatrix}
\begin{bmatrix}1&amp;amp;0\\0&amp;amp;1\end{bmatrix}
\begin{bmatrix}z_1\\z_2\end{bmatrix}
= z_1^2+z_2^2 &amp;gt; 0\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;양의 정부호행렬의 예시2&lt;/p&gt;

\[\textbf{M}_2 = \begin{bmatrix}2&amp;amp;-1&amp;amp;0\\-1&amp;amp;2&amp;amp;-1\\0&amp;amp;-1&amp;amp;2\end{bmatrix} \text{일 때,}\\
\begin{bmatrix}z_1 &amp;amp; z_2 &amp;amp; z_3\end{bmatrix}
\begin{bmatrix}2&amp;amp;-1&amp;amp;0\\-1&amp;amp;2&amp;amp;-1\\0&amp;amp;-1&amp;amp;2\end{bmatrix}
\begin{bmatrix}z_1\\z_2\\z_3\end{bmatrix} \\
= 
2z_1^2-2z_1z_2 + 2z_2^2 - 2z_2z_3 + 2z_3^2\\
= z_1^2 + (z_1-z_2)^2 + (z_2-z_3)^2 + z_3^2 &amp;gt;0\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;특징&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Invertible&lt;/strong&gt;, with positive definite inverse&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;모든 실수 eigenvalues &amp;gt; 0&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Trace&lt;/strong&gt; is &amp;gt; 0&lt;/p&gt;

        &lt;blockquote&gt;
          &lt;p&gt;&lt;strong&gt;대각합(&lt;em&gt;Trace&lt;/em&gt;)&lt;/strong&gt;은 정사각행렬의 주대각성 성분의 합을 의미한다.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Cholesky&lt;/strong&gt; decomposition $\textbf{A} = \textbf{L}\textbf{L}^T$&lt;/p&gt;

        &lt;p&gt;※ $\textbf{L}$ : 하삼각행렬&lt;/p&gt;

        &lt;blockquote&gt;
          &lt;p&gt;&lt;strong&gt;숄레스키 분해(&lt;em&gt;Cholesky decomposition&lt;/em&gt;)&lt;/strong&gt;는 양의 정부호행렬(positive-definite matrix)의 분해에서 사용된다.&lt;/p&gt;

\[\textbf{A} = \textbf{L}\textbf{L}^*\]

          &lt;p&gt;이때 $\textbf{A}$의 모든 성분이 실수이면, $\textbf{L}$의 모든 성분도 실수이므로 다음과 같이 분해된다.&lt;/p&gt;

\[$\textbf{A}=\textbf{L}\textbf{L}^T\]
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;rotation-matrix&quot;&gt;Rotation Matrix&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;회전변환행렬(&lt;em&gt;rotation matrix&lt;/em&gt;)&lt;/strong&gt;은 $\det=±1$을 만족하는 직교행렬(orthonormal matrix)이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;예시&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;2D rotations&lt;/strong&gt;&lt;/p&gt;

\[R(\theta) = 
\begin{bmatrix}
	\cos(\theta) &amp;amp; -\sin(\theta)\\
	\sin(\theta) &amp;amp; \cos(\theta)
\end{bmatrix}\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;3D rotations along the main axes&lt;/strong&gt;&lt;/p&gt;

\[R_x(\theta) = 
\begin{bmatrix}
	1&amp;amp;0&amp;amp;0\\
	0&amp;amp;\cos(\theta)&amp;amp;-\sin(\theta)\\
	0&amp;amp;\sin(\theta)&amp;amp;\cos(\theta)
\end{bmatrix}\]

\[R_y(\theta) = 
\begin{bmatrix}
	\cos(\theta)&amp;amp;0&amp;amp;-\sin(\theta)\\
	0&amp;amp;1&amp;amp;0\\
	\sin(\theta)&amp;amp;0&amp;amp;\cos(\theta)
\end{bmatrix}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;특징&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Not commutative&lt;/strong&gt;&lt;/p&gt;

        &lt;blockquote&gt;
          &lt;p&gt;e.g.,
\(R_x(\frac{\pi}{4})·R_y(\frac{\pi}{4}) = ... = \begin{bmatrix}-1.414\\ 0.586\\ 3.414\end{bmatrix}\\
≠ R_y(\frac{\pi}{4})·R_x(\frac{\pi}{4}) = ... = \begin{bmatrix}-1.793\\ 0.707\\ 3.207\end{bmatrix}\)&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;affine-transformations&quot;&gt;Affine Transformations&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;아핀 변환(&lt;em&gt;Affine Transformation&lt;/em&gt;)&lt;/strong&gt;이란, 기하학적 성질을 보존하는 두 아핀 공간 사이의 함수이다.&lt;/p&gt;

  &lt;p&gt;※ 점, 직선, 평면이 보존되는 선형 매핑 ※&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;3차원 변환을 묘사하는 가장 쉽고 대중적인 방법은 행렬을 이용하는 것이다.&lt;/p&gt;

\[\textbf{A} = \begin{bmatrix}\textbf{R}&amp;amp;\textbf{t}\\\textbf{0}&amp;amp;\textbf{1}\end{bmatrix} \textbf{A}^{-1}
= \begin{bmatrix}
	\textbf{R}^T &amp;amp; -\textbf{R}^T\textbf{t} \\
	\textbf{0} &amp;amp; \textbf{1}
\end{bmatrix} \textbf{p}
=
\begin{bmatrix}\textbf{t}\\\textbf{1}\end{bmatrix}\]

&lt;ul&gt;
  &lt;li&gt;$\textbf{R}$ : Rotation matrix&lt;/li&gt;
  &lt;li&gt;$\textbf{t}$ : Translation vector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;combining-transformations&quot;&gt;Combining Transformations&lt;/h3&gt;

&lt;p&gt;변환 행렬(&lt;em&gt;transformation matrices&lt;/em&gt;)를 여러 개 결합하여(chaining) 간단하게 하나의 변환 행렬로 표현할 수 있다. 예를 들어 다음과 같이 로봇과 센서, 객체에 대한 행렬이 주어졌다고 가정하자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-16-00_Overall_Linear_Algebra/image-20220114223349649.png&quot; alt=&quot;image-20220114223349649&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$\textbf{A}$ : the pose of a robot in the space&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\textbf{B}$ : the position of a sensor on the robot&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The sensor perceives an object at a given location $\textbf{p}$, in its own frame&lt;/p&gt;

    &lt;p&gt;※ The sensor has no clue on where it is in the world ※&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이때 global frame에서 object의 위치는 다음의 과정으로 구할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\textbf{B}\textbf{p}$ : the pose of the object wrt the robot&lt;/li&gt;
  &lt;li&gt;$\textbf{A}\textbf{B}\textbf{p}$ : the pose of the object wrt the world&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;01.Linear Algebra, Introduction to Mobile Robotics - SS 2021, &lt;a href=&quot;http://ais.informatik.uni-freiburg.de/teaching/ss21/robotics/0&quot;&gt;http://ais.informatik.uni-freiburg.de/teaching/ss21/robotics/&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;“[선형대수학] 양의 정부호 행렬(positive definite matrix)이란?.txt”, bskyvision, 2017년 11월 22일 수정, 2022년 01월 14일 접속, &lt;a href=&quot;https://bskyvision.com/205&quot;&gt;https://bskyvision.com/205&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;“[기계학습] Positive-Definite Matrix 란?”, 자연의 원리에 귀를 기울이다 네이버 블록, 2017년 12월 7일 수정, 2022년 01월 14일 접속, &lt;a href=&quot;https://m.blog.naver.com/sw4r/221157302215&quot;&gt;https://m.blog.naver.com/sw4r/221157302215&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 16 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//mathematics-linearalgebra/00_Overall_Linear_Algebra/</link>
        <guid isPermaLink="true">https://dazory.github.io//mathematics-linearalgebra/00_Overall_Linear_Algebra/</guid>
        
        <category>Mathematics</category>
        
        <category>Linear Algebra</category>
        
        
        <category>mathematics-linearAlgebra</category>
        
      </item>
    
      <item>
        <title>[ROS2-002] 002. 메시지 통신(message communication)</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;주요 참고자료&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        페이지: &lt;a href=&quot;https://cafe.naver.com/openrt/25288&quot;&gt;001 ROS 2 개발 환경 구축 (오픈소스 소프트웨어 &amp;amp; 하드웨어: 로봇 기술 공유 카페 (오로카))&lt;/a&gt;&lt;br /&gt;
        작성자: 표윤석
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;본 글은 오로카(오픈 로보틱스 커뮤니티)의 표윤석님께서 작성하신 로봇 운영체제 ROS2 강좌를 공부한 뒤 정리한 내용으로, 내용의 생략과 수정이 있습니다. 원문을 참고하고싶으신 분들은 본 글의 References 파트를 참고해주시길 바랍니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;메시지-통신message-communication&quot;&gt;메시지 통신(&lt;em&gt;message communication&lt;/em&gt;)&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;노드(&lt;em&gt;node&lt;/em&gt;)&lt;/strong&gt; : 최소 단위의 실행가능한 프로세스&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;메시지(&lt;em&gt;message&lt;/em&gt;)&lt;/strong&gt; : integer, floating point, boolean 등과 같은 변수 형태의 데이터&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;ROS2에서 &lt;strong&gt;노드(&lt;em&gt;node&lt;/em&gt;)&lt;/strong&gt;끼리 데이터를 주고받는 것을 &lt;strong&gt;메시지 통신(&lt;em&gt;message communication&lt;/em&gt;)&lt;/strong&gt;이라고 한다. 이때 메시지 통신은 방법에 따라 다음과 같이 세 가지로 나누어진다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;토픽 (&lt;em&gt;topic&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;서비스 (&lt;em&gt;service&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;액션 (&lt;em&gt;action&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;파라미터 (&lt;em&gt;parameter&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;간단-비교--토픽-vs-서비스-vs-액션&quot;&gt;간단 비교 : 토픽 vs. 서비스 vs. 액션&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;인터페이스(&lt;em&gt;interface&lt;/em&gt;) : 데이터 통신에서 사용되는 데이터의 형태&lt;/p&gt;

      &lt;p&gt;다음의 세 가지 종류가 있음&lt;/p&gt;

      &lt;ul&gt;
        &lt;li&gt;msg 인터페이스&lt;/li&gt;
        &lt;li&gt;srv 인터페이스&lt;/li&gt;
        &lt;li&gt;action 인터페이스&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;비교대상&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;토픽(topic)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;서비스(service)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;액션(action)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;연속성&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;연속성&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;일회성&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;복합(토픽+서비스)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;방향성&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;단방향&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;양방향&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;양방향&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;동기성&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;비동기&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;동기&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;동기+비동기&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;다자간 연결&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:1, 1:N, N:1, N:N&lt;br /&gt;(publisher:subscriber)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:1&lt;br /&gt;(server:client)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:1&lt;br /&gt;(server:client)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;노드 역할&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;발행자 (publisher)&lt;br /&gt;구독자 (subscriber)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;서버 (server)&lt;br /&gt;클라이언트 (client)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;서버 (server)&lt;br /&gt;클라이언트 (client)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;동작 트리거&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;발행자&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;클라이언트&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;클라이언트&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;인터페이스&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msg 인터페이스&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;srv 인터페이스&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;action 인터페이스&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;CLI 명령어&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ros2 topic&lt;br /&gt;ros2 interface&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ros2 service&lt;br /&gt;ros2 interface&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ros2 action&lt;br /&gt;ros2 interface&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;사용 예&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;센서 데이터, 로봇 상태, 로봇 좌표, 로봇 속도 명령 등&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;LED 제어, 모터 토크 On/Off, IK/FK 계산, 이동 경로 계산 등&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;목적지로 이동, 물건 파지, 복합 테스크 등&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;토픽-topic&quot;&gt;토픽 (&lt;em&gt;topic&lt;/em&gt;)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/Selection_045.png&quot; alt=&quot;img&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;노드의 역할&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;u&gt;Publisher&lt;/u&gt; : msg를 발간 (동작 트리거)&lt;/li&gt;
      &lt;li&gt;&lt;u&gt;Subscriber&lt;/u&gt; : msg를 구독&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;메시지 형태&lt;/strong&gt; : &lt;u&gt;메시지 (message)&lt;/u&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;메시지 인터페이스&lt;/strong&gt; : &lt;u&gt;msg 인터페이스&lt;/u&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;특징&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;u&gt;연속성&lt;/u&gt;, &lt;u&gt;단방향&lt;/u&gt;, &lt;u&gt;비동기성&lt;/u&gt;&lt;/li&gt;
      &lt;li&gt;&lt;u&gt;1:1 (default), 1:N, N:1 N:N 통신 가능&lt;/u&gt; + 셀프 구독도 가능!&lt;/li&gt;
      &lt;li&gt;ROS 메시지 통신에서 &lt;u&gt;가장 널리 사용되는 통신 방법&lt;/u&gt; (대략 70%를 차지)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;사용 예시&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;센서 데이터, 로봇 상태, 로봇 좌표, 로봇 속도 명령 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cli-명령어&quot;&gt;&lt;strong&gt;CLI 명령어&lt;/strong&gt;&lt;/h3&gt;

&lt;h4 id=&quot;ros2-node&quot;&gt;ros2 node&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 node info&lt;/code&gt; : 노드 정보 확인&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108164818176.png&quot; alt=&quot;image-20220108164818176&quot; style=&quot;height:200px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;ros2-topic&quot;&gt;ros2 topic&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 topic list&lt;/code&gt; : 현재 개발환경에서 동작중인 모든 노드의 토픽 정보 확인 가능&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108164913375.png&quot; alt=&quot;image-20220108164913375&quot; style=&quot;height:100px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 topic info&lt;/code&gt; : 토픽 메시지 형태, 토픽의 발행 및 구독 정보 확인&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108164932672.png&quot; alt=&quot;image-20220108164932672&quot; style=&quot;height:50px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 topic echo&lt;/code&gt; : 특정 토픽의 메시지 내용을 실시간으로 출력&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108165002143.png&quot; alt=&quot;image-20220108165002143&quot; style=&quot;height:150px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 topic bw&lt;/code&gt; : 메시지의 대역폭(송수신받는 토픽 메시지의 크기) 확인&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108165018275.png&quot; alt=&quot;image-20220108165018275&quot; style=&quot;height:50px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 topic hz&lt;/code&gt; : 토픽의 주기 확인&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108165044827.png&quot; alt=&quot;image-20220108165044827&quot; style=&quot;height:50px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 topic delay&lt;/code&gt; : 토픽의 지연시간 확인&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108165200344.png&quot; alt=&quot;image-20220108165200344&quot; style=&quot;height:50px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 topic pub&lt;/code&gt; : 토픽 발행&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108165343632.png&quot; alt=&quot;image-20220108165343632&quot; style=&quot;height:100px&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;ros2-bag&quot;&gt;ros2 bag&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;rosbag : 토픽을 파일 형태로 저장/불러오는 ROS의 기능으로 디버깅시 유용&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 bag record&lt;/code&gt; : rosbag 파일로 기록&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108165539103.png&quot; alt=&quot;image-20220108165539103&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 bag info&lt;/code&gt; : rosbag 파일의 정보를 확인&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108165710159.png&quot; alt=&quot;image-20220108165710159&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 bag play&lt;/code&gt; : rosbag 파일을 재생&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108165745815.png&quot; alt=&quot;image-20220108165745815&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;ros2-interface&quot;&gt;ros2 interface&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 interface show&lt;/code&gt; : 인터페이스 정보를 출력&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170245213.png&quot; alt=&quot;image-20220108170245213&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 interface list&lt;/code&gt; : 인터페이스 목록을 출력&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170320690.png&quot; alt=&quot;image-20220108170320690&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 interface packages&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170342190.png&quot; alt=&quot;image-20220108170342190&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 interface package&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170402419.png&quot; alt=&quot;image-20220108170402419&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 interface proto&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170429071.png&quot; alt=&quot;image-20220108170429071&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;msg-인터페이스&quot;&gt;msg 인터페이스&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170127476.png&quot; alt=&quot;image-20220108170127476&quot; style=&quot;width:500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;서비스-service&quot;&gt;서비스 (&lt;em&gt;service&lt;/em&gt;)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/Selection_046.png&quot; alt=&quot;img&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;노드의 역할&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;u&gt;Service Client&lt;/u&gt; :  서비스를 요청(&lt;em&gt;Request&lt;/em&gt;) (동작 트리거)&lt;/li&gt;
      &lt;li&gt;&lt;u&gt;Service Server&lt;/u&gt; : 서비스를 응답(&lt;em&gt;Response&lt;/em&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;메시지 형태&lt;/strong&gt; : &lt;u&gt;서비스(*service*)&lt;/u&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;메시지 인터페이스&lt;/strong&gt; : &lt;u&gt;srv 인터페이스&lt;/u&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;특징&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;u&gt;일회성&lt;/u&gt;, &lt;u&gt;양방향&lt;/u&gt;, &lt;u&gt;동기성&lt;/u&gt;&lt;/li&gt;
      &lt;li&gt;&lt;u&gt;1:1 통신&lt;/u&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;사용 예시&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;LED제어, 모터 토크 On/Off, IK/FK 계산, 이동 경로 계산 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cli-명령어-1&quot;&gt;CLI 명령어&lt;/h3&gt;

&lt;h4 id=&quot;ros2-service&quot;&gt;ros2 service&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 service list&lt;/code&gt; : 서비스 목록 출력&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170515864.png&quot; alt=&quot;image-20220108170515864&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 service type&lt;/code&gt; : 서비스의 형태 확인&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170555519.png&quot; alt=&quot;image-20220108170555519&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 service find&lt;/code&gt; : 특정 형태의 서비스를 사용하는 서비스명 찾기&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170631206.png&quot; alt=&quot;image-20220108170631206&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 service call&lt;/code&gt; : 서비스 요청&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;/clear&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170725671.png&quot; alt=&quot;image-20220108170725671&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;/kill&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170754584.png&quot; alt=&quot;image-20220108170754584&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;/reset&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170843846.png&quot; alt=&quot;image-20220108170843846&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;/set_pen&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170859188.png&quot; alt=&quot;image-20220108170859188&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;/spawn&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108170914337.png&quot; alt=&quot;image-20220108170914337&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;ros2-interface-1&quot;&gt;ros2 interface&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;srv-인터페이스&quot;&gt;srv 인터페이스&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 interface show&lt;/code&gt; 명령어를 통해 서비스 인터페이스의 정보를 출력할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108171014551.png&quot; alt=&quot;image-20220108171014551&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;srv 인터페이스는 구분자로 나눠지는 다음의 구성 요소로 구성되어있음을 알 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;서비스 요청(&lt;em&gt;Request&lt;/em&gt;)&lt;/strong&gt; : e.g., float32 x, float32 y, float32 theta, string name&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;서비스 응답(&lt;em&gt;Response&lt;/em&gt;)&lt;/strong&gt; : e.g., string name&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 예시에서는 srv 인터페이스를 사용하여 service client와 service server가 각각 다음과 같이 서비스를 요청하고 서비스를 응답한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108171245503.png&quot; alt=&quot;image-20220108171245503&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;액션-action&quot;&gt;액션 (&lt;em&gt;action&lt;/em&gt;)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/Selection_060.png&quot; alt=&quot;img&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;노드의 역할&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;u&gt;Action Client&lt;/u&gt; :  액션 목표(&lt;em&gt;goal&lt;/em&gt;)를 지정 (동작 트리거)&lt;/li&gt;
      &lt;li&gt;&lt;u&gt;Action Server&lt;/u&gt; : 액션 목표를 받아 특정 태스크를 수행 &amp;amp; 중간 결과값인 액션 피드백(&lt;em&gt;feedback&lt;/em&gt;)과 최종 결과값인 액션 결과(&lt;em&gt;result&lt;/em&gt;)를 전송&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;메시지 형태&lt;/strong&gt; : &lt;u&gt;액션 (*action*)&lt;/u&gt; : 토픽과 서비스의 혼합&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;액션 목표(&lt;em&gt;action goal&lt;/em&gt;), 액션 결과(&lt;em&gt;action result&lt;/em&gt;) : 서비스 (&lt;em&gt;service&lt;/em&gt;) ⇒ 일회성·비동기
        &lt;ul&gt;
          &lt;li&gt;목표 전달(send_goal), 목표 취소(cancel_goal), 결과 받기(get_result)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;액션 피드백(&lt;em&gt;action feedback&lt;/em&gt;) : 토픽 (&lt;em&gt;topic&lt;/em&gt;) ⇒ 연속성·동기
        &lt;ul&gt;
          &lt;li&gt;목표 상태(goal_state), 피드백(feedback)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;메시지 인터페이스&lt;/strong&gt; : &lt;u&gt;action 인터페이스&lt;/u&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;특징&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;복합(토픽+서비스)&lt;/u&gt;, &lt;u&gt;양방향&lt;/u&gt;, &lt;u&gt;동기+비동기&lt;/u&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;1:1 통신&lt;/u&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;동기·비동기 방식의 혼합을 원활하게 구현하기위해 상태머신(state machine)을 사용&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/goal_state_machine.png&quot; alt=&quot;img&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;사용 예시&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;목적지로 이동, 물건 파지, 복합 태스크 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cli-명령어-2&quot;&gt;CLI 명령어&lt;/h3&gt;

&lt;h4 id=&quot;ros2-node-1&quot;&gt;ros2 node&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 node info&lt;/code&gt; : 노드의 정보를 출력&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108171501830.png&quot; alt=&quot;image-20220108171501830&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;ros2-action&quot;&gt;ros2 action&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 action list&lt;/code&gt; : 액션 목록을 출력&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108171543074.png&quot; alt=&quot;image-20220108171543074&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 action info&lt;/code&gt; : 특정 액션의 정보를 출력&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108171610120.png&quot; alt=&quot;image-20220108171610120&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 action send_goal&lt;/code&gt; : 액션목표(action goal)를 전달&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108171652141.png&quot; alt=&quot;image-20220108171652141&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;※ 실시간으로 피드백을 표시하고싶다면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--feedback&lt;/code&gt; 옵션을 추가할 것 ※&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;ros2-interface-2&quot;&gt;ros2 interface&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 interface show&lt;/code&gt; : 인터페이스 정보를 출력&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108171744941.png&quot; alt=&quot;image-20220108171744941&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;action-인터페이스&quot;&gt;action 인터페이스&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 interface show&lt;/code&gt; 명령어를 통해 액션 인터페이스의 정보를 출력할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220108171744941.png&quot; alt=&quot;image-20220108171744941&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;action 인터페이스는 구분자로 나눠지는 다음의 구성 요소로 구성되어있음을 알 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;액션 목표(&lt;em&gt;action goal&lt;/em&gt;)&lt;/strong&gt; : e.g., float32 theta&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;액션 결과(&lt;em&gt;action result&lt;/em&gt;)&lt;/strong&gt; : e.g., float32 delta&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;액션 피드백(&lt;em&gt;action feedback&lt;/em&gt;)&lt;/strong&gt; : e.g., float32 remaining&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;파라미터-parameter&quot;&gt;파라미터 (&lt;em&gt;parameter&lt;/em&gt;)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/Selection_049.png&quot; alt=&quot;img&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;노드의 역할&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;u&gt;Parameter Client&lt;/u&gt; :  서버로부터 파라미터 정보를 가져오거나 서버의 파라미터 정보를 세팅&lt;/li&gt;
      &lt;li&gt;&lt;u&gt;Parameter Server&lt;/u&gt; : 클라이언트의 요청을 수행&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;특징&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;서비스의 목적은 서비스 요청과 응답이라는 RPC(&lt;em&gt;Remote Procedure Call&lt;/em&gt;)인 반면
파라미터의 목적은 &lt;u&gt;노드 내 매개변수를 서비스 데이터 통신 방법을 사용하여 쉽게 지정(set), 변경, 가져오는(get) 것&lt;/u&gt;이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cli-명령어-3&quot;&gt;CLI 명령어&lt;/h3&gt;

&lt;h4 id=&quot;ros2-param&quot;&gt;ros2 param&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 param list&lt;/code&gt; : 파라미터 목록을 출력&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220109011906048.png&quot; alt=&quot;image-20220109011906048&quot; style=&quot;width:200px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 param describe&lt;/code&gt; : 파라미터 내용 확인&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220109011936507.png&quot; alt=&quot;image-20220109011936507&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 param get&lt;/code&gt; : 파라미터 읽기&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220109012007087.png&quot; alt=&quot;image-20220109012007087&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 param set&lt;/code&gt; : 파라미터 쓰기&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220109012037633.png&quot; alt=&quot;image-20220109012037633&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220109012050495.png&quot; alt=&quot;image-20220109012050495&quot; style=&quot;zoom:25%;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 param dump&lt;/code&gt; : 파라미터 저장&lt;/p&gt;

    &lt;p&gt;아래와 같이 현재 폴더에 노드 이름으로 .yaml 파일 형태로 저장됨&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220109012132735.png&quot; alt=&quot;image-20220109012132735&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;’./turtlesim.yaml’ 파일의 내용을 확인하면 다음과 같이 파라미터의 현재 값이 저장되어 있음&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220109012309510.png&quot; alt=&quot;image-20220109012309510&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;이후 노드 실행시 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--ros-args --params-file&lt;/code&gt; 옵션과 함께 적어주면 노드 실행시 초기 파라미터값을 지정해줄 수 있음&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220109012356483.png&quot; alt=&quot;image-20220109012356483&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros2 param delete&lt;/code&gt; : 파라미터 삭제&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220109012424108.png&quot; alt=&quot;image-20220109012424108&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;아래와 같이 삭제되었음을 확인할 수 있다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2022-01-08-robot-ros-ros2002/image-20220109012438104.png&quot; alt=&quot;image-20220109012438104&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cafe.naver.com/openrt/24086&quot;&gt;“008 ﻿ROS 2 노드와 데이터 통신,” 오로카, 2020년 09월 10일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24086.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cafe.naver.com/openrt/24101&quot;&gt;“009 ﻿ROS 2 토픽 (topic),” 오로카, 2020년 09월 04일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24101.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cafe.naver.com/openrt/24128&quot;&gt;“010 ﻿ROS 2 서비스 (service),” 오로카, 2021년 02월 08일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24128.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cafe.naver.com/openrt/24142&quot;&gt;“011 ﻿ROS 2 액션 (action),” 오로카, 2020년 09월 10일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24142.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cafe.naver.com/openrt/24154&quot;&gt;“012 ROS 2 토픽/서비스/액션 정리 및 비교,” 오로카, 2021년 03월 17일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24154.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cafe.naver.com/openrt/24165&quot;&gt;“013 ROS 2 파라미터 (parameter),” 오로카, 2020년 09월 11일 수정, 2022년 01월 08일 접속, https://cafe.naver.com/openrt/24165.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 09 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//robotics-ros/ros2002/</link>
        <guid isPermaLink="true">https://dazory.github.io//robotics-ros/ros2002/</guid>
        
        <category>robotics</category>
        
        <category>ROS</category>
        
        <category>ROS2</category>
        
        
        <category>robotics-ros</category>
        
      </item>
    
      <item>
        <title>[References-001] opencv with C++ 프로젝트</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;기본&quot;&gt;기본&lt;/h1&gt;

&lt;h2 id=&quot;경로&quot;&gt;경로&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;filename.type&quot;&lt;/code&gt; : 코드가 위치한 곳&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;/filename.type&quot;&lt;/code&gt; : 코드가 위치한 프로젝트 기준&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;opencv&quot;&gt;OpenCV&lt;/h1&gt;

&lt;h2 id=&quot;mat&quot;&gt;Mat&lt;/h2&gt;

&lt;h3 id=&quot;generate&quot;&gt;Generate&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Scalar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Scalar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Type&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;CV_8U&lt;/strong&gt; (=0) : 8-bit unsigned integer: uchar ( 0..255 )&lt;/p&gt;

        &lt;p&gt;※ 일반적인 .JPG 파일의 type은 CV_8UC3이다. ※&lt;br /&gt;
※ Vec3b(uchar) type으로 pixel단위 접근을 할 수 있다. ※&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-11-28-etc-references-opencv/image-20210927234133991.png&quot; alt=&quot;image-20210927234133991&quot; style=&quot;width:200px;&quot; /&gt;&lt;img src=&quot;/files/2021-11-28-etc-references-opencv/image-20210927234549686.png&quot; alt=&quot;image-20210927234549686&quot; style=&quot;width:100px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;CV_8S&lt;/strong&gt; (=1) : 8-bit signed integer: schar ( -128..127 )&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-11-28-etc-references-opencv/image-20210927234159040.png&quot; alt=&quot;image-20210927234159040&quot; style=&quot;width:200px;&quot; /&gt;&lt;img src=&quot;/files/2021-11-28-etc-references-opencv/image-20210927234611358.png&quot; alt=&quot;image-20210927234611358&quot; style=&quot;width:100px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;CV_16U&lt;/strong&gt; (=2) : 16-bit unsigned integer: ushort ( 0..65535 )&lt;/p&gt;

        &lt;p&gt;※ Vec3w(ushort) type으로 pixel단위 접근을 할 수 있다. ※&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-11-28-etc-references-opencv/image-20210927234213848.png&quot; alt=&quot;image-20210927234213848&quot; style=&quot;width:200px;&quot; /&gt;&lt;img src=&quot;/files/2021-11-28-etc-references-opencv/image-20210927234630359.png&quot; alt=&quot;image-20210927234630359&quot; style=&quot;width:100px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;CV_16S&lt;/strong&gt; (=3) : 16-bit signed integer: short ( -32768..32767 )&lt;/p&gt;

        &lt;p&gt;※ Vec3s(short) type으로 pixel단위 접근을 할 수 있다. ※&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;CV_32S&lt;/strong&gt; (=4) : 32-bit signed integer: int ( -2147483648..2147483647 )&lt;/p&gt;

        &lt;p&gt;※ Vec3i(int) type으로 pixel단위 접근을 할 수 있다. ※&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;CV_32F&lt;/strong&gt; (=5) : 32-bit floating-point number: float ( -FLT_MAX..FLT_MAX, INF, NAN )&lt;/p&gt;

        &lt;p&gt;※ Vec3f(float) type으로 pixel단위 접근을 할 수 있다. ※&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-11-28-etc-references-opencv/image-20210927234231565.png&quot; alt=&quot;image-20210927234231565&quot; style=&quot;width:300px;&quot; /&gt;&lt;img src=&quot;/files/2021-11-28-etc-references-opencv/image-20210927234651138.png&quot; alt=&quot;image-20210927234651138&quot; style=&quot;width:100px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;CV_64F&lt;/strong&gt; (=6) : 64-bit floating-point number: double ( -DBL_MAX..DBL_MAX, INF, NAN )&lt;/p&gt;

        &lt;p&gt;※ Vec3d(double) type으로 pixel단위 접근을 할 수 있다. ※&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-11-28-etc-references-opencv/image-20210927234249304.png&quot; alt=&quot;image-20210927234249304&quot; style=&quot;width:300px;&quot; /&gt;&lt;img src=&quot;/files/2021-11-28-etc-references-opencv/image-20210927234708659.png&quot; alt=&quot;image-20210927234708659&quot; style=&quot;width:100px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;※ 이러한 type 뒤에 channel의 개수를 붙여서 사용하기도 함 (e.g., CV_8UC1) ※&lt;br /&gt;
※ channel이 하나 증가할 때 마다 8씩 증가된다. ※
e.g., CV_8UC1=0,  CV_8UC2=8,   CV_8UC3=16;
        CV_8SC1=1,  CV_8SC2=9,  CV_8SC3=17;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;read&quot;&gt;Read&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;img.jpg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;write&quot;&gt;Write&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;img&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imwrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;※ 이때, 저장할 경로의 directory가 이미 존재해야한다. 없을 경우 저장되지 않음. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;show&quot;&gt;Show&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;title&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;waitKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;destroyAllWindows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;approach&quot;&gt;Approach&lt;/h3&gt;

&lt;h4 id=&quot;each-pixel&quot;&gt;each pixel&lt;/h4&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// in: Index along the dimension n&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// idx: Array of Mat::dims indices&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// pt: Element position specified as Point(j,i).&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uchar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vec3b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;type&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Mat1b&lt;/strong&gt; (=Mat&amp;lt;uchar&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat2b&lt;/strong&gt; (=Mat&amp;lt;Vec2b&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat3b&lt;/strong&gt; (=Mat&amp;lt;Vec3b&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat4b&lt;/strong&gt; (=Mat&amp;lt;Vec4b&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec2b&lt;/strong&gt; (=Vec&amp;lt;uchar, 2&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec3b&lt;/strong&gt; (=Vec&amp;lt;uchar, 3&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec4b&lt;/strong&gt; (=Vec&amp;lt;uchar, 4&amp;gt;)&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Mat1w&lt;/strong&gt; (=Mat&amp;lt;ushort&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat2w&lt;/strong&gt; (=Mat&amp;lt;Vec2w&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat3w&lt;/strong&gt; (=Mat&amp;lt;Vec3w&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat4w&lt;/strong&gt; (=Mat&amp;lt;Vec4w&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec2w&lt;/strong&gt; (=Vec&amp;lt;ushort, 2&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec3w&lt;/strong&gt; (=Vec&amp;lt;ushort, 3&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec4w&lt;/strong&gt; (=Vec&amp;lt;ushort, 4&amp;gt;)&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Mat1s&lt;/strong&gt; (=Mat&amp;lt;short&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat2s&lt;/strong&gt; (=Mat&amp;lt;Vec2s&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat3s&lt;/strong&gt; (=Mat&amp;lt;Vec3s&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat4s&lt;/strong&gt; (=Mat&amp;lt;Vec4s&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec2s&lt;/strong&gt; (=Vec&amp;lt;short, 2&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec3s&lt;/strong&gt; (=Vec&amp;lt;short, 3&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec4s&lt;/strong&gt; (=Vec&amp;lt;short, 4&amp;gt;)&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Mat1i&lt;/strong&gt; (=Mat&amp;lt;int&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat2i&lt;/strong&gt; (=Mat&amp;lt;Vec2i&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat3i&lt;/strong&gt; (=Mat&amp;lt;Vec3i&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat4i&lt;/strong&gt; (=Mat&amp;lt;Vec4i&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec2i&lt;/strong&gt; (=Vec&amp;lt;int, 2&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec3i&lt;/strong&gt; (=Vec&amp;lt;int, 3&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec4i&lt;/strong&gt; (=Vec&amp;lt;int, 4&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec6i&lt;/strong&gt; (=Vec&amp;lt;int, 6&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec8i&lt;/strong&gt; (=Vec&amp;lt;int, 8&amp;gt;)&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Mat1f&lt;/strong&gt; (=Mat&amp;lt;float&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat2f&lt;/strong&gt; (=Mat&amp;lt;Vec2f&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat3f&lt;/strong&gt; (=Mat&amp;lt;Vec3f&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat4f&lt;/strong&gt; (=Mat&amp;lt;Vec4f&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec2f&lt;/strong&gt; (=Vec&amp;lt;float, 2&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec3f&lt;/strong&gt; (=Vec&amp;lt;float, 3&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec4f&lt;/strong&gt; (=Vec&amp;lt;float, 4&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec6f&lt;/strong&gt; (=Vec&amp;lt;float, 6&amp;gt;)&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Mat1d&lt;/strong&gt; (=Mat&amp;lt;double&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Mat2d&lt;/strong&gt; (=Mat&amp;lt;Vec2d&amp;gt;&lt;br /&gt;
&lt;strong&gt;Mat3d&lt;/strong&gt; (=Mat&amp;lt;Vec3d&amp;gt;&lt;br /&gt;
&lt;strong&gt;Mat4d&lt;/strong&gt; (=Mat&amp;lt;Vec4d&amp;gt;&lt;br /&gt;
&lt;strong&gt;Vec2d&lt;/strong&gt; (=Vec&amp;lt;double, 2&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec3d&lt;/strong&gt; (=Vec&amp;lt;double, 3&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec4d&lt;/strong&gt; (=Vec&amp;lt;double, 4&amp;gt;)&lt;br /&gt;
&lt;strong&gt;Vec6d&lt;/strong&gt; (=Vec&amp;lt;double, 6&amp;gt;)&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;range&quot;&gt;Range&lt;/h4&gt;

\[\text{Mat M }  = \begin{bmatrix} 
	1&amp;amp;2&amp;amp;3\\ 4&amp;amp;5&amp;amp;6\\ 7&amp;amp;8&amp;amp;9
\end{bmatrix}\]

&lt;p&gt;일 때,&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;행&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;열&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;행의&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;범위&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;열의&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;범위&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example :&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M_0x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// [1,2,3]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M_1x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// [4,5,6]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M_2x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// [7,8,9]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M_x0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// [1;4;7;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M_x1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// [2;5;8;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M_x2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// [3;6;9;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// [2,3; 5,6;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;circle&quot;&gt;Circle&lt;/h4&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;circle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Point2f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;resize&quot;&gt;Resize&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// sz: New number of rows&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Scalar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// s: Value assigned to the newly added elements&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;convert-type&quot;&gt;Convert Type&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convertTo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OutputArray&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convertTo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV_8UC3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;c&quot;&gt;C++&lt;/h1&gt;

&lt;h2 id=&quot;minmax&quot;&gt;min/max&lt;/h2&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 1&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이런식으로 중괄호를 통해 여러 값에 대해 최소/최대값을 구할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;vector&quot;&gt;Vector&lt;/h2&gt;

&lt;p&gt;array와 달리 동적으로 원소를 추가할 수 있으며 크기가 자동으로 늘어난다.&lt;/p&gt;

&lt;h3 id=&quot;생성&quot;&gt;생성&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vector&amp;lt;int&amp;gt; v;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;추가--삭제&quot;&gt;추가 &amp;amp; 삭제&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// vector의 back에 원소를 추가&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// vector의 back에 원소를 삭제&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;접근&quot;&gt;접근&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// i번째 원소를 반환&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// i번째 원소를 반환&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_front&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;front&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// front([0]) 원소를 반환&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_back&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;// back([size-1]) 원소를 반환&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;기타&quot;&gt;기타&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;isEmpty&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// vector가 비었으면 true를 반환&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;// vector의 원소 개수를 반환&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;queue&quot;&gt;Queue&lt;/h2&gt;

&lt;h3 id=&quot;생성-1&quot;&gt;생성&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;추가--삭제-1&quot;&gt;추가 &amp;amp; 삭제&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// queue의 back에 원소 추가&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// queue의 front쪽 원소를 삭제&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;탐색&quot;&gt;탐색&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;front&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;front&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// queue의 front쪽 원소를 반환&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;back&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// queue의 back쪽 원소를 반환&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;기타-1&quot;&gt;기타&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;isEmpty&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// queue가 비었으면 true를 반환&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;// queue의 size를 반환&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 28 Nov 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//etc-references/opencv/</link>
        <guid isPermaLink="true">https://dazory.github.io//etc-references/opencv/</guid>
        
        <category>references</category>
        
        <category>opencv</category>
        
        <category>C++</category>
        
        
        <category>etc-references</category>
        
      </item>
    
      <item>
        <title>[NeuromorphicVision-002] 뉴로모픽 비전(Neuromorphic Vision)의 현황(1)</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;랩세미나에서 Neuromorphic vision에 관한 발표를 보고 관심이 생겼다. 관련하여 배경과 주요 개념 및 현황을 조사하고자 본 글을 작성하게 되었다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;neuromorphic-vision&quot;&gt;Neuromorphic Vision&lt;/h1&gt;

&lt;h2 id=&quot;현황&quot;&gt;현황&lt;/h2&gt;

&lt;p&gt;기존의 프레임 기반 카메라의 한계를 극복하기 위해 생물학적 망막을 모사한 실리콘 망막(&lt;em&gt;silicon retina&lt;/em&gt;)이 개발되었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mahowald and Mead, silicon VLSI retina in 1991&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;적응적 광수용체와 픽셀의 2차원 육각격자의 칩으로 구성된다. 광수용체, 양극성 세포(&lt;em&gt;bipolar cells&lt;/em&gt;), 그리고 수평 세포(&lt;em&gt;horizontal cells&lt;/em&gt;)로 구성된 생물학적 망을 모방하였다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Parvo-Magno retina, Zaghloul and Boahen, silicon VLSI retina&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;5개의 망막 층으로 모델링되었다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Tobi Delbruck’s team, practicable silicon retina DVS(Dynamic Vision Sensor) based on biological principles&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;오늘날 대부분의 실리콘 망막은 이러한 Tobi Delbruck and Christoph Posch의 구조에서 파생되었다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;실리콘 망막이 현실세계에 실용적으로 적용되기 시작한 것은 Tobi Delbruck 팀의 DVS(&lt;em&gt;Dynamic Vision Sensor&lt;/em&gt;) 이후였으며, 그 이전까지는 뉴로모픽 비전의 가능성을 증명하기 위해 이론적 위주로 개발되어왔다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;dynamic-vision-sensor-dvs&quot;&gt;Dynamic Vision Sensor (DVS)&lt;/h3&gt;

&lt;h4 id=&quot;구조&quot;&gt;구조&lt;/h4&gt;

&lt;p&gt;DVS의 픽셀 회로는 &lt;u&gt;인간 망막의 삼층 모델을 모방&lt;/u&gt;하였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211120231549490.png&quot; alt=&quot;image-20211120231549490&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;광수용체 층(&lt;span style=&quot;background-color:#A9ACFA; font-weight:bold;&quot;&gt;Phtoreceptor layer&lt;/span&gt;)&lt;/p&gt;

    &lt;p&gt;광신호에 의해 발생한 광전류 $I_{\text{ph}}$ 를 추적하여 전압 $V_{\text{log}}$ 로 표현한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;외부 플렉시폼 층(&lt;span style=&quot;background-color:#FFCAA3; font-weight:bold;&quot;&gt;Outer plexiform layer&lt;/span&gt;)&lt;/p&gt;

    &lt;p&gt;스파이크 이벤트 $v_{\text{diff}}$가 광전류 $I_{\text{ph}}$의 변화에 따라 반응한다.&lt;/p&gt;

    &lt;p&gt;전류값이 증가하면(positive gradient) ON 스파이크, 감소하면(negative gradient) OFF 스파이크가 발생한다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211120232424556.png&quot; alt=&quot;image-20211120232424556&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;내부 플렉시폼 층(&lt;span style=&quot;background-color:#FFF572; font-weight:bold;&quot;&gt;Inner plexiform layer&lt;/span&gt;)&lt;/p&gt;

    &lt;p&gt;스파이크 이벤트에 의해 ON 이벤트(빛 증가를 의미, 하얀색으로 표현됨)와 OFF 이벤트(빛 감소를 의미, 검정색으로 표현됨) 출력한다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211120232538665.png&quot; alt=&quot;image-20211120232538665&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;통신&quot;&gt;통신&lt;/h4&gt;

&lt;p&gt;칩 와이어링(&lt;em&gt;Chip wiring&lt;/em&gt;) 문제로 인해 각 픽셀은 고유 케이블을 가질 수 없다. 이러한 문제는 &lt;strong&gt;&lt;em&gt;Address Event Representation (AER)&lt;/em&gt;&lt;/strong&gt; 기술을 통해 해결할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211120233249017.png&quot; alt=&quot;image-20211120233249017&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;송신기(&lt;em&gt;transmitter&lt;/em&gt;)에서 스파이크(&lt;em&gt;spikes&lt;/em&gt;)에 의해 이진 이벤트(&lt;em&gt;binary events&lt;/em&gt;)가 출력된다.&lt;/li&gt;
  &lt;li&gt;송신기로부터 출력되는 이진 이벤트는 주소 인코더(&lt;em&gt;Address Encoder&lt;/em&gt;)를 통해 이진 주소(&lt;em&gt;binary address&lt;/em&gt;)로 변환된다.&lt;/li&gt;
  &lt;li&gt;이진 주소는 버스 시스템을 통해 주소 디코더(&lt;em&gt;Address Decoder&lt;/em&gt;)로 전달된다.&lt;/li&gt;
  &lt;li&gt;주소 디코더는 이진 주소를 이진 이벤트로 변환시킨다.&lt;/li&gt;
  &lt;li&gt;수신기(&lt;em&gt;receiver&lt;/em&gt;)는 이진 이벤트를 스파이크로 변환한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이러한 과정을 통해 하나의 이벤트는 다음과 같이 정의된다.&lt;/p&gt;

\[\text{An event} : (x,y,t,p)\]

&lt;p&gt;※ $(x,y)$: 픽셀 주소;  $t$: 타임스탬프;  $p$: 극성(positive/negative); ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;장점&quot;&gt;장점&lt;/h4&gt;

&lt;p&gt;DVS 픽셀은 강도(&lt;em&gt;intensity&lt;/em&gt;)의 변화가 문턱값을 초과하는 &lt;u&gt;이벤트에 자동적으로 반응&lt;/u&gt;한다. 즉, 이벤트 기반이라는 점에서 본질적으로 프레임 기반 카메라 센서와 구분된다. 이러한 특성으로 인해 &lt;u&gt;이벤트 기반 뉴로모픽 비전 센서&lt;/u&gt;는 다음의 장점을 갖는다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;에너지 친화적인 특성 (&lt;em&gt;Energy-friendly properties&lt;/em&gt;)&lt;/b&gt;&lt;/p&gt;

    &lt;p&gt;이벤트가 발생하는 경우에만 자동적으로 반응하기 때문에 에너지 친화적이다. 이는 차량에 장착되는 온보드 장치에서 매우 중요한 특성이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;저지연 (&lt;em&gt;Low latency&lt;/em&gt;)&lt;/b&gt;&lt;/p&gt;

    &lt;p&gt;각 픽셀이 독립적으로 동작하므로 프레임의 전역 노출이 필요하지 않아 지연시간이 매우 짧다. 가령, 제어 시스템을 위한 시간을 많이 할당해주기 위해서는 객체 감지 시스템에서 시간 소요를 줄이는 것이 관건이므로 이러한 특성은 인식 시스템(&lt;em&gt;perception system&lt;/em&gt;)에서 매우 중요하다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;높은 동적 영역 (&lt;em&gt;HDR, High dynamic range&lt;/em&gt;)&lt;/b&gt;&lt;/p&gt;

    &lt;p&gt;DVS와 같은 이벤트 기반 뉴로모픽 비전 센서는 프레임 기반 카메라의 동적 영역(60dB)에 비해 높은 동적 영역 (120dB)을 갖는다 (HDR). 이는 매우 어둡거나 밝은 자극에 대해 로버스트한 인지 시스템 갭라로 이어지므로 터널과 같은 공간을 지나는 자율주행 시스템에서 중요하다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;마이크로초 해상도 (&lt;em&gt;Microsecond resolution&lt;/em&gt;)&lt;/b&gt;&lt;/p&gt;

    &lt;p&gt;아날로그 회로에 의해 밝기 변화가 빠르게 취득되므로, 이벤트는 마이크로초 해상도를 가질 수 있다. 이는 위급한 주행 상황에서 매우 중요한 특성이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;모션블러가 없음 (&lt;em&gt;No motion blur&lt;/em&gt;)&lt;/b&gt;&lt;/p&gt;

    &lt;p&gt;빠르게 주행하는 환경에서 프레임 기반 카메라는 모션 블러를 발생시켜 인지 성능이 감소한다. 따라서 이벤트 기반 뉴로모픽 비전은 동적 모션을 매우 정교하게 취득할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;알고리즘&quot;&gt;알고리즘&lt;/h3&gt;

&lt;h4 id=&quot;이벤트-노이즈-처리&quot;&gt;이벤트 노이즈 처리&lt;/h4&gt;

&lt;p&gt;이벤트 기반 뉴로모픽 비전 센서는 움직이는 물체로 인해 발생하는 밝기 변화를 잘 감지할 수 있다. 따라서 한편으로는 노이즈로 인한 밝기 변화에도 민감하기 때문에 원시 데이터(&lt;em&gt;raw data&lt;/em&gt;)의 전처리가 중요하다. 이벤트 스트림에서 &lt;u&gt;이벤트 노이즈를 제거하는 방법&lt;/u&gt;으로는 다음과 같이 두 가지 방법이 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211120234736118.png&quot; alt=&quot;image-20211120234736118&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;시공간 상관 필터 (&lt;span style=&quot;color:red;&quot;&gt;*The spatial-temporal correlation filter*&lt;/span&gt;)&lt;/b&gt;&lt;/p&gt;

    &lt;p&gt;이벤트 $e_i=(x_i, y_i, t_i, p_i)$가 들어왔을 때, 현재 픽셀 위치 $(x_i, y_i)$의 가장 최근 주변 이웃들을 탐색한다 ($≤ \text{distance } D$). 즉, 다음의 수식이 만족될 때, 노이즈가 발생하지 않았다고 판단한다.&lt;/p&gt;

\[t_i-t_n&amp;lt;d_t\]

    &lt;ul&gt;
      &lt;li&gt;$d_t$ : 미리 정의된 문턱값&lt;/li&gt;
      &lt;li&gt;$t_i$ : 이벤트의 타임스탬프&lt;/li&gt;
      &lt;li&gt;$t_n$​ : 가장 최근에 발생한 이웃 이벤트의 타임스탬프&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;모션 일관성 필터 (&lt;span style=&quot;color:#00B0F0;&quot;&gt;The motion consistency filter&lt;/span&gt;)&lt;/b&gt;&lt;/p&gt;

    &lt;p&gt;물체에 의한 움직임(&lt;em&gt;object motion&lt;/em&gt;)인 경우, 로컬 영역에서 이전 이벤트와 일관된 움직임을 보여야한다. 따라서 속도 $(v_x, v_y)$를 통해 움직임 일관성을 평가하여 같은 움직임 영역인지를 판단한다. 만일 같은 평면을 공유하지 않으면 이벤트 노이즈(&lt;em&gt;event noise&lt;/em&gt;)라 판단하여 제거한다.&lt;/p&gt;

    &lt;p&gt;구체적으로 모션 일관성 평면은 다음과 같이 공식화될 수 있다.&lt;/p&gt;

\[ax_i+by_i+ct_i+d=0\]

    &lt;ul&gt;
      &lt;li&gt;$(a,b,c,d)∈\mathbb{R}^4$ : 평면 $M$을 정의&lt;/li&gt;
      &lt;li&gt;$(x_i,y_i)$ : 이벤트 $e_i$의 좌표&lt;/li&gt;
      &lt;li&gt;$t_i$ : 이벤트 $e_i$ 의 타임스탬프&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;비동기-이벤트의-표현&quot;&gt;비동기 이벤트의 표현&lt;/h4&gt;

&lt;p&gt;이벤트 기반 뉴로모픽 비전 센서는 단지 픽셀 레벨의 변화만을 전송하므로 희소한(&lt;em&gt;sparse&lt;/em&gt;) 비동기 이벤트 스트림 데이터가 출력된다. 이는 &lt;u&gt;CNN(*Convolutional Neural Network*) 기반 아키텍처와 같은 표준 비전 파이프라인에 직접적으로 적용할 수 없는 형태&lt;/u&gt;이다. 따라서 인코딩 기법을 통해 &lt;u&gt;비동기 이벤트를 동기 이미지 혹은 그리드(*grid*) 표현으로 변환&lt;/u&gt;해주어야 한다.&lt;/p&gt;

&lt;p&gt;여기서는 대표적인 최신 인코딩 기법으로 &lt;strong&gt;공간적 인코딩(&lt;em&gt;Spatial encoding&lt;/em&gt;)&lt;/strong&gt;과 &lt;strong&gt;시공간적 인코딩(&lt;em&gt;Spatial-temporal encoding&lt;/em&gt;)&lt;/strong&gt; 기법을 소개한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;공간적 인코딩(&lt;em&gt;Spatial encoding&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;픽셀 위치 $(x_i, y_i)$에서의 이벤트 데이터를 고정된 시간 간격 (e.g., 30ms) 혹은 고정된 이벤트 개수(e.g., 500events)으로 저장하여 이벤트 스트림을 이벤트 프레임으로 변환해준다. 이벤트 프레임에서 픽셀 값은 일반적으로 마지막 이벤트의 polarity (positive event=1, negative event=-1) 혹은 고정된 간격에서 이벤트 개수의 확률적 특성으로 표현된다.&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;상수 시간 프레임 (&lt;em&gt;constant time frame&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt;

\[F_j^t=\textbf{card}{(e_i \mid T·(j-1) ≤t_i ≤T·j)}\]

        &lt;ul&gt;
          &lt;li&gt;$F_j^t$​ : 시간 간격 $T$​의 $j$​번째 프레임&lt;/li&gt;
          &lt;li&gt;$\textbf{card}()$​ : 집합의 카디널리티(&lt;em&gt;cardinality&lt;/em&gt;)&lt;/li&gt;
          &lt;li&gt;$e_i$ : 이벤트 스트림의 $i$번째 이벤트&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;상수 개수 프레임 (&lt;em&gt;constant count frame&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt;

\[F_j^e=\textbf{card}{(e_i \mid E·(j-1) ≤i≤E·j)}\]

        &lt;ul&gt;
          &lt;li&gt;$F_j^e$ : 이벤트 $E$를 포함하는 $j$번째 프레임&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;이벤트 개수 프레임 (&lt;em&gt;event count frame&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211120235613145.png&quot; alt=&quot;image-20211120235613145&quot; /&gt;&lt;/p&gt;

\[{Hist}^+(x,y)=\sum_{p_i=+1, t_i∈T}{δ(x-x_i, y-y_i)}\]

        &lt;ul&gt;
          &lt;li&gt;${Hist}^+(x,y)$ : positive event에 대한 히스토그램&lt;/li&gt;
          &lt;li&gt;$δ$​ : 크로네커 델타 함수(&lt;em&gt;Kronecker delta function&lt;/em&gt;)&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;시공간적 인코딩(&lt;em&gt;Spatial-temporal encoding&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;이벤트의 시공간 정보를 결합한 방식으로, 이벤트를 간결한 표현으로 변환해준다.&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;활성 이벤트의 표면, &lt;em&gt;Surface of active events (SAE)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

        &lt;p&gt;픽셀값을 표현할 때, 강도(&lt;em&gt;intensity&lt;/em&gt;)값 대신에 타임 스탬프값을 사용한다.&lt;/p&gt;

\[\text{SAE}:t_i↦P(x_i,y_i)\]

        &lt;ul&gt;
          &lt;li&gt;$t_i$ : 각 픽셀에서 가장 최근에 발생한 이벤트의 타임스탬프&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;$(x_i,y_i)$에서 발생한 이전의 이벤트 정보를 무시한다는 단점이 있다.&lt;/p&gt;

        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Leaky integrate-and-fire (LIF)&lt;/strong&gt;&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211120235918979.png&quot; alt=&quot;image-20211120235918979&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;LIF는 생물학적 인지 원리와 계산 요소에서 영감을 받은 인공 뉴런으로, 뉴런이 DVS에서 발생된 입력 스파이크(이벤트)를 수신하여 막 전위(&lt;em&gt;membrane potential&lt;/em&gt;)을 수정한다.막 전위가 미리 정의된 문턱값을 초과하면 스파이크 자극이 출력으로 전송된다.&lt;/p&gt;

        &lt;p&gt;LIF 뉴런은 다음과 같이 모델링될 수 있다.&lt;/p&gt;

\[τ \frac{dV}{dt}=-(V(t)-V_{\text{reset}} )+RI(t)\]

        &lt;ul&gt;
          &lt;li&gt;$V(t)$ : 막전위&lt;/li&gt;
          &lt;li&gt;$I(t)$​ : 전체 시냅스 전류&lt;/li&gt;
          &lt;li&gt;$R$​ : 막저항&lt;/li&gt;
          &lt;li&gt;$τ$ : 막 시간 상수&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;막전위가 threshold voltage $V_th$에 도달하면 reset voltage $V_{\text{reset}}$​ 으로 리셋되며 뉴런이 스파이크를 출력한다 (fire). 또한 LIF 뉴런은 이벤트 데이터 표현뿐만아니라 스파이킹 신경망(SNN, &lt;em&gt;spiking neural network&lt;/em&gt;)의 기본 유닛으로도 사용된다.&lt;/p&gt;

        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;복셀 그리드 (&lt;em&gt;Voxel grid&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211121000151382.png&quot; alt=&quot;image-20211121000151382&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;시간 도메인에서 이벤트 스트림의 해상도를 향상시키기 위한 이벤트 표현법이다. $N$개의 이벤트 집합 $(x_i, y_i,t_i,p_i )_{i∈[1,N]}$이 주어졌을 때, 시간 차원을 $B$개의 빈(&lt;em&gt;bins&lt;/em&gt;)으로 나누어 범위를 $[0,B-1]$으로 스케일링한다.&lt;/p&gt;

        &lt;p&gt;이때 이벤트 복셀 그리드는 다음과 같이 정의된다.&lt;/p&gt;

\[\hat{t}=(B-1)\frac{(t_i-t_1)}{(t_N-t_1)}\\
V(x,y,t)= \sum_i^N{p_i k(x-x_i )k(y-y_i )k(t-\hat{t})}\\
k(z)=max⁡{(0, 1-\mid z \mid)}\]

        &lt;ul&gt;
          &lt;li&gt;$k(z)$ : trilinear voting kernel&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;이벤트 스파이크 텐서 (EST, &lt;em&gt;Event spike tensor&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt;

        &lt;p&gt;EST는 end-to-end로 학습된 표현법이다. 주어진 시간 간격 $T$에서, EST는 컨볼루션 신호(convolved signal)를 샘플링하여 형성된다.&lt;/p&gt;

\[S_±[x,y,t]= \sum_{e_i∈p_±}{f_± (x_i, y_i, t_i ) k_c (x-x_i, y-y_i, t-t_i)}\]

        &lt;ul&gt;
          &lt;li&gt;$f_± (x_i,y_i,t_i)$​ : 각 이벤트에 할당된 측정값&lt;/li&gt;
          &lt;li&gt;$k_c$ : 이벤트 스트림으로부터 의미있는 신호를 얻기 위한 커널 컨볼루션 함수&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;데이터-세트&quot;&gt;데이터 세트&lt;/h4&gt;

&lt;p&gt;이벤트 기반 뉴로모픽 비전, 뉴로로보틱스, 그리고 자율주행 차량의 연구를 위한 다양한 데이터세트가 존재한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;DET 데이터세트&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211121000729709.png&quot; alt=&quot;image-20211121000729709&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;차선 검출을 위한 데이터 세트&lt;/li&gt;
      &lt;li&gt;터널, 다리, 육교, 그리고 도시 공간에서 주행한 다양한 교통 장면을 포함&lt;/li&gt;
      &lt;li&gt;라벨링된 1,280×800 pixels의 5,424 event frames을 포함
        &lt;ul&gt;
          &lt;li&gt;학습 세트: 2,716 frames&lt;br /&gt;검증 세트: 873 frame&lt;br /&gt;테스트 세트: 1,835 frames&lt;/li&gt;
          &lt;li&gt;두 종류로 라벨: ‘차선이 아닌 픽셀’과 ‘차선인 픽셀’&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;N-CARS 데이터세트&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211121000839918.png&quot; alt=&quot;image-20211121000839918&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;DVS를 통해 도시환경에서 기록된 데이터를 제공&lt;/li&gt;
      &lt;li&gt;12,336 자동차 샘플과 11,693 배경(noncar) 샘플을 포함
        &lt;ul&gt;
          &lt;li&gt;학습 세트: 7,940 자동차 샘플+ 7,842 배경 샘플&lt;br /&gt;테스트 세트: 그 외의 모든 샘플들&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;각 예들은 잘못된 예제를 수동으로 보정한 반자동 프로토콜에 의해 라벨링된다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;MVSEC 데이터세트&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211121000953107.png&quot; alt=&quot;image-20211121000953107&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;다중 센서로 3차원 인식을 하기 위해 생성된 MultiVehicle Stereo Event Camera dataset&lt;/li&gt;
      &lt;li&gt;동기화된 스테레오 이벤트 기반 뉴로모픽 비전 시스템에 대한 첫 번째 데이터 세트&lt;/li&gt;
      &lt;li&gt;보정된(calibrated) 라이다 시스템으로부터 생성된 ground-truth 깊이 데이터를 포함&lt;/li&gt;
      &lt;li&gt;다양한 조도 및 주행 속도 환경에서의 긴 야외 시퀀스로 구성됨&lt;/li&gt;
      &lt;li&gt;이벤트 기반 시각적 주행거리 측정(&lt;em&gt;visual odometry&lt;/em&gt;), 위치 파악(&lt;em&gt;localization&lt;/em&gt;), 장애물 회피(&lt;em&gt;obstacle avoidance&lt;/em&gt;), 그리고 3차원 재구성(&lt;em&gt;3D reconstruction&lt;/em&gt;) 평가에 사용할 수 있음&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;DDD17 데이터세트&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-11-21-cv-neuromorphic vision/image-20211121001020199.png&quot; alt=&quot;image-20211121001020199&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;DAVIS 센서로 취득된 첫 번재 대규모 공공 데이터&lt;/li&gt;
      &lt;li&gt;스위스에서 독일까지 주행하며 고속도로와 도시 환경에서 기록한 데이터를 포함&lt;/li&gt;
      &lt;li&gt;1,000km 이상의 거리를 커버하는 서로 다른 날씨, 길, 그리고 빛 환경에서 수집된 12시간 이상의 데이터&lt;/li&gt;
      &lt;li&gt;속력, GPS 위치, 운전자 조향, throttle 그리고 브레이크와 같은 차량 데이터가 함께 수집됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Chen, Guang, et al. “Event-based neuromorphic vision for autonomous driving: a paradigm shift for bio-inspired visual sensing and perception.” IEEE Signal Processing Magazine 37.4 (2020): 34-49.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음 포스팅에서는 뉴로모픽 비전의 현황과 전망, 그리고 사례에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 21 Nov 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//ai-computervision/neuromorphicVision02/</link>
        <guid isPermaLink="true">https://dazory.github.io//ai-computervision/neuromorphicVision02/</guid>
        
        <category>robotics</category>
        
        <category>robot vision</category>
        
        <category>neuromorphic vision</category>
        
        
        <category>ai-computerVision</category>
        
      </item>
    
      <item>
        <title>[NeuromorphicVision-001] 뉴로모픽 비전(Neuromorphic Vision)의 배경 및 개념</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;랩세미나에서 Neuromorphic vision에 관한 발표를 보고 관심이 생겼다. 관련하여 배경과 주요 개념 및 현황을 조사하고자 본 글을 작성하게 되었다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;neuromorphic-vision&quot;&gt;Neuromorphic Vision&lt;/h1&gt;

&lt;h2 id=&quot;배경&quot;&gt;배경&lt;/h2&gt;

&lt;h3 id=&quot;기존-프레임-기반-이미지-센서&quot;&gt;기존 프레임 기반 이미지 센서&lt;/h3&gt;

&lt;p&gt;현재 가장 대중적인 이미지 센서는 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CCD(&lt;em&gt;charge-coupled devices&lt;/em&gt;)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CMOS APS(&lt;em&gt;Active Pixel Sensors&lt;/em&gt;)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;두 이미지 센서는 모두 시각적 정보를 연속적인 “스냅샷(&lt;em&gt;snapshot&lt;/em&gt;)” 영상 (frames)의 형태로 취득하는 &lt;u&gt;프레임 기반 이미지 센서&lt;/u&gt;이다. 따라서 다음의 한계를 수반한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;고정된 프레임 속도(&lt;em&gt;frame rate&lt;/em&gt;)로 동작함에 따라 발생하는 문제&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;낮은 프레임 속도인 경우, 중요한 정보의 손실이 발생&lt;/li&gt;
      &lt;li&gt;높은 프레임 속도인 경우, 과하게 불필요한 정보까지 취득&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;⇒ 데이터 집약적(&lt;em&gt;data-intensive&lt;/em&gt;)이거나 지연에 민감한(&lt;em&gt;delay-sensitive&lt;/em&gt;) 응용 분야(e.g., 고속 모터 제어, 자율 로봇 네비게이션)에서 치명적인 단점으로 작용&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;또한 기존의 이미지 센서는 시각정보를 디지털 정보로 받아들이기 위한 &lt;strong&gt;2차원 이미지 센서 어레이(&lt;em&gt;2D image sensors array&lt;/em&gt;, CCD or APS)&lt;/strong&gt;, 시각 정보를 저장하기 위한 &lt;strong&gt;메모리 유닛(&lt;em&gt;memory unit&lt;/em&gt;)&lt;/strong&gt;, 그리고 컴퓨터비전 알고리즘을 실행하기 위한 &lt;strong&gt;프로세싱 유닛(&lt;em&gt;processing unit&lt;/em&gt;)&lt;/strong&gt;으로 구성된다. 이러한 폰 노이만 구조&lt;sup&gt;&lt;a href=&quot;#footnote_폰노이만&quot;&gt;[개념]&lt;/a&gt;&lt;/sup&gt;는 이미지 센서와 프로세싱 유닛 간의 데이터 이동에 따라 다음의 한계를 수반한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;지연 (&lt;em&gt;latency&lt;/em&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;높은 통신 대역폭(&lt;em&gt;communication bandwidth&lt;/em&gt;)이 요구됨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;높은 전력 소모 (&lt;em&gt;high power consumption&lt;/em&gt;)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;이벤트-기반-뉴로모픽-비전&quot;&gt;이벤트 기반 뉴로모픽 비전&lt;/h3&gt;

&lt;p&gt;고전적인 이미지 센서와 달리 생물학적 시스템, 예를 들어 인간의 시각 시스템은 시각 정보가 지나가는 &lt;strong&gt;렌즈(&lt;em&gt;lens&lt;/em&gt;)&lt;/strong&gt;, 들어온 정보를 인지하고 전처리하는 &lt;strong&gt;망막(&lt;em&gt;retina&lt;/em&gt;)&lt;/strong&gt;, 그리고 추출된 정보가 지나가는 &lt;strong&gt;시신경(&lt;em&gt;optic nerves&lt;/em&gt;)&lt;/strong&gt;과 최종적인 처리를 위한 &lt;strong&gt;시각피질(&lt;em&gt;visual cortex&lt;/em&gt;)&lt;/strong&gt;로 구성된다. 뉴로모픽 비전 센서는 이 중 &lt;strong&gt;망막(&lt;em&gt;retina&lt;/em&gt;)&lt;/strong&gt;의 역할에서 아이디어를 얻었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;망막의 역할
    &lt;ul&gt;
      &lt;li&gt;불필요한 시각 정보를 버림&lt;/li&gt;
      &lt;li&gt;뇌에서 더 많은 정보 처리(e.g., 패턴 인식, 해석)를 하도록 가속화해줌&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, 망막은 인간이 시각적 정보를 동시에 센싱하고 전처리할 수 있도록 해준다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;뉴로모픽 비전(&lt;em&gt;Neuromorphic Vision&lt;/em&gt;)&lt;/strong&gt;은 이러한 &lt;u&gt;생물학적 비전 시스템을 모방&lt;/u&gt;하여 기존의 디지털 비전 센서들이 갖고있는 문제점을 해결하고자 하였다. 즉, 저전력(&lt;em&gt;low-power&lt;/em&gt;) 고효율(&lt;em&gt;high-efficient&lt;/em&gt;) 이미지 처리를 목표로 한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;개념&quot;&gt;개념&lt;/h2&gt;

&lt;p&gt;뉴로모픽 비전은 &lt;strong&gt;뉴로모픽 비전 칩(&lt;em&gt;Neuromorphic vision chips&lt;/em&gt;)&lt;/strong&gt;을 통해 달성된다. 뉴로모픽 비전 칩은 여러 개의 특수 목적 비전 센서를 활용하여 고차원의 입력을 저차원의 출력으로 감소시켜준다.&lt;/p&gt;

&lt;h3 id=&quot;구조&quot;&gt;구조&lt;/h3&gt;

&lt;p&gt;뉴로모픽 비전 칩은 입력을 전처리하기 위해 다양한 특수목적 비전 센서를 여러 개 사용한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;특수목적 센서의 예시
    &lt;ul&gt;
      &lt;li&gt;운동방향 센서(&lt;em&gt;direction-of-motion sensors&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;속도센서(&lt;em&gt;velocity sensors&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;추적센서(&lt;em&gt;tracking sensors&lt;/em&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이러한 각 지각 센서(&lt;em&gt;perceptive sensors&lt;/em&gt;)를 통해 고차원의 입력을 저차원의 출력으로 감소시킬 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;구현&quot;&gt;구현&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;뉴런 ⇔ 트랜지스터 회로&lt;/li&gt;
  &lt;li&gt;시냅스 ⇔ 시냅스 모방 소자&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;뉴로모픽 비전 칩의 기능은 트랜지스터 혹은 새로운 장치 기반의 고전적인 디지털 VLSI&lt;sup&gt;&lt;a href=&quot;#footnote_VLSI&quot;&gt;[개념]&lt;/a&gt;&lt;/sup&gt;기술로 제작된 아날로그 회로를 통해 구현된다. VLSI 기술을 사용하여 크기가 작고 밀집된 형태로 제작할 수 있으므로 기존의 프레임 기반 이미지 센서에 비해 이점을 갖는다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;특징&quot;&gt;특징&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;아날로그 방식(&lt;em&gt;analogue&lt;/em&gt;)&lt;/strong&gt; : 뉴로모픽 비전 칩의 기능은 아날로그 회로를 통해 구현된다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;에너지 효율적(&lt;em&gt;energy-efficiently processing&lt;/em&gt;)&lt;/strong&gt; : 디지털 시스템에 비해 10,000배 낮은 전력 소비&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;빠른 속도로 동작 (&lt;em&gt;quickly processing&lt;/em&gt;)&lt;/strong&gt; : 폰 노이만 구조가 아닌 병렬 구조로 연산을 수행하기 때문에 병목 현상을 해결하였다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;높은 동적 범위 (HDR, &lt;em&gt;high dynamic range&lt;/em&gt;)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;높은 시간적 해상도 (high temporal resolution)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;비동기(&lt;em&gt;asynchronously processing&lt;/em&gt;)&lt;/strong&gt; : 비동기 이벤트 스트림을 생성하여 환경을 감지 및 인식&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;사건 중심(&lt;em&gt;event-driven processing&lt;/em&gt;)&lt;/strong&gt; : 프레임 기반이 아닌 사건 기반의 환경 인식&lt;/li&gt;
  &lt;li&gt;self-adaptive&lt;/li&gt;
  &lt;li&gt;확장성(&lt;em&gt;scalable&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;부록관련-개념&quot;&gt;(부록)관련 개념&lt;/h1&gt;

&lt;h2 id=&quot;집적회로-ic-integrated-circuit&quot;&gt;집적회로 (IC, &lt;em&gt;Integrated Circuit&lt;/em&gt;)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;IC(&lt;em&gt;Integrated Circuit&lt;/em&gt;, 집적회로)&lt;/strong&gt;란, 반도체에 만든 전자회로의 집합을 의미한다.&lt;/p&gt;

&lt;h3 id=&quot;집적도에-따른-분류&quot;&gt;집적도에 따른 분류&lt;/h3&gt;

&lt;p&gt;IC는 &lt;u&gt;동일한 면적에 얼마나 많은 논리 소자를 집적했는지에 관한 지표&lt;/u&gt;인 &lt;strong&gt;집적도&lt;/strong&gt;에 따라 분류할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;SSI (&lt;em&gt;Small Scale Integration&lt;/em&gt;)&lt;/strong&gt; : 소규모 집적회로 (~100개의 게이트)(e.g., 기본적인 게이트 기능, 플립플롭)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MSI (&lt;em&gt;Medium Scale Integration&lt;/em&gt;)&lt;/strong&gt; : 중간 규모 집적회로 (100~1,000개의 게이트) (e.g., 인코더, 디코더, 카운터, 레지스터, 멀티플레서, 디멀티플렉서, 소형 기억 장치)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LSI (&lt;em&gt;Large Scale Integration&lt;/em&gt;)&lt;/strong&gt; : 대규모 집적회로 (1,000~10,000개의 게이트) (e.g., 컴퓨터의 메인 메모리, 계산기의 부품)&lt;/li&gt;
  &lt;li&gt;&lt;a name=&quot;footnote_VLSI&quot;&gt;&lt;strong&gt;VLSI (&lt;em&gt;Very Large Scale Integration&lt;/em&gt;)&lt;/strong&gt;&lt;/a&gt; : 초대규모 집적회로 (10,000~1,000,000개의 게이트) (e.g., 대규모 메모리, 대형 마이크로프로세서, 단일 칩 마이크로프로세서)
    &lt;ul&gt;
      &lt;li&gt;CMOS 기술의 설계 규칙 규격화로 그 제조 기술이 널리 보급되었음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ULSI (&lt;em&gt;Ultra Large Scale Integration&lt;/em&gt;)&lt;/strong&gt; : (1,000,000~개의 게이트) (e.g., 인텔의 486, 펜티엄의 ULSI)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;최근에는 하나의 집적회로에 최소 억 단위개의 소자가 집적된다. 따라서 이러한 분류는 점점 의미가 없어지고 있는 추세이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;제작-구성에-따른-분류&quot;&gt;제작 구성에 따른 분류&lt;/h3&gt;

&lt;p&gt;집적회로는 회로의 구성 요소에 따라 다음과 같이 분류할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;저항-트랜지스터 로직 (&lt;em&gt;Resistor-transistor logic&lt;/em&gt;, RTL)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Register_transfer_level_-_example_toggler.svg/300px-Register_transfer_level_-_example_toggler.svg.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;초기의 상용 논리군으로, 디지털 게이트의 기본 동작을 설명하기에 좋은 출발점이다.&lt;/li&gt;
      &lt;li&gt;최근에는 거의 쓰이지 않는다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;다이오드-트랜지스터 로직 (&lt;em&gt;Diode-transistor logic&lt;/em&gt;, DTL)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/DTL_NAND_Gate.svg/220px-DTL_NAND_Gate.svg.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;최근에는 거의 쓰이지 않는다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;트랜지스터-트랜지스터 로직 (&lt;em&gt;Transistor-transistor logic, TTL&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;양극성 트랜지스터와 NAND 회로를 사용한다.&lt;/li&gt;
      &lt;li&gt;가격이 저렴하며 논리회로 구현에 많이 사용된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;이미터-결합 논리 (&lt;em&gt;Emitter-coupled logic&lt;/em&gt;, ECL)&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;기본회로로 NOR 회로를 사용한다.&lt;/li&gt;
      &lt;li&gt;게이트 지연시간이 적어 고속회로로 사용된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;금속-산화물 반도체 (&lt;em&gt;Metal-oxide semiconductor&lt;/em&gt;, MOS)&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;금속 산화물을 사용하며, 절연체를 입혀서 구성되므로 집적도가 높다.&lt;/li&gt;
      &lt;li&gt;전력소모가 적으며 제조가 쉽다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;상보형 MOS (&lt;em&gt;Complementary metal-oxide semiconductor&lt;/em&gt;, CMOS)&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;인버터회로에 P-채널과 N-채널 트랜지스터를 같이 구성한다.&lt;/li&gt;
      &lt;li&gt;소비전력이 적다.&lt;/li&gt;
      &lt;li&gt;속도가 느리다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;기타&quot;&gt;기타&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a name=&quot;footnote_폰노이만&quot;&gt;&lt;strong&gt;폰 노이만 구조&lt;/strong&gt;&lt;/a&gt; : 대부분의 컴퓨터가 따르고 있는 기본 구조로, 주기억 장치, 중앙 처리 장치, 입출력 장치로 구성된다. 폰 노이만 구조 기반 반도체 칩은 정보를 순차적으로 처리하므로 대량의 비정형 정보에 대해 병목현상의 한계를 갖는다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Indiveri, Giacomo, and Rodney Douglas. “Neuromorphic vision sensors.” &lt;em&gt;Science&lt;/em&gt; 288.5469 (2000): 1189-1190.&lt;/li&gt;
  &lt;li&gt;Liao, Fuyou, Feichi Zhou, and Yang Chai. “Neuromorphic vision sensors: Principle, progress and perspectives.” &lt;em&gt;Journal of Semiconductors&lt;/em&gt; 42.1 (2021): 013105.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음 포스팅에서는 뉴로모픽 비전의 현황과 전망, 그리고 사례에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 07 Nov 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//ai-computervision/neuromorphicVision01/</link>
        <guid isPermaLink="true">https://dazory.github.io//ai-computervision/neuromorphicVision01/</guid>
        
        <category>robotics</category>
        
        <category>robot vision</category>
        
        <category>neuromorphic vision</category>
        
        
        <category>ai-computerVision</category>
        
      </item>
    
      <item>
        <title>[ROS2-001] 001. 개발환경 구축</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;주요 참고자료&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        페이지: &lt;a href=&quot;https://cafe.naver.com/openrt/25288&quot;&gt;001 ROS 2 개발 환경 구축 (오픈소스 소프트웨어 &amp;amp; 하드웨어: 로봇 기술 공유 카페 (오로카))&lt;/a&gt;&lt;br /&gt;
        작성자: 표윤석
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
본 글은 표윤석님의 &lt;a href=&quot;https://cafe.naver.com/openrt/25288&quot;&gt;001 ROS 2 개발 환경 구축 (오픈소스 소프트웨어 &amp;amp; 하드웨어: 로봇 기술 공유 카페 (오로카))&lt;/a&gt; 글을 바탕으로 작성되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;ros2-개발-환경-구축&quot;&gt;ROS2 개발 환경 구축&lt;/h1&gt;

&lt;p&gt;ROS 2 개발을 위해 다음과 같이 개발 환경을 구축한다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;구분&lt;/th&gt;
      &lt;th&gt;내용&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;기본 운영 체제&lt;/td&gt;
      &lt;td&gt;Linux Mint 20.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;로봇 운영 체제&lt;/td&gt;
      &lt;td&gt;ROS 2 Foxy FItzroy&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;통합 개발 환경 (IDE)&lt;/td&gt;
      &lt;td&gt;Visual Studio Code&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;프로그래밍 언어&lt;/td&gt;
      &lt;td&gt;Python 3(3.8.0), C++ 14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;기타&lt;/td&gt;
      &lt;td&gt;CMake 3.16.3, Qt 5.12.5, OpenCV 4.2.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;개발환경은 &lt;a href=&quot;https://cafe.naver.com/openrt/25288&quot;&gt;오로카-001 ROS 2 개발 환경 구축&lt;/a&gt;에서 추천하는 설정으로 맞추었다. 이 외의 다른 선택사항이 궁금하다면 본 글을 참고하길 바란다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;기본-운영-체제&quot;&gt;기본 운영 체제&lt;/h2&gt;

&lt;p&gt;Linux Mint 20.x 대신에 Ubuntu 20.04.x LTS를 사용해도 된다. 이전에 졸업프로젝트할 때 Ubuntu 18.04 LTS를 사용한 경험이 있는데, Linux Mint는 써본 적이 없어서 궁금하기도 했다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;linux-mint-vs-ubuntu&quot;&gt;Linux Mint vs. Ubuntu&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Feature&lt;/th&gt;
      &lt;th&gt;Ubuntu 20.04&lt;/th&gt;
      &lt;th&gt;Linux Mint 20&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Release Dates&lt;/td&gt;
      &lt;td&gt;2020.04.23&lt;/td&gt;
      &lt;td&gt;2020.06.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Support Period&lt;/td&gt;
      &lt;td&gt;2025&lt;/td&gt;
      &lt;td&gt;2025&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Based on&lt;/td&gt;
      &lt;td&gt;Debian&lt;/td&gt;
      &lt;td&gt;Ubuntu 20.04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Linux Kernel&lt;/td&gt;
      &lt;td&gt;Linux 5.4 kernel&lt;/td&gt;
      &lt;td&gt;Linux 5.4 kernel&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Ubuntu 대신 Linux Mint를 사용하는 대략적인 이유[1]는 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;느리고 리소스를 많이 소모하는 Ubuntu Software Center와 달리 Linux Mint의 Software Manager는 더 가볍고 빠르다.&lt;/li&gt;
  &lt;li&gt;Linux Mint는 외부 패키지 제거, 누락된 키 추가, 중복 항목 제거 등 Ubuntu에서 얻을 수 없는 옵션들을 제공한다.&lt;/li&gt;
  &lt;li&gt;Ubuntu는 모든 AMD 시스템에서 정기적으로 충돌하는 반면 Linux Mint는 안정적이다.&lt;/li&gt;
  &lt;li&gt;Unity 기반 그래픽인 Ubuntu와 달리, Linux Mint의 Cinnamon은 가벼운 그래픽 환경을 제공하여 속도가 빠르다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;사실 Linux Mint가 절대적으로 Ubuntu보다 좋다고 말할 수는 없다(Ubuntu의 사용자의 수가 Linux Mint보다 훨씬 많다). 찾아 본 결과, 둘이 큰 차이가 있는 것 같진 않다. 만일 기존에 windows를 사용했었고, 초보자라면 Ubuntu보다는 Linux Mint를 추천한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;부팅-방법&quot;&gt;부팅 방법&lt;/h3&gt;

&lt;p&gt;나는 하나의 노트북으로 Windows와 Linux Mint를 모두 사용해야 한다. 두 가지 OS를 하나의 PC에서 동시에 사용하는 방법은 아래와 같이 크게 두 가지가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Virtual Machine(가상 머신)&lt;/li&gt;
  &lt;li&gt;Dual Booting (듀얼 부팅)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;-virtual-machine&quot;&gt;① Virtual Machine&lt;/h4&gt;

&lt;p&gt;Virtual Machine은 컴퓨팅 환경을 소프트웨어로 구현한 것이다. 즉, 하나의 PC안에 가상의 PC를 생성하여 사용하는 것이다. Virtual Machine을 구현하는 소프트웨어는 크게 세 가지가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.virtualbox.org/&quot;&gt;VirtualBox&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.vmware.com/kr.html&quot;&gt;VMware&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;클라우드 가상머신&lt;br /&gt;&lt;a href=&quot;https://aws.amazon.com/ko/ec2/&quot;&gt;EC2(Amazon Web Services)&lt;/a&gt;, &lt;a href=&quot;https://cloud.google.com/compute&quot;&gt;Google Compute Engine(Google Cloud Platform)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://www.virtualbox.org/raw-attachment/wiki/Screenshots/OpenSuse13.2_on_Windows_7.png&quot; alt=&quot;OpenSuse13.2_on_Windows_7.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 이미지(출처: VirtualBox)처럼 새 창 자체가 하나의 PC처럼 동작한다.&lt;/p&gt;

&lt;p&gt;Virutal Machine은 설치 및 제거가 비교적 쉽지만 실제 물리적 PC가 아니므로 오류가 심하고, 느리고, 일부 소프트웨어를 지원하지 않는다는 단점이 있다. 따라서 간단한 작업을 할 때에는 유용하지만 개발을 하기에는 적합하지 않은 환경이다. 따라서 나는 두 번째 방법을 사용했다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;-dual-booting&quot;&gt;② Dual Booting&lt;/h4&gt;

&lt;p&gt;멀티 부팅은 하나의 PC의 자원을 나누어 여러 OS를 동시에 사용하는 것이다. 자원을 나눈다는 것은 partition을 나눈다는 것과 같으며, ~~~Virtual Machine과 달리 물리적 PC를 기반으로 하기때문에 일반적인 사용감과 동일하다.&lt;/p&gt;

&lt;p&gt;Windows+Ubuntu를 사용하든 Windows+Linux Mint를 사용하든, 기본적으로 파티션 나누는 방식은 동일하다. 듀얼부팅 방법은 검색어(“windows linux mint dual boot”) 결과로 나오는 여러 포스팅 중 하나를 골라서 따라하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/c/cf/GRUB_with_ubuntu_and_windows_vista.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dual Booting을 사용하면, PC의 전원을 켤 때 위와 같은 화면이 나오며 OS를 선택할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;linux-mint-20x-설치&quot;&gt;Linux Mint 20.x 설치&lt;/h3&gt;

&lt;h4 id=&quot;iso파일-다운로드&quot;&gt;.ISO파일 다운로드&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://linuxmint.com/edition.php?id=288&quot;&gt;LinuxMint&lt;/a&gt;에서 Linux Mint 20.2 Cinnamon Edition의 .ISO파일을 다운로드 받는다. 나는 Download mirrors에서 &lt;a href=&quot;https://ftp.kaist.ac.kr/linuxmint-iso/stable/20.2/linuxmint-20.2-cinnamon-64bit.iso&quot;&gt;KAIST&lt;/a&gt;를 통해 다운로드받았다.&lt;/p&gt;

&lt;h4 id=&quot;라이브-usb로-만들기&quot;&gt;라이브 USB로 만들기&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20210928235522477.png&quot; alt=&quot;image-20210928235522477&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://rufus.ie/ko/&quot;&gt;Rufus&lt;/a&gt;를 이용하여 USB를 만들 수 있다. 사이트에서 사용 방법을 참고하여 ISO 파일로부터 부팅 가능한 USB 드라이브를 생성해준다.&lt;/p&gt;

&lt;h4 id=&quot;installation&quot;&gt;Installation&lt;/h4&gt;

&lt;p&gt;전원이 켜질 때 F2(또는 F10 또는 F12; 삼성 노트북은 F2)를 눌러 BIOS 설정으로 들어간다. BIOS 설정에서 [Boot] 를 선택한다. 이후 Priority선택에서 위 USB 드라이브를 1순위로 선택 &amp;amp; Save &amp;amp; Exit하여 해당 OS로 들어간다. 이후 바탕화면에 있는 설치파일을 이용하여 설치해주면 된다.&lt;/p&gt;

&lt;p&gt;※ 설치할 때에는 꼭 language를 영어로 하는 것을 추천한다. 개발할 때에는 영어 설정이 무조건 디폴트이다..! ※&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[참고] BIOS에서 부팅 순서 변경 방법 : &lt;a href=&quot;https://steemit.com/kr/@nightofwin/4fsdnu&quot;&gt;https://steemit.com/kr/@nightofwin/4fsdnu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[참고] Linux Mint 20설치 방법(ISO다운로드부터 설치까지): &lt;a href=&quot;https://websetnet.net/ko/how-to-install-linux-mint-20-the-simplest-way-possible/&quot;&gt;https://websetnet.net/ko/how-to-install-linux-mint-20-the-simplest-way-possible/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;linux-mint-초기-세팅&quot;&gt;Linux Mint 초기 세팅&lt;/h3&gt;

&lt;h4 id=&quot;한글-사용-가능&quot;&gt;한글 사용 가능&lt;/h4&gt;

&lt;p&gt;초반에 설치할 때 language를 영어로했다면, 한글이 입력되지 않을 것이다. 한글을 사용하는 방법은 여러가지가 있는데, 그 중 &lt;a href=&quot;https://github.com/libhangul/nabi&quot;&gt;Nabi&lt;/a&gt; 소프트웨어를 이용한 방법을 소개한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Software manager에서 “Nabi”를 검색 및 다운로드한다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004225104860.png&quot; alt=&quot;image-20211004225104860&quot; style=&quot;height:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[System Settings]-[Input Method]에 들어간다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004225157071.png&quot; alt=&quot;image-20211004225157071&quot; style=&quot;height:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[Korean]에서 Input method framework를 “Hangul”로 변경해준다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004225238531.png&quot; alt=&quot;image-20211004225238531&quot; style=&quot;height:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이후 종료 후 다시 접속하면 우측 아래에 나비모양 아이콘이 생성된 것을 확인할 수 있다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004225341910.png&quot; alt=&quot;image-20211004225341910&quot; style=&quot;height:200px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;나비모양 아이콘을 클릭 및 설정을 변경하여 한영 변환키를 설정할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;한국에-있는-mirror로-변경&quot;&gt;한국에 있는 mirror로 변경&lt;/h4&gt;

&lt;p&gt;mirror는 컴퓨팅에서 자료 모음의 복사본이다 (출처: 위키백과). 세계 각 지역에 다양하게 분포된 사용자가 해당 파일을 빠르게 다운로드할 수 있도록 하기 위해 지역별로 mirror를 두어 일종의 cache처럼 사용한다. 따라서 나는 mirror site를 한국으로 변경해주었다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;[Software Sources]을 연다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004225704806.png&quot; alt=&quot;image-20211004225704806&quot; style=&quot;height:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mirrors의 site를 한국 국기가 있는 것들로 바꿔준다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004225740164.png&quot; alt=&quot;image-20211004225740164&quot; style=&quot;height:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;로봇-운영체제&quot;&gt;로봇 운영체제&lt;/h2&gt;

&lt;p&gt;로봇 운영체제로는 &lt;a href=&quot;https://docs.ros.org/en/foxy/Releases/Release-Foxy-Fitzroy.html&quot;&gt;ROS 2 Foxy FItzroy&lt;/a&gt;를 사용한다. 설치 방법은 크게 세 가지가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;소스 빌드&lt;/li&gt;
  &lt;li&gt;미리 빌드된 파일 사용&lt;/li&gt;
  &lt;li&gt;데비안 패키지 사용&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 중 세 번째 방법(데비안 패키지 사용)이 설치 및 업데이트가 가장 간단하므로 해당 방법을 사용하여 설치한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ros-2-설치-데비안-패키지-이용&quot;&gt;ROS 2 설치 (데비안 패키지 이용)&lt;/h3&gt;

&lt;p&gt;데비안 패키지를 사용하여 설치하는 방법은 &lt;a href=&quot;https://docs.ros.org/en/foxy/Installation/Ubuntu-Install-Debians.html&quot;&gt;ros의 공식 문서: Installing ROS 2 via Debian Packages&lt;/a&gt;를 참고하였다.&lt;/p&gt;

&lt;h4 id=&quot;1-set-locale&quot;&gt;1. Set Locale&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Locale&lt;/em&gt;&lt;/strong&gt;은사용자 인터페이스에서 사용되는 언어, 지역 설정, 출력 형식 등을 정의하는 문자열이다. locale이 UTF-8을 지원하도록 다음의 명령어를 통해 설정해줄 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;locale  &lt;span class=&quot;c&quot;&gt;# check for UTF-8&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;locales
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;locale-gen en_US en_US.UTF-8
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;update-locale &lt;span class=&quot;nv&quot;&gt;LC_ALL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;en_US.UTF-8 &lt;span class=&quot;nv&quot;&gt;LANG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;en_US.UTF-8
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;LANG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;en_US.UTF-8

locale  &lt;span class=&quot;c&quot;&gt;# verify settings&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;2-setup-sources&quot;&gt;2. Setup Sources&lt;/h4&gt;

&lt;p&gt;ROS 2 apt repositories를 시스템에 추가해야한다.&lt;/p&gt;

&lt;div class=&quot;callout&quot;&gt;
    &lt;b&gt;APT(Advanced Packaging Tool)&lt;/b&gt;란?&lt;br /&gt;
    데비안 GNU/Linux에서 소프트웨어 설치 및 제거를 위해 사용하는 설치 응용 프로그램이다. ① 사용자가 원하는 소프트웨어 패키지를 로드하고 ② 종속 항목을 자동으로 설치해주고 ③ 설치된 모든 소프트웨어에 대한 업데이트를 자동으로 해주는 역할을 한다. 설치나 업데이트를 할 때 관련 데이터는 repository라는 소프트웨어 패키지의 DB로부터 가져올 수 있다.
&lt;/div&gt;

&lt;p&gt;이를 위해 우선 아래와 같이 GPG key를 승인받는다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;curl gnupg2 lsb-release
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-sSL&lt;/span&gt; https://raw.githubusercontent.com/ros/rosdistro/master/ros.key  &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; /usr/share/keyrings/ros-archive-keyring.gpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이후 repository를 sources list에 추가한다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;deb [arch=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;dpkg &lt;span class=&quot;nt&quot;&gt;--print-architecture&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu focal main&quot;&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;sudo tee&lt;/span&gt; /etc/apt/sources.list.d/ros2.list &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;3-install-ros-2-packages&quot;&gt;3. Install ROS 2 packages&lt;/h4&gt;

&lt;p&gt;repositories를 세팅한 후 apt repository caches를 업데이트해준다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt update
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;ros-foxy-desktop ros-foxy-rmw-fastrtps&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; ros-foxy-rmw-cyclonedds&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros-foxy-desktop&lt;/code&gt; : ROS, RViz, demos, tutorials를 포함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros-foxy-rmw-fastrtps*&lt;/code&gt; : rmw_fastrtps는 ROS 2와 eProsima’s Fast DDS 미들웨어간의 interface를 제공&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ros-foxy-rmw-cyclonedds*&lt;/code&gt; : 쉽고, 빠르고, 신뢰할 수 있고, 작은 ROS 2를 위한 Eclipse Cyclone DDS 미들웨어로, 이를 통해 Eclipse Cyclone DDS를 기본 DDS 구현으로 사용할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;4-environment-setup&quot;&gt;4. Environment Setup&lt;/h4&gt;

&lt;p&gt;다음의 file을 소싱(Sourcing)함으로써 테스트를 위한 환경을 설정할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /opt/ros/foxy/setup.bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;여기까지 문제없이 설치되었다면, 몇 가지 examples를 시도해볼 수 있다. 터미널창을 두 개 열어서 아래의 예제를 수행해보도록 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;C++ talker&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /opt/ros/foxy/setup.bash
ros2 run demo_nodes_cpp talker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Python listener&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /opt/ros/foxy/setup.bash
ros2 run demo_nodes_py listener
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;실행결과&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004231206247.png&quot; alt=&quot;image-20211004231206247&quot; style=&quot;height:200px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;5-ros-개발-툴-설치&quot;&gt;5. ROS 개발 툴 설치&lt;/h4&gt;

&lt;p&gt;ROS 2를 이용한 로봇 프로그래밍에 필수인 소프트웨어들을 아래와 같이 설치한다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  build-essential &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  cmake &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  git &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  libbullet-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  python3-colcon-common-extensions &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  python3-flake8 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  python3-pip &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  python3-pytest-cov &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  python3-rosdep &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  python3-setuptools &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  python3-vcstool &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  wget
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-U&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; argcomplete &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; flake8-blind-except &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; flake8-builtins &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; flake8-class-newline &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; flake8-comprehensions &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; flake8-deprecated &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; flake8-docstrings &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; flake8-import-order &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; flake8-quotes &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; pytest-repeat &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; pytest-rerunfailures &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; pytest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--no-install-recommends&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; libasio-dev &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; libtinyxml2-dev &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; libcunit1-dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;6-ros-2-빌드-테스트&quot;&gt;6. ROS 2 빌드 테스트&lt;/h4&gt;

&lt;p&gt;workspace 폴더를 생성한 뒤 빌드하며 ROS 2 설치가 잘 되어 빌드에 문제가 없는지를 알아본다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /opt/ros/foxy/setup.bashmkdir &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; ~/robot_ws/srccd ~/robot_ws/colcon build &lt;span class=&quot;nt&quot;&gt;--symlink-installls&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;아래와 같이 하위폴더로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;install&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src&lt;/code&gt; 폴더가 생성되었다면 빌드에 성공한 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004231347445.png&quot; alt=&quot;image-20211004231347445&quot; style=&quot;height:200px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;7-run-commands-설정&quot;&gt;7. Run Commands 설정&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source /opt/ros/foxy/setup.bash&lt;/code&gt;는 환경을 설정하기 위해 file을 소싱하는 명령어이다. 매번 환경설정을 불러오기가 귀찮으므로 아래와 같이 run commands(rc)만을 모아두는 bashrc 파일에 자주 사용되는 alias, source, export를 설정해두면 매우 편리하다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nano ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;bashrc 파일의 기존 내용은 냅두고 맨 아래에 다음의 설정을 추가해준다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /opt/ros/foxy/setup.bashsource ~/robot_ws/install/local_setup.bashsource /usr/share/colcon_argcomplete/hook/colcon-argcomplete.bashsource /usr/share/vcstool-completion/vcs.bashsource /usr/share/colcon_cd/function/colcon_cd.shexport &lt;span class=&quot;nv&quot;&gt;_colcon_cd_root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;~/robot_wsexport &lt;span class=&quot;nv&quot;&gt;ROS_DOMAIN_ID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;7export &lt;span class=&quot;nv&quot;&gt;ROS_NAMESPACE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;robot1export &lt;span class=&quot;nv&quot;&gt;RMW_IMPLEMENTATION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rmw_fastrtps_cpp# &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RMW_IMPLEMENTATION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rmw_connext_cpp# &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RMW_IMPLEMENTATION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rmw_cyclonedds_cpp# &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RMW_IMPLEMENTATION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rmw_gurumdds_cpp# &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RCUTILS_CONSOLE_OUTPUT_FORMAT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'[{severity} {time}] [{name}]: {message} ({function_name}() at {file_name}:{line_number})'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RCUTILS_CONSOLE_OUTPUT_FORMAT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'[{severity}]: {message}'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RCUTILS_COLORIZED_OUTPUT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1export &lt;span class=&quot;nv&quot;&gt;RCUTILS_LOGGING_USE_STDOUT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0export &lt;span class=&quot;nv&quot;&gt;RCUTILS_LOGGING_BUFFERED_STREAM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1alias &lt;span class=&quot;nv&quot;&gt;cw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'cd ~/robot_ws'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;cs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'cd ~/robot_ws/src'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ccd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'colcon_cd'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;cb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'cd ~/robot_ws &amp;amp;&amp;amp; colcon build --symlink-install'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;cbs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'colcon build --symlink-install'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;cbp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'colcon build --symlink-install --packages-select'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;cbu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'colcon build --symlink-install --packages-up-to'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ct&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'colcon test'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ctp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'colcon test --packages-select'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ctr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'colcon test-result'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;rt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'ros2 topic list'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'ros2 topic echo'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;rn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'ros2 node list'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;killgazebo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'killall -9 gazebo &amp;amp; killall -9 gzserver  &amp;amp; killall -9 gzclient'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;af&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'ament_flake8'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ac&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'ament_cpplint'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;testpub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'ros2 run demo_nodes_cpp talker'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;testsub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'ros2 run demo_nodes_cpp listener'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;testpubimg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'ros2 run image_tools cam2image'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;testsubimg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'ros2 run image_tools showimage'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;통합-개발-환경-ide&quot;&gt;통합 개발 환경 (IDE)&lt;/h2&gt;

&lt;p&gt;VSCode는 다양한 운영체제 및 프로그래밍 언어를 지원하며, 32bit, amd64, arm64를 지원하므로 ARM 계열의 임베디드 보드에서도 사용가능하다는 장점이 있다. 따라서 ROS 2 개발 환경으로 VSCode를 설치하도록 한다.&lt;/p&gt;

&lt;h3 id=&quot;vscode-설치&quot;&gt;VSCode 설치&lt;/h3&gt;

&lt;h4 id=&quot;1-설치&quot;&gt;1. 설치&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://code.visualstudio.com/Download&quot;&gt;visual studio-Download&lt;/a&gt;에서 .deb 파일을 다운로드 및 실행하여 설치한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004231648721.png&quot; alt=&quot;image-20211004231648721&quot; style=&quot;height:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;2-실행&quot;&gt;2. 실행&lt;/h4&gt;

&lt;p&gt;터미널을 열어 아래 명령어를 통해 실행한다.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;code
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;3-extensions-설치&quot;&gt;3. Extensions 설치&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004231805641.png&quot; alt=&quot;image-20211004231805641&quot; style=&quot;height:200px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 이미지에서 왼쪽 다섯 번째 줄의 Extensions을 클릭하여 아래의 항목들을 설치한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;C/C++/Python Extensions (VS Code Extensions for C++ and Python)
    &lt;ul&gt;
      &lt;li&gt;C/C++&lt;/li&gt;
      &lt;li&gt;CMake&lt;/li&gt;
      &lt;li&gt;CMake Tools&lt;/li&gt;
      &lt;li&gt;Python&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ROS Extensions (VS Code Extensions for ROS, URDF, Colcon)
    &lt;ul&gt;
      &lt;li&gt;ROS (code: ms-iot.vscode-ros)&lt;/li&gt;
      &lt;li&gt;URDF (code: smilerobotics.urdf)&lt;/li&gt;
      &lt;li&gt;Colcon Tasks (code: deitry.colcon-helper)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;File Format Extensions (VS Code Extensions for XML, YAML, Markdown)
    &lt;ul&gt;
      &lt;li&gt;XML Tools (code: dotjoshihohnson.xml)&lt;/li&gt;
      &lt;li&gt;YAML (code: redhat.vscode-yaml)&lt;/li&gt;
      &lt;li&gt;Markdown All in One (code: yzhang.markdown-all-in-one)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;useful Extensions (VS Code Extensions for Etc.)
    &lt;ul&gt;
      &lt;li&gt;Highlight Trailing White Spaces (code: ybaumes.highlight-trailing-white-spaces)&lt;/li&gt;
      &lt;li&gt;EOF Mark (code: msfukui.eof-mark)&lt;/li&gt;
      &lt;li&gt;Bracket Pair Colorizer (code: coenraads.bracket-pair-colorizer)&lt;/li&gt;
      &lt;li&gt;Better Comments (code: aaron-bond.better-comments)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;4-workspace-설정&quot;&gt;4. Workspace 설정&lt;/h4&gt;

&lt;p&gt;[File]-[Add Folder to Workspace]를 선택한 뒤, 이전에 만들었던 robot_ws 디렉토리(혹은 사용하고자 하는 workspace)를 선택하여 workspace를 세팅할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;5-user-settings&quot;&gt;5. User Settings&lt;/h4&gt;

&lt;p&gt;다음의 세팅을 통해 VSCode 환경에서 ROS 개발을 할 수 있다.&lt;/p&gt;

&lt;h5 id=&quot;settingsjson&quot;&gt;settings.json&lt;/h5&gt;

&lt;p&gt;settings.json은 VSCode의 사용자별 글로벌 환경 설정을 지정하는 파일로, 여기에 기술된 설정들은 모든 작업 공간(workspace)에서 적용된다 (e.g., 미니맵 사용, 세로 제한 줄 표시, 탭 사이즈 등).&lt;/p&gt;

&lt;p&gt;[참고] How to edit settings.json in Visual Studio Code? : &lt;a href=&quot;https://supunkavinda.blog/vscode-editing-settings-json&quot;&gt;https://supunkavinda.blog/vscode-editing-settings-json&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.config/Code/User/settings.json&lt;/code&gt; 를 아래와 같이 수정해준다.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;cmake.configureOnOpen&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;editor.minimap.enabled&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;editor.mouseWheelZoom&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;editor.renderControlCharacters&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;editor.rulers&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;editor.tabSize&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;files.associations&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;some&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;extensions&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;ROS&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;*.repos&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;yaml&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;*.world&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;xml&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;*.xacro&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;xml&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;files.insertFinalNewline&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;files.trimTrailingWhitespace&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;terminal.integrated.scrollback&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;workbench.iconTheme&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;vscode-icons&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;workbench.editor.pinnedTabSizing&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;compact&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ros.distro&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;foxy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;       &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Version&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;ROS&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;colcon.provideTasks&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Use&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;the&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;supporting&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;colcon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h5 id=&quot;c_cpp_propertiesjson&quot;&gt;c_cpp_properties.json&lt;/h5&gt;

&lt;p&gt;C/C++을 사용하기 위한 관련 설정이다. C/C++는 어떤 표준을 기준으로 규칙을 사용할 것인지, 컴파일 경로, intelliSense 모드 등을 설정할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl&lt;/code&gt;+&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shift&lt;/code&gt;+&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt;를 누른 뒤, “c/c++ edit configuration (json)”을 선택하여 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/robot_ws/.vscode/c_cpp_properties.json&lt;/code&gt;위치에 파일을 자동으로 생성할 수 있다. 파일 내용은 다음과 같이 설정한다.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;configurations&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Linux&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;includePath&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;          &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;${default}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;          &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;${workspaceFolder}/**&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;          &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/opt/ros/foxy/include/**&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;defines&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;compilerPath&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/usr/bin/g++&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;cStandard&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;c99&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;cppStandard&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;c++14&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;intelliSenseMode&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;linux-gcc-x64&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h5 id=&quot;tasksjson&quot;&gt;tasks.json&lt;/h5&gt;

&lt;p&gt;VSCode에서 외부 프로그램을 CLI(Command Line Interface)를 통해 연동하는 기능을 task라고 한다. ROS 2에서 빌드할 때 사용되는 colcon과 관련된 build, test, clean 작업을  task로 만들 수 있다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/robot_ws/.vscode/tasks.json&lt;/code&gt;위치에 아래의 내용을 설정한다.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2.0.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;tasks&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;colcon: build&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;shell&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;command&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;colcon build --cmake-args '-DCMAKE_BUILD_TYPE=Debug'&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;problemMatcher&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;group&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;kind&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;build&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;isDefault&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;colcon: test&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;shell&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;command&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;colcon test &amp;amp;&amp;amp; colcon test-result&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;colcon: clean&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;shell&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;command&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rm -rf build install log&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h5 id=&quot;launchjson&quot;&gt;launch.json&lt;/h5&gt;

&lt;p&gt;Launch는 Run and Debug에서 사용되는 실행 명령어이다. launch.json의 설정을 통해 Launch가 실행되기 전(즉, 디버깅하기 전)에 사용할 Task를 지정하거나 콘솔 기능을 설정할 수 있다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/rotob_ws/.vscode/launch.json&lt;/code&gt; 위치에 아래의 세팅을 통해 Python과 C++언어에 맞는 디버깅 툴을 지정할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0.2.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;configurations&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Debug-rclpy(debugpy)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;python&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;request&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;launch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;program&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;${file}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;console&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;integratedTerminal&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Debug-rclcpp(gbd)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;cppdbg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;request&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;launch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;program&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;${workspaceFolder}/install/${input:package}/lib/${input:package}/${input:node}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;args&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;preLaunchTask&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;colcon: build&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;stopAtEntry&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;cwd&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;${workspaceFolder}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;externalConsole&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;MIMode&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;gdb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;setupCommands&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;          &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;description&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Enable pretty-printing for gdb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;          &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;-enable-pretty-printing&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;          &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ignoreFailures&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;inputs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;package&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;promptString&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;description&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;package name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;default&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;topic_service_action_rclcpp_example&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;node&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;promptString&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;description&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;node name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;default&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;argument&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;C++은 디버깅툴로 GDB를 사용. GDB를 실행하기 전에 colcon build를 수행하도록 세팅.&lt;/li&gt;
  &lt;li&gt;Python은 디버깅툴로 debugpy를 사용. 별도의 빌드 없이 디버깅하도록 세팅.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;기타&quot;&gt;기타&lt;/h2&gt;

&lt;h3 id=&quot;qtcreator&quot;&gt;QtCreator&lt;/h3&gt;

&lt;p&gt;QtCreator는 GUI를 개발하는 데에 사용되는 프로그램이다.&lt;/p&gt;

&lt;p&gt;터미널에서 아래의 명령어를 통해 손쉽게 설치할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;qtcreator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이후 터미널에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qtcreator&lt;/code&gt;를 입력하여 실행할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;qtcreator-ros&quot;&gt;QtCreator ROS&lt;/h3&gt;

&lt;p&gt;QtCreator ROS는 ROS의 통일된 개발환경 구축을 위해 QtCreator의 플러그인 형태로 만든 것이다. QtCreator에 ROS 개발 환경 설정을 더 편하게 해놓았다. 설치 방법은 &lt;a href=&quot;https://ros-qtc-plugin.readthedocs.io/en/latest/index.html&quot;&gt;ROS Industrial Website의 문서&lt;/a&gt;를 참고했다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;설치파일 다운로드&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://ros-qtc-plugin.readthedocs.io/en/latest/_source/How-to-Install-Users.html&quot;&gt;ROS Industrial Website의 Docs » How to Install (Users)&lt;/a&gt;에서 가장 최신 버전의 설치 파일을 다운로드 받는다. 나는 Bionic Online Installer(.run)를 설치하였다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;설치&lt;/p&gt;

    &lt;p&gt;.run파일은 linux에서 직접적으로 설치가 불가능하다. 따라서 아래의 명령어를 통해 실행가능한 파일로 바꿔준 뒤, 실행한다.&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/Downloadschmod +x qtcreator-ros-bionic-latest-online-installer.runsudo ./qtcreator-ros-bionic-latest-online-installer.run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;※ .run 파일이 설치된 경로로 이동해주어야한다. 나는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/Downloads&lt;/code&gt;경로에 설치했으므로 해당 디렉토리로 이동하였다. ※&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004233711987.png&quot; alt=&quot;image-20211004233711987&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;기본세팅은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/root/QtCreator&lt;/code&gt;인데, 여기에 저장하면 관리가 어렵기때문에 설치 경로는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/(유저네임)/QtCreator&lt;/code&gt;로 지정해주었다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004233814661.png&quot; alt=&quot;image-20211004233814661&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;Qt Creator 컴포넌트를 선택해준다.&lt;/p&gt;

    &lt;p&gt;Next를 눌러가며 설치를 완료해준다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;업데이트&lt;/p&gt;

    &lt;p&gt;QtCreator를 열어서 [Help]-[Check Updates]를 눌러준다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-10-06-robotics-ros2001/image-20211004233931049.png&quot; alt=&quot;image-20211004233931049&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;설치가 잘 되었는지 확인&lt;/p&gt;

    &lt;p&gt;[File]-[New File or Project]-[Projects]-[Other Project]에 “ROS Workspace” 항목이 존재한다면 모든 설치가 정상적으로 완료된 것이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;delete-ros-2&quot;&gt;Delete ROS 2&lt;/h3&gt;

&lt;p&gt;만일 추후에 ROS 2를 삭제하고싶다면(재설치 등의 이유로), 아래의 명령어를 통해 삭제 가능하다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt remove ros-foxy-&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt autoremove
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;“000 로봇 운영체제 ROS 강좌 목차”, 네이버카페 오로카 : &lt;a href=&quot;https://cafe.naver.com/openrt&quot;&gt;https://cafe.naver.com/openrt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Linux-mint vs. Ubuntu : &lt;a href=&quot;https://itsfoss.com/linux-mint-vs-ubuntu/&quot;&gt;https://itsfoss.com/linux-mint-vs-ubuntu/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;BIOS에서 부팅 순서 변경 방법 : &lt;a href=&quot;https://steemit.com/kr/@nightofwin/4fsdnu&quot;&gt;https://steemit.com/kr/@nightofwin/4fsdnu&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 06 Oct 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//robotics-ros/ros2001/</link>
        <guid isPermaLink="true">https://dazory.github.io//robotics-ros/ros2001/</guid>
        
        <category>robotics</category>
        
        <category>ROS</category>
        
        <category>ROS2</category>
        
        
        <category>robotics-ros</category>
        
      </item>
    
      <item>
        <title>[Algorithms-Sanjoy Dasgupta-02] Ch3. Decompositions of graphs</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;교재 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        교재명: Algorithms&lt;br /&gt;
        Author: Sanjoy Dasgupta&lt;br /&gt;
        사이트: &lt;a href=&quot;https://github.com/eherbold/berkeleytextbooks&quot;&gt;https://github.com/eherbold/berkeleytextbooks&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;ch3-decompositions-of-graphs&quot;&gt;Ch3. Decompositions of graphs&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;31-why-graphs&quot;&gt;3.1. Why graphs?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Graph(그래프)&lt;/strong&gt;를 통해 다양한 문제를 명확하고 정밀하게 표현할 수 있다. 이번 장에서는 그래프의 기본 연결 구조를 설명하는 알고리즘 중 가장 기본적인 알고리즘에 대해 알아본다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Graph에 관한 기본적인 개념은 이전에 포스팅했던 “kmooc algorithm 시리즈”를 참고하길 바란다.&lt;/p&gt;

  &lt;p&gt;관련링크:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 예시&lt;/strong&gt; : Graph 자료구조의 사용 예시 - 지도의 구역 색칠하기 문제&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02 /image-20210910143211131.png&quot; alt=&quot;image-20210910143211131&quot; style=&quot;width:400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가령 Figure 3.1.(a)와 같이 지도의 구역을 색칠하는 문제는 Figure 3.1.(b)와 같이 graph형태로 표현될 수 있다. 이렇게 표현하면, 각 구역이 어떤 구역과 맞닿아있는지를 쉽게 파악할 수 있어서 색상이 겹치지 않게 색칠하는 것이 가능해진다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;개념&quot;&gt;개념&lt;/h3&gt;

\[G=(V,E)\]

&lt;p&gt;※ $G$: graph,  $V$: vertices,  $E$: edges ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 분류&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Graph는 크게 두 가지로 분류 가능하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;무향 그래프 (Undirected graph)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;※ vertex $x$와 $y$가 symmetric(대칭) 관계로 edge $e$에 의해 연결될 때, $e={x,y}$로 표현됨 ※&lt;/p&gt;

    &lt;p&gt;e.g., 위 “지도 색칠 문제”가 이에 해당&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;유향 그래프 (Directed graph)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;※ vertex $x$에서 $y$로 가는 directed edges $e$는 $e=(x,y)$로 표현됨 ※&lt;/p&gt;

    &lt;p&gt;e.g., World Wide Web: Internet에서 각 site 정보를 vertex에 담고 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;representations&quot;&gt;Representations&lt;/h3&gt;

&lt;p&gt;$n(=│V│)$개의 vertices $v_1, …, v_n$를 갖는 graph $G$를 표현하는 방식에는 크게 &lt;em&gt;Adjacency matrix&lt;/em&gt; 방식과 &lt;em&gt;Adjacency list&lt;/em&gt; 방식이 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Adjacency matrix&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02
/image-20210910145759418.png&quot; alt=&quot;image-20210910145759418&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;adjacency matrix&lt;/strong&gt; 형태 ($n×n$ array)로 표현될 수 있다.&lt;/p&gt;

\[a_{ij}
= 
\begin{cases}
	1 &amp;amp; \text{if there is an edge from $v_i$ to $v_j$}\\
	0 &amp;amp; \text{otherwise}
\end{cases}\]

&lt;p&gt;&lt;strong&gt;● 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;특정 edge가 constant time 내에 확인된다 : $Θ(1)$ (😍: 빠르군!)&lt;/li&gt;
  &lt;li&gt;$O(n^2)$의 메모리 공간을 요구한다. (🤔: edges 개수가 많지 않다면 공간 낭비가 심하군!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Adjacency list&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02
/image-20210910145903347.png&quot; alt=&quot;image-20210910145903347&quot; /&gt;&lt;/p&gt;

&lt;p&gt;각 vertex에 대해 $│V│$개의 &lt;strong&gt;linked lists&lt;/strong&gt; 형태로 표현되며, 각 vertex는 연결된 vertices의 정보를 담고있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;undirected graph의 경우 $2│E│$의, directed graph의 경우 $│E│$의 메모리 공간을 갖는다. : overall memory complexity = $O(│E│)$&lt;/li&gt;
  &lt;li&gt;특정 edge 탐색에는 $O(│V│)$의 시간 복잡도가 소요된다. (😅: adjacency matrix보단 느리지만 반복적인 탐색이 가능하므로 편리하군!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Adjacency matrix vs. Adjacency list&lt;/strong&gt;: 무엇을 사용할 것인가?&lt;/p&gt;

&lt;p&gt;graph를 표현하는 두 가지 기법 &lt;em&gt;adjacency matrix&lt;/em&gt;와 &lt;em&gt;adjacency list&lt;/em&gt; 은 어떤 특정 방법이 더 좋다고 말할 수 없다. 상황에 따라 특정 방법이 유리할 수도, 불리할 수도 있기 때문이다.&lt;/p&gt;

&lt;div class=&quot;callout&quot;&gt;
    graph에서 edges의 최소 개수는 V개이며, 최대 개수는 V²개이다.
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Case1. $│E│ \approx │V│^2$ : dense graph&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;edges의 개수가 최대값에 가까운 경우, graph를 “&lt;em&gt;dense&lt;/em&gt;“하다고 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Case2. $│E│ \approx │V│$ : sparse graph&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;edges의 개수가 최소값에 가까운 경우, graph를 “&lt;em&gt;sparse&lt;/em&gt;“하다고 한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;graph의 edges 개수 $│E│$는 graph에서 더 좋은 알고리즘을 선택하는 데에 중요한 요소이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 예시&lt;/strong&gt; : World Wide Web&lt;/p&gt;

&lt;p&gt;검색엔진은 약 80억개의 웹사이트를 갖고 있다. 이 경우 adjacency matrix보다 adjacency list로 구현하는 것이 더 효과적이다. adjacency matrix로 표현하는 경우 $\text{80억}×\text{80억}$개의 메모리가 필요한데, 80억 개의 웹사이트 규모에 비해 각 웹사이트를 연결하는 edges의 개수는 sparse하기 때문에 adjacency list로 구현하는 것이 메모리 낭비를 줄일 수 있는 방법이기 때문이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;32-depth-first-search-in-undirected-graphs&quot;&gt;3.2. Depth-first search in undirected graphs&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;321-exploring-mazes&quot;&gt;3.2.1. Exploring mazes&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02
/image-20210910152431498.png&quot; alt=&quot;image-20210910152431498&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림과 같이 미로에서 길찾는 문제는 길의 각 지점을 vertices로 삼아 graph로 표현함으로써 해결할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ node $v$에서 갈 수 있는 모든 nodes를 찾는 알고리즘&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;procedure explore(G, v): 
Input:	G=(V,E) is a graph; v∈V
Output:	visited(u) is set to true for all nodes u reachable from v

	visited(v) = true
	previsit(v)
	for each edge (v,u) ∈ E:
		if not visited(u):	explore(u)
	postvisit(v)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;※ 여기서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;previsit()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;postvisit()&lt;/code&gt;에 대해서는 3.2.4에서 자세히 다룬다. ※&lt;/p&gt;

&lt;p&gt;그래프로 표현한 미로 문제에서 node $A$에서 갈 수 있는 모든 nodes를 찾기위해 explore(G,A)를 수행한 결과는 다음과 같이 &lt;u&gt;tree edges&lt;/u&gt;로 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02
/image-20210910231631953.png&quot; alt=&quot;image-20210910231631953&quot; /&gt;&lt;/p&gt;

&lt;p&gt;※ tree는 cycle이 없는 connected graph이다. &lt;br /&gt;※ 여기서 점선은 이전에 방문한 nodes로 되돌아가는 &lt;u&gt;back edges&lt;/u&gt;를 의미한다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 알고리즘 동작&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;vertex $v$를 방문했다고 표시&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;previsit($v$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;vertex $v$와 연결된 edges 집합 $E$에 있는 각 edge에 대해 아래 동작을 반복:&lt;/p&gt;

    &lt;p&gt;3.1. edge $(v,u)$일때, vertex $u$를 방문하지 않았다면, vertex $u$에 대해 explore을 실행&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;postvisit($v$)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 알고리즘 증명&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;graphs 연구에서 종종 사용되는 능동적 유도 과정을 통해 위 알고리즘이 vertex $v$에서 갈 수 있는 모든 vertices를 탐색가능함을 증명해본다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02
/image-20210910231135433.png&quot; alt=&quot;image-20210910231135433&quot; /&gt;&lt;/p&gt;

&lt;p&gt;알고리즘의 타당성을 위해, explore($v$)를 통해 직접적으로 연결되지 않은 vertex $u$를 탐색할 수 있음을 증명하면 된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;explore($v$)는 $v$와 직접적으로 연결된 vertex $z$를 탐색할 수 있다.
이를 통해 explore($z$)를 재귀적으로 호출하게 된다.&lt;/li&gt;
  &lt;li&gt;따라서 vertex $v$와는 직접적으로 연결되지 않지만 vertex $v$의 neighbors와 직접 연결된 vertex $w$를 탐색할 수 있다.
이를 통해 explore($w$)를 재귀적으로 호출하게 된다.&lt;/li&gt;
  &lt;li&gt;이러한 행위를 반복함으로써 vertex $v$와 직접 연결되지는 않지만 도달할 수 있는 모든 vertices(e.g., $u$)를 탐색할 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;322-depth-first-search&quot;&gt;3.2.2. Depth-first search&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;depth-first search (DFS)&lt;/strong&gt; 알고리즘은 explore을 반복적으로 수행함으로써 전체 graph를 순회한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 알고리즘&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;procedure dfs(G):
	for all v ∈ V:
		visited(v) = false
	for all v ∈ V:
		if not visited(v):	explore(v)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 실행시간 분석&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;각 node에 대해 고정적으로 실행되는 $O(1)$의 작업(e.g., visited (혹은 pre/postvisit))&lt;/p&gt;

    &lt;p&gt;⇒ total complexity = $O(│V│)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;가보지 않은 곳으로 가기 위해 인접 edges를 탐색하기 위한 loop (in explore function)&lt;/p&gt;

    &lt;p&gt;⇒ total complexity = $O(│E│)$ (∵ 각 edge마다 2번씩 탐색되므로)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서 DFS의 overall running time complexity = $O(│V│ + │E│) $&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;323-connectivity-in-undirected-graphs&quot;&gt;3.2.3. Connectivity in undirected graphs&lt;/h3&gt;

&lt;p&gt;undirected graph의 Connectivity(연결성)에 대해 알아본다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Connectedg graph&lt;/strong&gt; : 어떤 node에서 다른 node로 가는 path가 언제나 존재하는 경우&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unconnected graph&lt;/strong&gt; : 어떤 node에서 다른 node로 가는 path가 존재하지 않을 수도 있는 경우&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;connected components&lt;/em&gt;(= subgraph)로 구성된다.&lt;/p&gt;

        &lt;p&gt;※ connected components는 분리된 connected regions을 의미한다. ※&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DFS 알고리즘은 각 node $v$에 정수 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ccnum[v]&lt;/code&gt;값을 할당함으로써 graph의 연결성을 검증하는 수단으로 사용될 수 있다. (즉, connected graph라면 ccnum[v]의 값이 $│V│-1$개일 것이다.)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;procedure previsit(v):
	ccnum[v] = cc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;※ cc의 초기값은 0이며, DFS가 explore을 호출할 때마다 하나씩 값이 증가된다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;324-previsit-and-postvisit-orderings&quot;&gt;3.2.4. Previsit and postvisit orderings&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;explore&lt;/em&gt; 알고리즘에서 수행되는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;previsit()&lt;/code&gt;과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;postvisit()&lt;/code&gt;의 동작에 대해서 자세히 살펴본다. DFS알고리즘은 undirected graph의 연결성뿐만 아니라 보다 다양한 문제에서 사용될 수 있는데, 이러한 다재다능성은 previsit과 postvisit에 의해 실현된다. 가령, 각 node에 대해 previsit과 postvisit은 다음의 두 중요한 events 시간을 기록한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;previsit&lt;/strong&gt; : the moment of first discovery (해당 node를 처음으로 발견된 순간)&lt;/p&gt;

    &lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;procedure previsit(v):
	pre[v] = clock
	clock = clock + 1
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;postvisit&lt;/strong&gt; : the moment of final departure (해당 node를 떠나는 순간)&lt;/p&gt;

    &lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;procedure postvisit(v):
	post[v] = clock
	clock = clock + 1
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;tree edges에 previsit과 postvisit에 의해 기록된 시간을 표시하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02
/image-20210910234710727.png&quot; alt=&quot;image-20210910234710727&quot; /&gt;&lt;/p&gt;

&lt;p&gt;※ 예를 들어, node $I$는 5번째 순간에 최초로 탐색되었으며, 8번째 순간에 $I$ 너머의 모든 vertices에 대해 탐색이 완료되어 explore()실행이 종료되었음을 알 수 있다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;어떤 nodes $u$와 $v$에 대해 두 개의 구간 [$\text{pre($u$)}$, $\text{post($u$)}$]와 [$\text{pre($v$)}$, $\text{post($v$)}$]는 서로 disjoint(분리되)거나 하나가 다른 하나를 포함하는 관계이다.&lt;/p&gt;

&lt;p&gt;※ ∵ DFS알고리즘은 stack구조를 이용하여 구현되는데, stack은 LIFO(Last-In First-Out)으로 동작한다. 따라서 [$\text{pre($u$)}$, $\text{post($u$)}$]는 본질적으로 vertex $u$가 stack에 있는 동안의 시간을 의미한다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;33-depth-first-search-in-directed-graphs&quot;&gt;3.3. Depth-first search in directed graphs&lt;/h2&gt;

&lt;p&gt;이전까지는 undirected graph에 대한 DFS에 대해 살펴보았다. 이번에는 edges가 방향을 갖는 directed graphs에 대해 DFS를 적용해본다.&lt;/p&gt;

&lt;h3 id=&quot;331-types-of-edges&quot;&gt;3.3.1. Types of edges&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;■ 배경지식&lt;/strong&gt;: tree에서 nodes간의 관계에 대한 terminology(용어)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02
/image-20210911000711232.png&quot; alt=&quot;image-20210911000711232&quot; /&gt;&lt;/p&gt;

&lt;p&gt;※ 위 이미지를 기준으로 각 terminology에 대해 설명한다. ※&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$A$: search tree의 &lt;strong&gt;&lt;em&gt;root&lt;/em&gt;&lt;/strong&gt;. 즉, 이 외의 모든 nodes가 &lt;em&gt;descendant(자손)&lt;/em&gt;이다.&lt;/li&gt;
  &lt;li&gt;$E$: &lt;strong&gt;&lt;em&gt;descendants&lt;/em&gt;&lt;/strong&gt;로 nodes $F, G, H$ 를 갖는다. 반대로, 이 세 nodes $F,G,H$에 대한 &lt;strong&gt;&lt;em&gt;ancestor&lt;/em&gt;&lt;/strong&gt;이다.&lt;/li&gt;
  &lt;li&gt;$C$: node $D$의 &lt;strong&gt;&lt;em&gt;parent(부모)&lt;/em&gt;&lt;/strong&gt;;   $D$: node $C$의 &lt;strong&gt;&lt;em&gt;child(자식)&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, 요약하자면 $v_{\text{parent}}→v_{\text{child}}$이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Types of edges&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;undirected graph에 대해 DFS를 적용할 때, edges를 다음과 같이 두 종류로 분류할 수 있었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;tree edges&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;nontree edges&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이와 달리 undirected graph에서는 edges를 다음과 같이 네 종류로 분류할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02
/image-20210911001103123.png&quot; alt=&quot;image-20210911001103123&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Tree edges&lt;/strong&gt; : DFS forest의 부분에 해당하는 edges  ($v_{\text{parent}}→v_{\text{child}}$)&lt;/p&gt;

    &lt;p&gt;※ solid한 실제 edges에 해당. 이 외의 모든 edge 종류들은 점선에 해당하는 가상의 edges이다. ※&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Forward edges&lt;/strong&gt; : DFS tree에서 어떤 한 node에서 &lt;em&gt;nonchild&lt;/em&gt; descendant로 향하는 edges ($v_{\text{ancestor}}→v_{\text{nonchild descendant}}$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Back edges&lt;/strong&gt; : DFS tree에서 ancestor를 향하는 edges ($v_{\text{descendant}}→v_{\text{ancestor}}$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cross edges&lt;/strong&gt; : descendant으로도 ancestor으로도 향하지 않는 edges. 즉, 이미 완벽하게 탐색 완료된(= already postvisited) node를 향하는 edges&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Ancestor와 descendant의 관계&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ancestor와 descendant의 관계는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pre&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;post&lt;/code&gt; 값을 통해 바로 읽을 수 있다. 가령, edge $(u,v)$의 edge 종류는 다음과 같이 판별 가능하다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Tree edges (or Forward edges)&lt;/strong&gt;: $u→v$&lt;/p&gt;

\[[u\ [v\ v]\ u]\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Back edges&lt;/strong&gt; : $v→u$&lt;/p&gt;

\[[v\ [u\ u]\ v]\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cross edges&lt;/strong&gt; :&lt;/p&gt;

\[[u\ u]\ [v\ v] \text{ or } [v\ v]\ [u\ u]\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;332-directed-acyclic-graphs&quot;&gt;3.3.2. Directed acyclic graphs&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;■ Acyclicity Test with DFS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;directed graph는 path의 종류에 따라 두 종류로 구분할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;cyclic&lt;/em&gt; graph&lt;/strong&gt;: circular(순환하는) path $v_0→v_1→…→v_k→v_0$를 갖는 graph&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;acyclic&lt;/em&gt; graph&lt;/strong&gt;: cycle이 없는 graph&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이때, 한 번의 DFS알고리즘을 통해 graph의 acyclicity를 확인할 수 있다.&lt;/p&gt;

\[\text{directed graph가 cycle을 갖는다.} ⇔ \text{DFS에서 back edge가 존재한다.}\]

&lt;p&gt;※ ∵ $(u,v)$가 back edge라면, search tree에 $v→u$ 경로와 함께 cycle을 구성할 수 있기 때문에 ※&lt;br /&gt;※ ∵ graph가 어떤 cycle $v_0→v_1→…→v_k→v_0$를 갖는다면, 이 cycle의 첫 번째 node $v_0$는 다른 모든 nodes를 descendants로 갖는데, 마지막 $v_k$는 $v_0$로 향하는 back edge를 갖는다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Directed Acyclic Graphs (DAGs)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Directed Acyclic Graphs (DAGs)&lt;/strong&gt;은 인과관계, 계층구조, 시간 종속성과 같은 관계를 모델링하기에 좋다. 어떤 task를 수행하기 전에 다른 task를 할 수 없다는 제약조건은 두 nodes간의 edge 방향으로 간편하게 표현되기 때문이다.&lt;/p&gt;

&lt;p&gt;※ 반면 그래프가 Directed Cyclic Graphs인 경우에는 이러한 순서가 의미가 없게 된다. ※&lt;/p&gt;

&lt;p&gt;작업의 순서는 &lt;em&gt;linearize&lt;/em&gt;(혹은 &lt;em&gt;topologically sort&lt;/em&gt;)를 통해 표현할 수 있으며, 이는 DFS로 구현 가능하다. 이때, linearize는 post numbers를 내림차순으로 나열함으로써 구현된다.&lt;/p&gt;

&lt;p&gt;※ DAGs는 back edges를 가질 수 없다는 점을 기억하자. ※&lt;br /&gt;※ DFS를 이용하여 dag의 nodes를 sort하는 데에는 linear time이 소요된다. (∵ linearize) ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● DAG의 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;dag&lt;/em&gt;에서 모든 edge는 작은 post number를 갖는 node로 향한다.&lt;/p&gt;

    &lt;p&gt;∵ linearized가 post numbers의 내림차순으로 구현되므로&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모든 &lt;em&gt;dag&lt;/em&gt;는 적어도 하나의 source와 적어도 하나의 sink를 갖는다.&lt;/p&gt;

    &lt;p&gt;∵ linearization은 ① source를 찾아 출력하고 graph에서 삭제 ② graph가 빌 때까지 이러한 과정을 반복하는 과정을 통해 구현될 수 있기 때문에&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;34-strongly-connected-components&quot;&gt;3.4. Strongly connected components&lt;/h2&gt;

&lt;h3 id=&quot;341-defining-connectivity-for-directed-graphs&quot;&gt;3.4.1. Defining connectivity for directed graphs&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;■ Connectivity in directed graphs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;undirected graphs에서 connectivity는 각각의 connected components에 대해 DFS를 수행함으로써 증명되었다. 반면, &lt;strong&gt;directed graphs에서 connectivity&lt;/strong&gt;는 다음과 같이 정의된다.&lt;/p&gt;

\[\text{path $u→v$와 path $v→u$가 모두 존재할 때, 두 nodes $u, v$는 connected이다.}\]

&lt;p&gt;이러한 정의를 통해 &lt;strong&gt;&lt;em&gt;strongly connected components&lt;/em&gt;&lt;/strong&gt;의 개념을 정의할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Strongly Connected Components&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02
/image-20210911005311792.png&quot; alt=&quot;image-20210911005311792&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 (a)는 directed graph를 보여주며, 이를 구성하는 &lt;em&gt;strongly connected components&lt;/em&gt;를 하나의 single meta-node로 표현한 결과(&lt;strong&gt;&lt;em&gt;meta-graph&lt;/em&gt;&lt;/strong&gt;)는 (b)이다. 이때 이 meta-graph는 dag임이 명확하다 (∵ nodes간에 cycle이 존재한다는 것은 strongly connected component임을 의미하므로 meta-node로 치환된다).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● directed graph의 특징&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;모든 directed graph는 strongly connected components로 구성된 하나의 dag로 표현될 수 있다. 이러한 특징은 directed graph가 두 가지 연결성 구조를 가지고 있음을 시사한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;상위 레벨에 존재하는 하나의 dag. (linearized 가능하다.)&lt;/li&gt;
  &lt;li&gt;하위 레벨에 존재하는 dag의 nodes. (더 자세히 조사할 수 있다.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;342-an-efficient-algorithm&quot;&gt;3.4.2. An efficient algorithm&lt;/h3&gt;

&lt;p&gt;Directed Graph를 strongly connected components로 분해하는 것은 여러 유용한 정보를 제공해준다.&lt;/p&gt;

&lt;p&gt;※ 운좋게도, 발전된 DFS를 통해 분해 과정에 linear time이 소요된다. ※&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 특징 1.&lt;/strong&gt; explore subroutine이 node $u$에서 시작할 때, 이 subroutine은 $u$에서 도달가능한 모든 nodes를 방문한 시점에 종료된다.&lt;/p&gt;

&lt;p&gt;이러한 특징으로 인해 &lt;em&gt;sink&lt;/em&gt; strongly connected component 내부의 어떤 node에서 explore를 수행하면 정확하게 해당 component만을 얻을 수 있다. &lt;br /&gt;⇒ sink strongly connected component를 알고있을 때, 하나의 strongly connected component를 찾는 방법&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 특징 2.&lt;/strong&gt; DFS를 통해 구한 &lt;em&gt;최대 post number&lt;/em&gt;에 해당하는 node는 &lt;em&gt;source&lt;/em&gt; strongly connected component에 속한다.&lt;/p&gt;

&lt;p&gt;이를 통해 graph의 source strongly connected components 내에 있는 node를 찾을 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 특징 3.&lt;/strong&gt; $C$와 $C’$가 strongly connected components일 때, $C$에 있는 어떤 node에서 $C’$에 있는 다른 node로 가는 edge가 존재한다면, $C$에서의 최대 post number는 $C’$에서의 최대 post number보다 크다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;증명:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;$C$가 $C’$보다 더 먼저 탐색된 경우, 특징1$^*$에 의해 도달가능한 모든 nodes를 방문한 뒤 (즉, $C’$가 모두 탐색된 후)에 $C$에 대한 탐색이 종료된다. 따라서 늦게 종료된 $C$의 post number가 $C’$의 post number보다 크다.&lt;/li&gt;
    &lt;li&gt;$C’$가 $C$보다 더 먼저 탐색된 경우, 특징1$^*$에 의해 $C’$가 이미 종료된 후에 $C$가 탐색된다.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;따라서 strongly connected components는 내부의 최대 post number를 내림차순으로 정렬하여 linearized할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;특징2를 활용하여 sink strongly connected component를 알 수 있다. $G^R$과 $G$는 같은 strongly connected components를 가지므로 특징2를 활용하여 $G^R$에 대해 source strongly connected components를 구하면 이는 $G$의 sink strongly connected component가 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02
/image-20210911012216827.png&quot; alt=&quot;image-20210911012216827&quot; /&gt;&lt;/p&gt;

&lt;p&gt;※ 이때 $G^R$은 graph $G$의 reverse graph이다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;따라서 DFS를 이용하여 directed graph를 strongly connected components로 분해하는 알고리즘은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;graph $G^R$에 대해 DFS를 수행한다.&lt;/li&gt;
  &lt;li&gt;step1 과정을 통해 얻은 post numbers의 내림차순에 해당하는 nodes 순서대로, grpah $G$에 대해 undirected connected components algorithm(from Section 3.2.2)를 수행한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;※ linear-time에 수행된다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-10-01-cs-algorithm-algorithmsSanjoyDasgupta02
/image-20210911012641728.png&quot; alt=&quot;image-20210911012641728&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 graph에 이 알고리즘을 적용하는 과정은 다음과 같다.:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;graph $G^R$에 대해 DFS를 수행하여 얻은 post numbers의 내림차순 결과는 다음과 같다.:&lt;/p&gt;

    &lt;p&gt;$G, I, J, L, K, H, D, C, F, B, E, A.$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;위 결과 순서대로 graph G에 대해 undirected connected components algorithm 수행 결과는 다음과 같다.:&lt;/p&gt;

    &lt;p&gt;${G,H,I,J,K,L}, {D}, {C,F}, {B,E}, {A}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;eherbold/berkeleytextbooks, github: &lt;a href=&quot;https://github.com/eherbold/berkeleytextbooks&quot;&gt;https://github.com/eherbold/berkeleytextbooks&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 01 Oct 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algorithmsSanjoyDasgupta02/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algorithmsSanjoyDasgupta02/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>Algorithms</category>
        
        <category>Sanjoy Dasgupta</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[Algorithms-Sanjoy Dasgupta-01] Ch2. Divide-and-conquer algorithms</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;교재 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        교재명: Algorithms&lt;br /&gt;
        Author: Sanjoy Dasgupta&lt;br /&gt;
        사이트: &lt;a href=&quot;https://github.com/eherbold/berkeleytextbooks&quot;&gt;https://github.com/eherbold/berkeleytextbooks&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;ch2-divide-and-conquer-algorithms&quot;&gt;Ch2. Divide-and-conquer algorithms&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;■ Divide-and-conquer:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;problem을 sub-problems으로 나눈다.&lt;/li&gt;
  &lt;li&gt;재귀적으로 sub-problems을 해결한다.&lt;/li&gt;
  &lt;li&gt;answers를 모두 combining한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;21-multiplication&quot;&gt;2.1. Multiplication&lt;/h2&gt;

&lt;p&gt;Divide-and-conquer 접근 방법이 어떠한 이점을 제공하는지 몇 가지 예시를 통해 살펴본다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시&lt;/strong&gt;  📢이어서 더 자세히 다루도록 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multiplication&lt;/li&gt;
  &lt;li&gt;Search
    &lt;ul&gt;
      &lt;li&gt;Binary Search&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sort
    &lt;ul&gt;
      &lt;li&gt;Merge Sort&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-product-of-two-complex-numbers-aka-gausss-trick&quot;&gt;The product of two complex numbers (a.k.a. Gauss’s Trick)&lt;/h3&gt;

\[(a+bi)(c+di) = ac -bd +(bc+ad)i\]

&lt;p&gt;위 방정식은 4개의 곱($ac$, $bd$, $bc$, $ad$)으로 구성되어있지만, divide-and-conquer 방식으로 접근했을 때 아래와 같이 3개의 곱($ac$, $bd$, $(a+b)(c+d)$)으로 곱셈연산 횟수를 줄일 수 있다.&lt;/p&gt;

\[(a+bi)(c+di) = ac -bd +\{ (a+b)(c+d) -ac -bd\}i\\
\text{(∵ (bc+ad) = (a+b)(c+d) -ac -bd )}\]

&lt;p&gt;big-O 관점에서 위와 같은 곱셈 연산 횟수 감소는 독창성 낭비라고 보여지지만, &lt;u&gt;재귀적인 관점&lt;/u&gt;에서 불필요한 곱셈의 수를 줄이는 것은 매우 중요하다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;regular-multiplication-정규곱셈&quot;&gt;Regular multiplication (정규곱셈)&lt;/h3&gt;

&lt;p&gt;$x$와 $y$가 두 개의 $n$-bit 정수이며 이때 $n$은 2의 제곱수라고 가정하자. divide-and-conquer하게 $x$와 $y$의 곱을 표현하고자 각각을 sub-problem으로 쪼갠 결과는 아래와 같다.&lt;/p&gt;

\[x = 2^{n/2}·x_L + x_R\\
y = 2^{n/2}·y_L + y_R\]

&lt;p&gt;이를 이용하여 $x$와 $y$의 곱 $xy$를 표현하면 다음과 같다.&lt;/p&gt;

\[xy = (2^{n/2}·x_L+x_R)(2^{n/2}·y_L+y_R)\\
= 2^{n}·x_Ly_L + 2^{n/2}(x_Ly_R +x_Ry_L) + x_Ry_R\]

&lt;p&gt;여기서 $x_Ly_L$, $x_Ly_R$, $x_Ry_L$, $x_Ry_R$은 재귀적으로 다시 호출될 수 있다. (아래 수도 알고리즘 참고)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;A divide-and-conquer algorithm for integer multiplication.

function multiply(x; y)
    Input: Positive integers x and y, in binary
    Output: Their product
    
    n = max(size of x, size of y)
    if n = 1: return xy
    
    xL, xR = leftmost [n/2], rightmost [n/2] bits of x
    yL, yR = leftmost [n/2], rightmost [n/2] bits of y
    P1 = multiply(xL; yL)
    P2 = multiply(xR; yR)
    P3 = multiply(xL + xR; yL + yR)

	return P1×2n + (P3-P1-P2)×2^{n/2} + P2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;따라서 $n$-bit inputs에 대한 전반적인 running time $T(n)$은 다음과 같다.&lt;/p&gt;

\[T(n) = 4T(n/2) + O(n)\\
∴ \text{complexity} = O(n^2)\]

&lt;p&gt;running time = $O(n^2)$은 전통적인 접근방법($O(n^2)$)과 비교했을 때 나아진 점이 없다. 따라서 앞서 배운 Gauss’s trick(두 개의 복소수 곱)을 $xy$에 적용하면, 세 개의 곱($x_Ly_L$, $x_Ry_R$, $(x_L+x_R)(y_L+y_R)$)으로 표현될 수 있다.&lt;/p&gt;

&lt;div class=&quot;callout&quot;&gt;
    &lt;b&gt;[참고] Gauss's trick&lt;/b&gt;&lt;br /&gt;
    (a+bi)(c+di)는 세 개의 곱(ac, bd, (a+b)(c+d))으로 표현된다.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;따라서 다음과 같이 running time을 줄일 수 있다.&lt;/p&gt;

\[T(n) = 3T(n/2) + O(n)\\
\text{∴ complexity = $O(n^{1.59})$}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;22-recurrence-relations&quot;&gt;2.2. Recurrence relations&lt;/h2&gt;

&lt;p&gt;Divide-and-conquer 알고리즘은 일반적인 패턴을 갖는다. 이를 공식화해보자.&lt;/p&gt;

&lt;h3 id=&quot;복잡도-공식화&quot;&gt;복잡도 공식화&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;가정&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\text{size}=n$인 problem을&lt;/li&gt;
  &lt;li&gt;$\text{size}=n/b$인 $a$개의 subproblems으로 재귀적으로 해결한다.&lt;/li&gt;
  &lt;li&gt;이때 추가적으로 발생하는 $\text{time}=O(n^d)$&lt;/li&gt;
&lt;/ul&gt;

\[T(n) = a·T(n/b)+O(n^d)\]

&lt;p&gt;running time이 위와 같을 때 아래와 같이 overall running time을 계산할 수 있다.&lt;/p&gt;

\[T(n) 
= a·T(\frac{n}{b})+O(n^d)\\
= a^2·T(\frac{n}{b^2})+\{ a·O({(\frac{n}{b})}^d) + O(n^d) \}\\
= a^3·T(\frac{n}{b^3})+\{ a^2·O({(\frac{n}{b^2})}^d) + a·O({(\frac{n}{b})}^d) + O(n^d) \}\\
...\\
= a^k·T(\frac{n}{b^k})+{\sum_{i=0}^{k-1}\{a^i·O((\frac{n}{b^i})^d)\}}\]

&lt;p&gt;이때, $b^k=n$ 즉, $k=\log_b(n)$일 때 위 방정식을 $T(1)$에 관해 표현할 수 있다.&lt;/p&gt;

\[T(n)
= a^{\log_b(n)}·T(\frac{n}{b^{\log_b(n)}})
	+{\sum_{i=0}^{\log_b(n)-1}\{a^i·O((\frac{n}{b^i})^d)\}} \\
= a^{\log_b(n)}·T(1)
	+{(\sum_{i=0}^{\log_b(n)-1}(\frac{a}{b^d})^i)·O(n^d)} \\
= a^{\log_b(n)}·T(1)
	+{(\frac{ 1·( \frac{a}{b^d}^{\log_b(n)}-1 ) } { \frac{a}{b^d}-1 })·O(n^d)} \\
= n^{\log_b(a)}·T(1)
	+{(\frac{ 1·( n^{(\log_b(a)-d)}-1 ) } { \frac{a}{b^d}-1 })·O(n^d)} \text{(∵ 로그의 성질)}\]

&lt;p&gt;이때, 우항의 두 번째 term은 다음과 같이 쓸 수 있다.&lt;/p&gt;

\[{(\frac{ 1·( n^{(\log_b(a)-d)}-1 ) } { \frac{a}{b^d}-1 })·O(n^d)}\\
= 
\begin{cases}
	O({ n^{(\log_b(a)-d)} }) · O(n^d) &amp;amp; \text{if $\log_b(a) &amp;gt; d$} \\
	\log_b(n) · O(n^d) &amp;amp; \text{if $\log_b(a) = d$} \\
	O(c) · O(n^d) &amp;amp; \text{if $\log_b(a) &amp;lt; d$} \\
\end{cases}\\
= 
\begin{cases}
	O( n^{(\log_b(a))} ) &amp;amp; \text{if $\log_b(a) &amp;gt; d$} \\
	O(n^d\log(n)) &amp;amp; \text{if $\log_b(a) = d$} \\
	O(n^d) &amp;amp; \text{if $\log_b(a) &amp;lt; d$} \\
\end{cases}\\\]

&lt;p&gt;따라서 최종적인 overall running time은 다음과 같다.&lt;/p&gt;

\[T(n)
= 
\begin{cases}
	O( n^{\log_b(a)} + n^{\log_b(a)} ) &amp;amp; \text{if $\log_b(a) &amp;gt; d$} \\
	O( n^{\log_b(a)} + n^d\log(n)) &amp;amp; \text{if $\log_b(a) = d$} \\
	O( n^{\log_b(a)} + n^d) &amp;amp; \text{if $\log_b(a) &amp;lt; d$} \\
\end{cases}\\
= 
\begin{cases}
	O( n^{\log_b(a)} ) &amp;amp; \text{if $\log_b(a) &amp;gt; d$} \\
	O( n^d\log(n)) &amp;amp; \text{if $\log_b(a) = d$} \\
	O( n^d) &amp;amp; \text{if $\log_b(a) &amp;lt; d$} \\
\end{cases}\\\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;재귀적 호출을 tree구조로 해석하면 다음과 같이도 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/Untitled/image-20210907182424920.png&quot; alt=&quot;image-20210907182424920&quot; style=&quot;width:800px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;overall running time $T(n)$을 depth=$k$일 때의 time $T(\frac{n}{b^k})$로 나타내면 다음과 같이 쓸 수 있다.&lt;br /&gt;(해석하자면, $T(\frac{n}{b^k})$짜리 nodes가 $a^k$만큼 있고, 여기에 초기 $O(n^d)$ complexity를 더한다.)&lt;/p&gt;

\[T(n) = a^k × T(\frac{n}{b^k}) + O(n^d)\]

&lt;p&gt;(예시를 통해 재검토)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$T(n) = 4T(n/2) + O(n)$일 때, (즉 $a=4, b=2, d=1$)&lt;/p&gt;

\[T(n) = O( n^{\log_2(4)} ) = O(n^2) \ \ \text{(∵ $\log_2(4)&amp;gt;1$이므로 )}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$T(n) = 3T(n/2) + O(n)$일 때, (즉 $a=3, b=2, d=1$)&lt;/p&gt;

\[T(n) = O( n^{\log_2(3)} ) = O(n^{1.59}) \ \ \text{(∵ $\log_2(3)&amp;gt;1$이므로 )}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;binary-search&quot;&gt;Binary Search&lt;/h3&gt;

&lt;p&gt;divide-and-conquer 알고리즘은 궁극적으로 binary search라고 해석할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시&lt;/strong&gt;: 정렬된 keys $z[0, 1, …, n-1]$를 포함하는 하나의 큰 파일에서 key값 $k$를 찾는 문제&lt;/p&gt;

&lt;p&gt;Binary Search를 통해 이러한 문제를 해결하는 과정은 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 수도 알고리즘&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;Binary_Search(z, 0, n-1, k):
	if(k==z[n/2]) { result=z[n/2]; }
	else if(k&amp;lt;z[n/2]) { Binary_Search(z, 0, n/2-1, k); }
	else if(k&amp;gt;z[n/2]) { Binary_Search(z, n/2, n-1, k); }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 복잡도&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;overall running time $T(n)$은 다음과 같다.&lt;/p&gt;

\[T(n) 
= T([n/2]) + O(1) &amp;amp; \text{즉, $a=1, b=2, d=0$}\\
= O(n^d\log(n)) = O(\log(n)) &amp;amp; \text{(∵ $\log_2(1) = 0$)}\]

&lt;div class=&quot;callout&quot;&gt;
    &lt;b&gt;내 생각&lt;/b&gt;&lt;br /&gt;
    결국, Binary Search는 주어진 elements를 2개로 나누며, 각 search에 O(1)의 시간이 걸리는것이다.&lt;br /&gt;
    이를 divide-and-conquer로 확장시켜 생각한다면 주어진 elements를 몇 개로 쪼갤지(a), 이렇게 쪼개면 각 단위의 elements의 size는 어떻게 되는지(b), 또한 각 단위를 처리하는데에 얼만큼의 시간이 소요되는지(d)와 관련시킬 수 있을 것 같다.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;23-mergesort&quot;&gt;2.3. Mergesort&lt;/h2&gt;

&lt;p&gt;Divide-and-conquer 알고리즘은 숫자들의 list를 정렬하는 문제에 쉽게 적용 가능하다. 가령 merge sort는 list를 절반씩 두 개로 쪼개고, 재귀적으로 각 절반씩에 대해 정렬을 하며, 그 결과로 반환된 sorted sublists를 merge(병합)한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;알고리즘&quot;&gt;알고리즘&lt;/h3&gt;

&lt;p&gt;mergesort의 알고리즘은 다음과 같다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;function mergesort(a[1...n]):
	* Input:	An array of numbers a[1...n]
	* Output:	A sorted version of this array

	if n&amp;gt;1:
		return merge( mergesort(a[1..⌊n/2⌋)), mergesort(a[⌊n/2⌋+1...n]))
	else:
		return a
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;input list의 element 개수가 2 이상이면, 절반씩 mergesort()한 뒤에 merge() 결과를 반환&lt;/li&gt;
  &lt;li&gt;input list의 element 개수가 1 이하이면, input list를 그대로 반환&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이때 merge 함수는 다음과 같다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;function merge(x[1...k], y[1...l]):
	if k=0:	return y[1...l]
	if l=0:	return x[1...k]
	if x[1]≤y[1]:
		return x[1] &amp;amp; merge( x[2...k], y[1...l] )
	else:
		return y[1] &amp;amp; merge( x[1...k], y[2...l] )
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;두 input list 중 비어있는 list가 있다면, 비어있지 않은 다른 input list를 반환&lt;/li&gt;
  &lt;li&gt;두 input list가 모두 비어있지 않다면,
    &lt;ul&gt;
      &lt;li&gt;두 input list 각각의 최소값(첫 번째 인덱스값)을 비교하여 적절히 concatenate and merge&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;※ 이때 operator &amp;amp;는 concatenation을 의미한다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;복잡도&quot;&gt;복잡도&lt;/h3&gt;

&lt;p&gt;mergesort의 overall running time $T(n)$은, mergesort의 재귀호출 시간 $2T(n/2)$와 merge의 시간복잡도 $O(n)$의 합으로 나타낼 수 있다.&lt;/p&gt;

&lt;p&gt;※ 길이가 각각 $k$, $l$인 두 lists를 합치는 merge 알고리즘의 running time은 $O(k+l)=O(n)$. ※&lt;/p&gt;

\[T(n) = 2T(n/2) + O(n) \\
= O(n^d\log(n)) \ \ \  \text{(∵$\log_2(2)==1$)} \\
= O(n\log(n)) \ \ \  \text{(∵ $a=2, b=2, c=1$)}\]

&lt;p&gt;즉, $T(n) = O(n\log(n))$.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;iteractive-mergesort&quot;&gt;iteractive mergesort&lt;/h3&gt;

&lt;p&gt;위에서 언급한 알고리즘을 그림으로 도식화한 결과는 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-29-cs-algorithm-algorithmsSanjoyDasgupta/image-20210908155859778.png&quot; alt=&quot;image-20210908155859778&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실상 모든 과정이 merging만으로 수행됨을 확인할 수 있다. 이러한 관점에서 봤을 때 mergesort는 재귀적인 방법(recursive)이 아닌 반복적인 방법(iteractive)을 이용해볼 수 있다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;function iterative-mergesort( a[1...n] )
	* Input:	elements a1, a2, ..., an to be sorted
	
	Q = [] (empty queue)
	for i=1 to n:
		inject( Q, [ai] )
		while │Q│&amp;gt;1:
			inject( Q, merge( eject(Q), eject(Q) ) )
	return eject(Q)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;비어있는 queue Q를 생성&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;주어진 input list a[1…n]의 element a1부터 an까지에 대해 아래를 수행:&lt;/p&gt;

    &lt;p&gt;Q의 elements 개수가 2개 이상이면 아래를 반복:&lt;/p&gt;

    &lt;p&gt;2.1. Q를 두 번 eject하여 두 elements를 꺼내고 merge를 수행&lt;/p&gt;

    &lt;p&gt;2.2. merge의 결과를 Q에 inject ※ 이때 두 elements가 merge되어 하나의 element가 된다. ※&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Q의 element를 반환&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;※ inject: queue의 마지막에 새로운 원소 추가 (enqueue) ※&lt;br /&gt;※ eject: queue의 시작 원소를 제거하고 반환 (dequeue) ※&lt;br /&gt;※ queue의 구조: FIFO(First-In-First-Out) ※&lt;/p&gt;

&lt;div class=&quot;callout&quot;&gt;
    &lt;b&gt;재귀(recursive)와 반복(iteractive)&lt;/b&gt;&lt;br /&gt;
    &lt;b&gt;재귀&lt;/b&gt;는 적은 코드로 알고리즘을 작성할 수 있기 때문에 코드를 이해하고 유지하는 것이 중요한 경우에 주로 사용된다. function call에 따른 메모리 비용 및 CPU 사용 시간이 더 소요된다는 단점이 있다.&lt;br /&gt;
    &lt;b&gt;반복&lt;/b&gt;은 function call에 따른 메모리 비용이 없기 때문에 (즉, method 호출을 위한 메모리는 한 번만 필요하기 때문에) 성능 측면에서 더 유리하다.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 알고리즘을 도식화 하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-29-cs-algorithm-algorithmsSanjoyDasgupta/image-20210908165655430.png&quot; alt=&quot;image-20210908165655430&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;an-nlogn-lower-bound-for-sorting&quot;&gt;An $n\log(n)$ lower bound for sorting&lt;/h3&gt;

&lt;p&gt;Sorting algorithms은 trees로 묘사될 수 있다. 가령 3개의 elements $a_1, a_2, a_3$를 갖는 하나의 array의 정렬 과정은 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-29-cs-algorithm-algorithmsSanjoyDasgupta/image-20210908170206501.png&quot; alt=&quot;image-20210908170206501&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이때 tree의 depth는 알고리즘의 the worst-case time complexity를 의미한다. 따라서 tree구조로 나타내면 알고리즘이 얼마나 최적화되었는지를 보여줄 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ $n$-elements sorting algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;가령, $n$개의 elements에 대해 정렬알고리즘을 사용할 때 어떠한 비교 sequence에 의해 정렬된 결과 (e.g., ${a_1, a_2, …, a_n}$)는 tree의 leaves에 저장된다. $\text{depth}=d$일 때 nodes의 개수는 $2^d$이므로, leaves의 총 개수는 $2^h$(h=height)이다. 한편, 가능한 모든 정렬의 개수는 $n!$이므로 다음과 같이 height를 표현할 수 있다.&lt;/p&gt;

\[h 
= \log(n!) \ \ \  \text{(∵ $2^h=n!$)} \\
≥ c·n\log(n) \ \ \  \text{for all c≥0 (∵ $n!≥(\frac{n}{2})^{\frac{n}{2}})$}\\
∴ h=Ω(n\log(n))\]

&lt;p&gt;위 결과에 의해 $n$개의 elements를 정렬하는 알고리즘의 worst-case는 $Ω(n\log(n))$에 비교되어야 함을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 예시&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;mergesort의 overall time complexity는 $O(n\log(n))$으로, 위에서 구한 worst-case time complexity $Ω(n\log(n))$과 일치한다. 즉, 적어도 정렬알고리즘의 worst-case time complexity는 $Ω(n\log(n))$보다 빠를 수 없는데, mergesort가 이를 충족한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;24-medians&quot;&gt;2.4. Medians&lt;/h2&gt;

&lt;h3 id=&quot;get-medians-with-sorting&quot;&gt;Get Medians with sorting&lt;/h3&gt;

&lt;p&gt;중앙값을 찾는 가장 간단한 방법은 단순히 sorting하여 중앙값을 반환하는 것이다. 하지만 이러한 방식은 단점이 명확하다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;too slow : $O(n\log(n))$의 time complexity가 소요된다.&lt;/li&gt;
  &lt;li&gt;too much : 중앙값을 찾기 위해 중앙값이 아닌 모든 요소를 정렬해야한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;get-medians-with-selection&quot;&gt;Get Medians with Selection&lt;/h3&gt;

&lt;p&gt;selection을 통해 sorting보다 더 효율적으로 median값을 찾을 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Selection&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;SELECTION
	Input:	A list of numbers S; an integer k
	Output: The kth smallest element of S
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;※ 이때 $k=1$은 $S$에서 최소값을 의미하며, $k=⌊│S│/2⌋$는 $S$에서 median을 의미한다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ A randomized divide-and-conquer algorithm for selection&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;selection을 위해 divide-and-conquer 접근 방법을 사용해볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 예시&lt;/strong&gt;: 어떤 값 $v$에 대해 list $S$를 세 개의 분류로 쪼개는 방법&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-29-cs-algorithm-algorithmsSanjoyDasgupta/image-20210908184709360.png&quot; alt=&quot;image-20210908184709360&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$v=5$일때 주어진 list $S$는 다음과 같이 쪼개질 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-29-cs-algorithm-algorithmsSanjoyDasgupta/image-20210908184836478.png&quot; alt=&quot;image-20210908184836478&quot; /&gt;&lt;/p&gt;

&lt;p&gt;※ $S_L, S_v, S_R$ : 각각 $S$보다 작은값들, 같은 값들, 큰 값들의 집합 ※&lt;/p&gt;

&lt;p&gt;이때 median값을 search하는 과정은 이러한 sublists 중 하나로 범위를 좁혀내려가며 얻어질 수 있다. 가령, $S$ 중 8번째로 작은 값은 $S_R$에서 세 번째로 작은 값임을 알 수 있다 ($S_L$과 $S_v$의 elements 개수가 5개이므로 8번째로 작은 값은 $S_R$에 속할 수 밖에 없다).&lt;/p&gt;

&lt;p&gt;이를 일반화하면 다음과 같다.&lt;/p&gt;

\[\text{selection}(S,k)
=
\begin{cases}
	\text{selection}(S_L, k) &amp;amp; \text{if $k≤ │S_L│$} \\
	v &amp;amp; \text{if $│S_L│&amp;lt;k≤│S_L│+│S_v│$} \\
	\text{selection}(S_R, k-│S_L│-│S_v│) &amp;amp; \text{if $k&amp;gt;│S_L│+│S_v│$}
\end{cases}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 복잡도&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이때 세 개의 sublists는 linear time안에 계산된다 (더욱이, 계산은 in-place로 동작하며 추가적인 memory allocating이 필요없다👍). 따라서 적절하게 sublist를 선택한다면 계산해야하는 elements의 개수를 $│S│$에서 적어도 $\max{(│S_L│, │S_R│)}$만큼은 감소시킬 수 있다.&lt;/p&gt;

&lt;p&gt;이상적인 상황($│S_L│, │S_R│ ≒ \frac{1}{2}│S│$)을 가정하면, selection의 overall running time은 다음과 같다.&lt;/p&gt;

\[T(n) = T(n/2)+O(n)\\
∴ T(n) = O(n)\]

&lt;p&gt;※ 단, 위 복잡도는 이상적인 상황을 가정하였음에 유의해야한다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● $v$에 따른 복잡도&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;알고리즘의 복잡도는 결국 $v$값에 따라 달라진다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Best-case: $│S_L│, │S_R│ ≒ \frac{1}{2}│S│$&lt;/p&gt;

\[T(n) = T(n/2) + O(n)\\
∴ T(n) = O(n)\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Worst-case: $v$가 매번 최대값 혹은 최소값인 경우&lt;/p&gt;

\[T(n) = \sum_{i=\frac{n}{2}}^{n}i = O(n^2)\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Average-case:&lt;/p&gt;

    &lt;p&gt;$S(25\%) ≤ v ≤ S(75\%)$라면( $S$의 50% 부분에 해당), $│S_L│, │S_R│ ≤ \frac{3}{4}│S│$가 된다.&lt;/p&gt;

    &lt;p&gt;이때 &lt;em&gt;Lemma&lt;/em&gt;에 의해  2번 split operation을 수행하면 평균적으로 전체 risk가 최대 $\frac{3}{4}$ 크기로 감소한다.&lt;/p&gt;

    &lt;div class=&quot;callout&quot;&gt;
    &lt;b&gt;Lemma란?&lt;/b&gt;&lt;br /&gt;
    On average a fair coin needs to be tossed two times before a &quot;heads&quot; is seen.&lt;br /&gt;
	즉, 동전이 앞면이기 위해서는 평균적으로 2번 던져야 한다.
&lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;따라서 Average-case의 time complexity는 다음과 같다.&lt;/p&gt;

\[T(n) ≤ T(\frac{3}{4}n) + O(2n)\\
  ∴T(n) = O(n)\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-unix-sort-command&quot;&gt;The Unix sort command&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;■ Mergesort vs. Median-finding&lt;/strong&gt; : 서로 다른 성질&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mergesort : 일단 생각없이 쪼개고, 이후에 열심히 정렬하면서 합침 (the most convenient way)&lt;/li&gt;
  &lt;li&gt;median-finding : 처음부터 신중하게 쪼개어 나감&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Quicksort ≈ Median-finding&lt;/strong&gt; :&lt;/p&gt;

&lt;p&gt;Quicksort는 median-finding 알고리즘의 방식과 유사하게 동작한다. median algorithm의 $v$ 대신 $\text{pivot}$이라는 변수를 도입하여, $\text{pivot}$을 기준으로 list를 쪼갠다. 이후 sublists 각각에 대해 정렬을 반복하는 과정을 거친다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● Time Complexity&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Quicksort의 time complexity는 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Worst-case : $Θ(n^2)$ (median-finding과 동일)&lt;/li&gt;
  &lt;li&gt;Average-case : $O(n\log(n))$ (≠median-finding $O(n^2)$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quicksort는 sorting algorithms 중에서 가장 빠른 알고리즘이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;25-matrix-multiplication&quot;&gt;2.5. Matrix Multiplication&lt;/h2&gt;

&lt;p&gt;두 개의 $n×n$ matrices $X$와 $Y$의 곱은 $n×n$ matrix $Z=XY$로 표현된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-29-cs-algorithm-algorithmsSanjoyDasgupta/image-20210908221921228.png&quot; alt=&quot;image-20210908221921228&quot; /&gt;&lt;/p&gt;

\[Z_{ij} = \sum_{k=1}^n{X_{ik}Y_{kj}}\]

&lt;p&gt;※ 일반적으로 $XY≠YX$이다. 즉, matrix multiplication은 commutative하지 않다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 배경&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;꽤 오랜시간동안 matrix multiplication은 $O(n^3)$의 복잡도를 갖는다고 여겨졌다 (∵ $n^2$개의 entries가 계산되고 각 계산은 $O(n)$이 소요되기 때문). 하지만 독일 수학자 Volker Strassen에 의해 divide-and-conquer 알고리즘을 적용하면 더 효율적인 알고리즘으로 만들 수 있다는 것이 증명되었다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;volker-strassens-idea&quot;&gt;Volker Strassen’s Idea&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;■ 이전까지의 생각&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$X$와 $Y$를 각각 4개의 $\frac{n}{2}×\frac{n}{2}$ blocks으로 쪼개면 다음과 같이 표현 가능하다.&lt;/p&gt;

\[X=
\begin{bmatrix}
	A &amp;amp; B\\
	C &amp;amp; D
\end{bmatrix}
,
Y=
\begin{bmatrix}
	E &amp;amp; F\\
	G &amp;amp; H
\end{bmatrix}\\

∴ XY = 
\begin{bmatrix}
	AE+BG &amp;amp; AF+BH\\
	CE+DG &amp;amp; CF+DH
\end{bmatrix}\]

&lt;p&gt;이때의 시간 복잡도는 다음과 같다.&lt;/p&gt;

\[T(n) = 8T(\frac{n}{2}) + O(n^2)\\
∴ T(n) = O(n^3)\\

\text{(∵ $\frac{n}{2}$의 크기를 갖는 8개의 submatrix 곱과 $O(n^2)$ 복잡도를 갖는 덧셈연산)}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Volker Strassen의 생각&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Volker는 $n$ 크기 matrix를 7개의 subproblems으로 쪼갬으로써 복잡도를 줄였다.&lt;/p&gt;

\[T(n) = 7T(\frac{n}{2}) + O(n^2)\\
∴ T(n) = O(n^{\log_2{7}}) ≈ O(n^{2.81})\]

&lt;p&gt;▶ 자세히: 7개의 subproblems으로 쪼개는 방법&lt;/p&gt;

\[XY = 
\begin{bmatrix}
	P_5 + P_4 - P_2 + P_6 &amp;amp; P_1 + P_2\\
	P_3 + P_4 &amp;amp; P_1 + P_5 - P_3 - P_7
\end{bmatrix}\]

&lt;p&gt;이때 $P_1 = A(F-H), P_2 = (A+B)H, P_3 = (C+D)E, P_4 = D(G-E)$, $P_5 = (A+D)(E+H), P_6 = (B-D)(G+H), P_7 = (A-C)(E+F)$ .&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;26-the-fast-fourier-transform&quot;&gt;2.6. The fast Fourier transform&lt;/h2&gt;

&lt;p&gt;지금까지 integers와 matrices에 대해 divide-and-conquer 알고리즘이 얼마나 유익한지를 살펴보았다. 이번에는 polynomials에 대해 divide-and-conquer 알고리즘을 적용해보자.&lt;/p&gt;

&lt;h3 id=&quot;polynomials-with-divide-and-conquer&quot;&gt;Polynomials with Divide-and-conquer&lt;/h3&gt;

&lt;p&gt;두 degree-$d$ polynomials $A(x), B(x)$의 곱은 degree-$2d$ polynomial $C(x)$로 표현된다.&lt;/p&gt;

\[A(x) = a_0 + a_1x + ... + a_dx^d\\
B(x) = b_0 + b_1x + ... + b_dx^d\\
C(x)=A(x)·B(x) = c_0 + c_1x + ... + C_{2d}x^{2d}\\
\text{이때 $c_k = a_0b_k + a_1b_{k-1} + ... + a_kb_0 = \sum_{i=0}^k{a_ib_{k-i}}$}\]

&lt;p&gt;※ for $i&amp;gt;d$, take $a_i$ and $b_i$ to be zero. ※&lt;/p&gt;

&lt;p&gt;이때, $c_k$를 계산하기 위해서는 $O(k)$단계가 소요되므로 $2d+1$개의 coefficients($c_0, c_1, … c_{2d}$)를 계산하기 위해서는 $Θ(d^2)$의 시간이 소요된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fast-fourier-transform&quot;&gt;Fast Fourier Transform&lt;/h3&gt;

&lt;p&gt;Fast fourier transform을 이용하면 더 빠른 계산을 수행할 수 있다. 자세한 과정은 생략한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;exercise&quot;&gt;Exercise&lt;/h2&gt;

&lt;p&gt;(추후 추가 예정)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;eherbold/berkeleytextbooks, github: &lt;a href=&quot;https://github.com/eherbold/berkeleytextbooks&quot;&gt;https://github.com/eherbold/berkeleytextbooks&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 29 Sep 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algorithmsSanjoyDasgupta/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algorithmsSanjoyDasgupta/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>Algorithms</category>
        
        <category>Sanjoy Dasgupta</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-15] Lect15. 동적계획법과 분할정복법의 차이 및 예제</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
이번 시간에는 &lt;hl&gt;동적계획법과 분할정복법의 차이&lt;/hl&gt;에 대해 이해하고, 예제 문제를 해결해본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;동적계획법의-방법론-이해&quot;&gt;동적계획법의 방법론 이해&lt;/h1&gt;

&lt;p&gt;&lt;hl&gt;동적계획법이 필요한 문제의 특성&lt;/hl&gt; 및 &lt;hl&gt;분할정복법과의 차이점&lt;/hl&gt;에 대해 학습한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;fibonacci-numbers&quot;&gt;Fibonacci Numbers&lt;/h2&gt;

&lt;p&gt;다이나믹 프로그래밍에 대해 이야기하기에 앞서 &lt;hl&gt;피보나치 수열&lt;/hl&gt;에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;● 예시&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1 → 1 → 2 → 3 → 5 → 8 → 13 → …&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;■ Run-time&lt;/p&gt;

\[T(n)=
\begin{cases}
	Θ(1) &amp;amp; \text{$n≤1$}\\
	T(n-1)+T(n-2)+Θ(1) &amp;amp; \text{$n&amp;gt;1$}
\end{cases}\]

&lt;p&gt;이러한 &lt;hl&gt;run time은 피보나치 수열만큼 증가함&lt;/hl&gt;을 알 수 있다. 그 이유는 피보나치 함수 $F(50)$을 계산하기 위해 각 n에 대해 다음과 같이 반복적으로 호출되기 때문이다.&lt;/p&gt;

\[F(45): \text{8번 호출}\\
F(40): \text{89번 호출}\\
F(30): \text{10,946번 호출}\\
F(20): \text{1,346,269번 호출}\\
F(10): \text{165,580,141번 호출}\\
F(1): \text{12,586,269,025번 호출}\]

&lt;p&gt;이러한 문제를 해결하기위한 방법으로 이전에 이미 구했던 결과를 저장하는 방법을 생각해볼 수 있다. 이러한 과정을 &lt;i&gt;&lt;b&gt;MemorizationM&lt;/b&gt;&lt;/i&gt;이라고 부른다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fibonacci-with-memorization&quot;&gt;Fibonacci with Memorization&lt;/h3&gt;

&lt;p&gt;&lt;hl&gt;Memorization을 사용하는 피보나치 함수&lt;/hl&gt;는 다음과 같이 프로그래밍될 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;isFirstCall&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isFirstCall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;}&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;memo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;gray&gt;※ global variable을 사용하는 것 보단 static을 사용하는 코딩 스타일이 더 좋다. ※&lt;/gray&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 알고리즘 해석&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memo&lt;/code&gt; static variable을 생성한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;처음 호출되는 경우 (메모리 할당이 필요):&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memo&lt;/code&gt;에 값이 있는 경우: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memo&lt;/code&gt; static 변수를 모두 삭제한다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n!=0&lt;/code&gt;인 경우(계산이 필요):&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memo&lt;/code&gt; 변수의 크기를 +1 늘려준 뒤, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memo&lt;/code&gt; array의 모든 값을 0으로 초기화한다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&amp;lt;=1&lt;/code&gt;인 경우: (계산 할 것도 없이) 1을 반환한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memo[n]==0&lt;/code&gt;인 경우(초기화된 직후): &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memo[n]&lt;/code&gt;의 위치에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;F(n-1)+F(n-2)&lt;/code&gt;의 결과를 저장한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;최종적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memo[n]&lt;/code&gt;을 반환한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;top-down-and-bottom-up-algorithms&quot;&gt;Top-down and Bottom-up Algorithms&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Top-down approach&lt;/strong&gt;: F(50)→F(49)→….&lt;/p&gt;

&lt;p&gt;※ 대부분의 Divide-and-conquer algorithm이 Top-down 방식으로 수행된다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bottom-up approach&lt;/strong&gt;: F(1)→F(2)→…&lt;/p&gt;

&lt;p&gt;피보나치 수열 문제는 Top-down이 아닌 Bottom-up 방식으로도 해결할 수 있다.&lt;/p&gt;

&lt;p&gt;e.g., Merge Sort: 미리 적당한 사이즈로 쪼개놓고, 위로 가는 방향&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● Fibonacci wiht Bottom-up approach&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;※ 코드가 상당히 간단하다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dynamic-programming&quot;&gt;Dynamic Programming&lt;/h2&gt;

&lt;p&gt;반복적으로 활용되는 &lt;hl&gt;sub-problem을 저장하여 재활용하여 문제를 해결하는 방법&lt;/hl&gt;이 &lt;i&gt;&lt;b&gt;다이나믹 프로그래밍&lt;/b&gt;&lt;/i&gt;이다. 이때 주로 &lt;hl&gt;Memorization 방식으로 속도를 향상&lt;/hl&gt;시킨다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;■ Divide-and-conquer와의 차이점&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Divide-and-conquer: &lt;hl&gt;sub-problem간에 overlap이 거의 없다.&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;e.g., Merge Sort: 양쪽으로 나눈 array간에 공통적인 부분이 없다. 즉, 재활용할 여지가 없다.&lt;/p&gt;

&lt;p&gt;Dynamic Programming: &lt;hl&gt;sub-problem에 overlap이 있다.&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;e.g., 피보나치 넘버: F(10)과같이 중복 문제가 계속 발생한다.&lt;/p&gt;

&lt;gray&gt;※ 이러한 중복 문제를 overlapping sub-problem이라고 한다. ※&lt;/gray&gt;

&lt;gray&gt;※ overlapping sub-problem의 값을 미리 저장하고 재활용하는 것이 dynamic programming의 핵심이다. ※&lt;/gray&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;행렬-체인-곱셈-문제의-동적계획법-기반-해결&quot;&gt;행렬 체인 곱셈 문제의 동적계획법 기반 해결&lt;/h1&gt;

&lt;p&gt;&lt;hl&gt;Dynamic Programming으로 matrix multiplication하는 문제를 해결&lt;/hl&gt;한다.&lt;/p&gt;

&lt;p&gt;행렬 체인 곱셈 문제의 정의와 AI분야에서의 중요성, 그리고 동적계획법 기반의 알고리즘을 학습한다.&lt;/p&gt;

&lt;p&gt;(추후 보충 예정)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 18 Sep 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo15/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo15/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-14] Lect14. 의존관계가 정의된 과제 그래프 모델링과 위상정렬 및 임계경로</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
이번 시간에는 &lt;hl&gt;의존관계가 정의된 과제들을 그래프로 모델링&lt;/hl&gt;해보고 &lt;hl&gt;위상정렬 및 임계경로&lt;/hl&gt;를 도출해본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dag-구조와-활용-분야&quot;&gt;DAG 구조와 활용 분야&lt;/h1&gt;

&lt;p&gt;&lt;hl&gt;의존 관계가 정의 된 과제&lt;/hl&gt;들의 &lt;hl&gt;방향-비순환(Directed Acyclic) 그래프&lt;/hl&gt;로의 모델링, 위상정렬의 정의, 활용 분야 및 알고리즘에 대해 학습한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;directed-acyclic-graph-dag&quot;&gt;Directed Acyclic Graph, DAG&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;DAG&lt;/b&gt;는 &lt;hl&gt;사이클을 포함하지 않는 유향(방향이 있는) 그래프&lt;/hl&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;vertex $v_j$→ $v_k$인 경로가 존재한다면, $v_k$→$v_j$인 경로는 존재하지 않는다.&lt;/p&gt;

    &lt;p&gt;(∵ 존재한다면, acyclic의 규칙이 깨지므로)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;topological-sort&quot;&gt;Topological Sort&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;■ 배경&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;주어진 작업 셋 간에 dependencies(의존성)이 있는 경우, &lt;hl&gt;의존성을 무너뜨리지 않고 모든 작업을 마치는 방법에 대한 해결책&lt;/hl&gt;이다. 이때 이러한 작업들은 &lt;hl&gt;Directed Acyclic Graph(DAG) 구조로 표현&lt;/hl&gt;할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Topological Sorting의 결과는 &lt;hl&gt;unique하지 않다.&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 응용&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;외출하기위해 옷을 입는 순서 (속옷→바지→티셔츠→…)&lt;/li&gt;
  &lt;li&gt;여러 개의 소스코드로 이루어진 프로그램의 컴파일 순서 결정&lt;/li&gt;
  &lt;li&gt;선행과목을 포함한 수강신청 (CS Basics→C Programming→Algorithms→…)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h3&gt;

&lt;p&gt;DAG $V$에 대해 &lt;hl&gt;Topological Sort&lt;/hl&gt;를 수행하는 과정은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;DAG $V$를 복사하여 DAG $W$에 저장한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;아래의 과정을 반복한다 :&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;DAG $W$ 내의 &lt;hl&gt;$\text{in-degree}=0$인 vertex $v$를 조사&lt;/hl&gt;한다. (즉, source인 vertex)&lt;/p&gt;

        &lt;gray&gt;※ in-degree는 vertex를 기준으로 들어오는 화살표를 의미한다. ※&lt;/gray&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;source에 해당하는 vertex $v$를 &lt;hl&gt;topological sort의 다음 순서에 넣는다.&lt;/hl&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;DAG $W$에서 &lt;hl&gt;vertex $v$를 제거&lt;/hl&gt;한다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;example&quot;&gt;Example&lt;/h3&gt;

&lt;p&gt;아래와 같이 12개의 vertex로 구성된 DAG를 Topological Sort한 결과를 구하라.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-18-cs-algorithm-algo14/image-20210918193131798.png&quot; alt=&quot;image-20210918193131798&quot; style=&quot;width:200px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt;

\[C,H,\ D,I,\ A,J,\ B,F,\  G,K,\ E,L\]

&lt;p&gt;※ vertex 선택 순서에 따라 topological sort의 결과는 달라질 수 있다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Initialization&lt;/p&gt;

    &lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ihead&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itail&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Testing if empty:&lt;/p&gt;

    &lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ihead&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itail&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For push&lt;/p&gt;

    &lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;itail&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itail&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For pop&lt;/p&gt;

    &lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_top&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ihead&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ihead&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot;&gt;
    topological sorting에 queue를 사용할 때 약간의 trick을 이용하여 topological sorting이 끝나면 queue의 sorting 순서대로 결과가 출력되게끔 만들 수 있다.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-18-cs-algorithm-algo14/image-20210918193226392.png&quot; alt=&quot;image-20210918193226392&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;analysis&quot;&gt;Analysis&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;vertex의 in-degree의 개수를 기록하는 table을 초기화: $Θ(V)$&lt;/li&gt;
  &lt;li&gt;아래의 과정을 $V$번 반복: $Θ(V)$
    &lt;ol&gt;
      &lt;li&gt;$\text{in-degree}=0$인 vertex를 찾기위해 in-degree table을 스캔 : $O(V)$&lt;/li&gt;
      &lt;li&gt;발견한 source vertex들을 queue에 모두 push &amp;amp;&amp;amp; pop : $Θ(V)$&lt;/li&gt;
      &lt;li&gt;source vertex들의 neighbors의 in-degree값에 -1 연산을 수행 :
        &lt;ul&gt;
          &lt;li&gt;adjacency matrix를 사용하는 경우: $Θ(V)$&lt;/li&gt;
          &lt;li&gt;adjacency list를 사용하는 경우: $Θ(E)$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;최종적으로 (adjacency list를 사용하는 경우) Topological Sorting은 다음의 complexity를 갖는다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;adjacency list를 사용하는 경우, $Θ(\lvert{V}\rvert+\lvert{E}\rvert)$.&lt;/li&gt;
  &lt;li&gt;adjacency matrix를 사용하는 경우, $Θ(\lvert{V}\rvert^2)$.&lt;/li&gt;
  &lt;li&gt;memory requirements = $Θ(\lvert{V}\rvert)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;critiacal-path-임계-경로&quot;&gt;Critiacal Path, 임계 경로&lt;/h1&gt;

&lt;p&gt;&lt;hl&gt;tasks에 대해 dependency와 수행시간이 정해져있을 때&lt;/hl&gt;&amp;gt; DAG에서 Critical Path(임계 경로)와 Critical Time(임계 시간)을 구하는 알고리즘에 대해 배운다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 주요 아이디어&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;아래와 같은 dependency를 갖는 tasks가 주어졌을 때, &lt;hl&gt;모든 작업을 마치는 데에 필요한 최소 시간&lt;/hl&gt;을 구해보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-18-cs-algorithm-algo14/image-20210918193424196.png&quot; alt=&quot;image-20210918193424196&quot; style=&quot;width:150px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;한 번에 하나의 작업만 수행 가능한 경우:&lt;/p&gt;

    &lt;p&gt;A→B→C→D→E의 순서로(사실 어떤 순서든 상관이 없다) 작업을 처리한다면 총 소요시간은 다음과 같다.&lt;/p&gt;

\[0.3+0.5+0.4+0.1+0.7=2.0(\text{sec})\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;B와 D를 동시에 수행할 수 있는 경우(&lt;hl&gt;병렬처리&lt;/hl&gt;):&lt;/p&gt;

    &lt;p&gt;(B,D)→A→C→E&lt;/p&gt;

\[\max(0.7,\ \min{(0.7, 0.5)}+0.3+0.4+0.1)=1.3(\text{sec})\]

    &lt;p&gt;※ 모든 Tasks가 끝나는 가장 빠른 시간을 &lt;i&gt;&lt;b&gt;Critical Time&lt;/b&gt;&lt;/i&gt;이라고 한다. ※&lt;/p&gt;

    &lt;p&gt;※ Ciritical Time에 해당하는 경로를 &lt;b&gt;&lt;i&gt;Critical Path&lt;/i&gt;&lt;/b&gt;라고 한다. ※&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;algorithm-1&quot;&gt;Algorithm&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-18-cs-algorithm-algo14/image-20210918193449940.png&quot; alt=&quot;image-20210918193449940&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-18-cs-algorithm-algo14/image-20210918193515037.png&quot; alt=&quot;image-20210918193515037&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;전체 tasks가 끝나기 위해서는 39.4sec의 시간이 소요되고, 각각의 task는 다음의 Ciritical Time을 갖는다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;A&lt;/th&gt;
      &lt;th&gt;B&lt;/th&gt;
      &lt;th&gt;C&lt;/th&gt;
      &lt;th&gt;D&lt;/th&gt;
      &lt;th&gt;E&lt;/th&gt;
      &lt;th&gt;F&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;5.2&lt;/td&gt;
      &lt;td&gt;11.3&lt;/td&gt;
      &lt;td&gt;31.3&lt;/td&gt;
      &lt;td&gt;39.4&lt;/td&gt;
      &lt;td&gt;26.6&lt;/td&gt;
      &lt;td&gt;17.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;i&gt;Ciritical Path&lt;/i&gt;는 &lt;hl&gt;previous Task를 이용하여 D를 기준으로 계산&lt;/hl&gt;하면 된다.&lt;/p&gt;

\[D←C←E←F←Φ : \text{Ciritical Path}\]

&lt;p&gt;이 Critical Path가 tasks를 모두 수행하기 위해 가장 오래 걸리는 tasks에 대한 순서이다. 따라서 &lt;hl&gt;전체 공정을 개선하고싶다면 FECD 중 무언가를 개선해야한다.&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 18 Sep 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo14/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo14/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-13] Lect13. 다익스트라 알고리즘 및 복잡도</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
이번 시간에는 &lt;hl&gt;다익스트라 알고리즘 및 복잡도&lt;/hl&gt;에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;그래프-최단경로-알고리즘&quot;&gt;그래프 최단경로 알고리즘&lt;/h1&gt;

&lt;p&gt;&lt;hl&gt;그래프 내 최단경로 문제&lt;/hl&gt;를 정의하고, 활용 분야 및 다익스트라 알고리즘의 핵심 아이디어를 이해한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;shortest-path&quot;&gt;Shortest Path&lt;/h2&gt;

&lt;p&gt;&lt;hl&gt;최단경로 문제&lt;/hl&gt;를 해결하는 방법에는 대표적으로 &lt;hl&gt;Dijkstra's Algorithm&lt;/hl&gt;이 있다. &lt;/p&gt;

&lt;h2 id=&quot;dijkstras-algorithm&quot;&gt;Dijkstra’s Algorithm&lt;/h2&gt;

&lt;p&gt;Prim’s Algorithm과 유사하지만 &lt;hl&gt;모든 edges의 weights값이 양수여야만 한다는 제한조건&lt;/hl&gt;을 갖는다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-18-cs-algorithm-algo13/image-20210912011036792.png&quot; alt=&quot;image-20210912011036792&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이처럼 Dijkstra’s Algorithm은 &lt;hl&gt;최단경로를 하나씩 확정해 나가는 방법&lt;/hl&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;다익스트라-알고리즘과-복잡도&quot;&gt;다익스트라 알고리즘과 복잡도&lt;/h1&gt;

&lt;p&gt;다익스트라 알고리즘의 구동 원리를 이해하고, 그래프 표현 자료구조 및 거리 저장 자료구조에 따른 복잡도 차이를 분석한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dijkstras-algorithm-1&quot;&gt;Dijkstra’s Algorithm&lt;/h2&gt;

&lt;h3 id=&quot;구성요소&quot;&gt;구성요소&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Prim’s Algorithm과 마찬가지로 초기에는 &lt;hl&gt;initial vertex에 대한 정보&lt;/hl&gt;만을 가지고 있다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;3개의 array(`distance`, `visit`, `parent`)&lt;/hl&gt;를 갖는다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;동작방식&quot;&gt;동작방식&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;초기화&lt;/li&gt;
  &lt;li&gt;아래의 동작을 모든 vertex를 방문할 때 까지 $V$번 반복한다.
    &lt;ol&gt;
      &lt;li&gt;unvisited vertex 중 initial vertex로부터 &lt;hl&gt;가장 거리가 짧은 vertex를 방문(visit)&lt;/hl&gt;한다.&lt;/li&gt;
      &lt;li&gt;방문한 vertex의 neighbors의 &lt;hl&gt;distance와 parent 정보를 업데이트&lt;/hl&gt;한다.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;※ shortest path의 거리가 무한대(∞)라면 해당 graph는 unconnected-graph이다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;example&quot;&gt;Example&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;■ 예제1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;initial vertex=K일 때, K로부터 다른 모든 vertex까지의 최단경로를 구해본다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-18-cs-algorithm-algo13/image-20210912011223861.png&quot; alt=&quot;image-20210912011223861&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;※ unvisited vertex의 distance = ∞라면, unconnected-graph이다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예제2&lt;/strong&gt;: 최단거리뿐만 아니라 최단 경로도 고려하는 문제&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-18-cs-algorithm-algo13/image-20210912011507809.png&quot; alt=&quot;image-20210912011507809&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;hl&gt;previous flag&lt;/hl&gt;를 추가하여 &lt;hl&gt;역순으로 추적&lt;/hl&gt;할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;analysis&quot;&gt;Analysis&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Initialization: $Θ(V)$ (∵vertex의 개수 $V$만큼 array를 순회)&lt;/li&gt;
  &lt;li&gt;아래의 동작을 $V-1$번 반복한다:
    &lt;ol&gt;
      &lt;li&gt;unvisited vertex 중 initial vertex로부터 &lt;hl&gt;가장 거리가 짧은 vertex를 방문(visit)&lt;/hl&gt;한다: $Θ(V)$ &lt;br /&gt;&lt;gray&gt;(∵ 현재까지 조회된 vertex들의 distance를 차례로 탐색)&lt;/gray&gt;&lt;/li&gt;
      &lt;li&gt;방문한 vertex의 neighbors의 &lt;hl&gt;distance와 parent 정보를 업데이트&lt;/hl&gt;한다:
        &lt;ul&gt;
          &lt;li&gt;adjacency matrix를 사용하는 경우: $Θ(V)$&lt;/li&gt;
          &lt;li&gt;adjacency list를 사용하는 경우: $Θ(E)$ (이때 $E&amp;lt;V^2$)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서 최종적으로 아래와 같은 &lt;hl&gt;복잡도&lt;/hl&gt;를 갖는다.&lt;/p&gt;

\[V+(V-1)·(V+V)\text{ or }V+(V-1)·(V)+E\\
∴ Θ(V^2)\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Dijkstra’s algorithm을 적용할 때, 단순히 distance table을 앞에서부터 차례로 순회하는 것이 아니라 &lt;hl&gt;priority queue(min binary heap)&lt;/hl&gt;과 같은 자료구조를 활용하면 보다 &lt;hl&gt;최적화&lt;/hl&gt;할 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Initialization: $Θ(V)$ (∵vertex의 개수 $V$만큼 array를 순회)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;아래의 동작을 $V$번 반복한다:&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;unvisited vertex 중 initial vertex로부터 &lt;hl&gt;가장 거리가 짧은 vertex를 방문(visit)&lt;/hl&gt;한다: $Θ(1)$ &lt;br /&gt;&lt;gray&gt;(∵ root로 바로 접근 가능)&lt;/gray&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;방문한 vertex의 neighbors의 &lt;hl&gt;distance와 parent 정보를 업데이트&lt;/hl&gt;한다: $Θ(\lg(V))$&lt;/p&gt;

        &lt;gray&gt;※ 이때, 업데이트 발생시, heap 내부 구조에서 또 다시 업데이트가 발생하여 다음의 추가 비용이 든다: $Θ(E\lg(V))$&lt;/gray&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서 최종적으로 아래와 같은 &lt;hl&gt;복잡도&lt;/hl&gt;를 갖는다.&lt;/p&gt;

\[V+V·(1+\lg(V))+E\lg(V)\\
∴ Θ(V\lg(V)+E\lg(V))\\
= \begin{cases}
	Θ(E\lg(V)) &amp;amp; \text{if $E&amp;lt;&amp;lt;V^2$}\\
	Θ(V^2\lg(V)) &amp;amp; \text{if $E≒V^2$}
\end{cases}\]

&lt;p&gt;즉, edges의 개수가 극단적으로 많지($Θ(V^2\lg(V))$) 않다면 &lt;hl&gt;priority queue&lt;/hl&gt;를 사용하는 것($Θ(E\lg(V))$)이 일반적인 array를 사용하는 것($Θ(V^2))$)보다 더 &lt;hl&gt;효율적&lt;/hl&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 18 Sep 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo13/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo13/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-12] Lect12. 서로소 집합 자료구조의 원리 및 크루스칼 알고리즘</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
이번 시간에는 &lt;hl&gt;서로소 집합 자료구조의 원리&lt;/hl&gt;에 대해 이해하고, &lt;hl&gt;크루스칼 알고리즘&lt;/hl&gt;을 적용해본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;disjoint-set-서로소-집합&quot;&gt;Disjoint Set, 서로소 집합&lt;/h1&gt;

&lt;p&gt;&lt;b&gt;서로소 집합(Disjoint Set)&lt;/b&gt; &lt;hl&gt;자료구조의 원리&lt;/hl&gt;를 이해하고, 각 &lt;hl&gt;연산의 복잡도&lt;/hl&gt;를 분석해본다.&lt;/p&gt;

&lt;h2 id=&quot;개념&quot;&gt;개념&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Disjoint Set&lt;/b&gt;이란 &lt;hl&gt;공통으로 중복되는 데이터가 없는 집합 $S_1, S_2, ... S_k$의 집합&lt;/hl&gt;이다. 이때 각 집합 요소($S_k$)는 &lt;hl&gt;하나의 대표 값으로 표현될 수 있다.&lt;/hl&gt;&lt;/p&gt;

\[C = \{S_1, S_2, ..., S_k\}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;operations&quot;&gt;Operations&lt;/h2&gt;

&lt;p&gt;Disjoint Set에 대해 대표적으로 &lt;hl&gt;세 가지 Operations&lt;/hl&gt;을 수행할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make-set(x)&lt;/code&gt;: 멤버가 오직 x뿐인 &lt;hl&gt;새로운 집합을 생성&lt;/hl&gt;한다. &lt;gray&gt;(∴ 대표값=x)&lt;/gray&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;union-set(x,y)&lt;/code&gt;: x, y로 대표되는 집합을 &lt;hl&gt;합집합&lt;/hl&gt;한다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find-set(x)&lt;/code&gt;: x를 멤버로 갖는 집합의 &lt;hl&gt;대표값을 반환&lt;/hl&gt;한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 응용&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find-set(x)==find-set(y)&lt;/code&gt;인 경우, 두 멤버 x, y는 &lt;hl&gt;같은 집합에 포함되어 있다.&lt;/hl&gt;&amp;gt;&lt;/li&gt;
  &lt;li&gt;connected components&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;connected-components&quot;&gt;Connected Components&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912000406869.png&quot; alt=&quot;image-20210912000406869&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;hl&gt;모든 edges에 대해 union-set 연산을 수행&lt;/hl&gt;한 최종 결과로 &lt;hl&gt;connected components&lt;/hl&gt;를 얻을 수 있다. &lt;gray&gt;※ DFS에 비해 비효율적인 방법 ※&lt;/gray&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;■ Poor Implementation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;hl&gt;Disjoint Set 자료구조를 구현&lt;/hl&gt;하기위한 가장 단순한 방법으로 아래와 같이 &lt;hl&gt;2D array&lt;/hl&gt;를 사용하는 방법을 생각해볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912002226447.png&quot; alt=&quot;image-20210912002226447&quot; style=&quot;width:550px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모든 vertex에 대한 &lt;hl&gt;대표자를 표시&lt;/hl&gt;하여 구현할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;멤버 x의 집합을 알아내는 Finding 연산: &lt;hl&gt;$Θ(1)$&lt;/hl&gt;&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;union-set(x,y)&lt;/code&gt;: &lt;hl&gt;$Θ(n)$&lt;/hl&gt; &lt;gray&gt;(∵ y가 속한 집합의 모든 멤버를 조사하여 x가 속한 집합의 대표자로 대표자값을 바꿔줘야 한다.)&lt;/gray&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Implementation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;2D array를 사용하는 방식은 연산에 비용이 많이 소요되기 때문에 실제로는 아래와 같이 &lt;hl&gt;트리 구조로 Disjoint Set을 구현&lt;/hl&gt;한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912002355644.png&quot; alt=&quot;image-20210912002355644&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;hl&gt;대표자를 root값으로&lt;/hl&gt; 두고, &lt;hl&gt;멤버를 자식 nodes로&lt;/hl&gt; 둔다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;union-set(x,y)&lt;/code&gt;을 수행하는 경우 &lt;hl&gt;단순히 root값을 자식 node로 추가&lt;/hl&gt;해준다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912002500395.png&quot; alt=&quot;image-20210912002500395&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find-set(x)&lt;/code&gt;: $O(h)$&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;union-set(x,y)&lt;/code&gt;: $O(h)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;※ 일반적인 tree 자료구조는 부모가 자식 nodes를 point하지만, disjoint set을 구현하기 위한 tree 구조에서는 &lt;hl&gt;자식이 부모를 point하는 방향&lt;/hl&gt;으로 구현된다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● Generation of Disjoint Set&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$n$개의 데이터에 대해 &lt;hl&gt;Disjoint Set를 구현&lt;/hl&gt;하기 위해 &lt;hl&gt;길이가 $n$인 array를 생성&lt;/hl&gt;한다.&lt;br /&gt;&lt;gray&gt;※ 이때 parent의 초기값은 자기 자신이 된다. ※&lt;/gray&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● Find-set(x)&lt;/strong&gt;: &lt;hl&gt;x의 parent를 찾는 연산&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;&lt;hl&gt;parent값이 자기 자신이 될 때 까지 parent를 참조&lt;/hl&gt;한다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Find_Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

\[\text{Complexity} : T_{\text{find}}(n) = O(h)\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● Union-set(x,y)&lt;/strong&gt;: x와 y가 속한 집합의 &lt;hl&gt;합집합&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;x와 y의 &lt;hl&gt;root parent를 구한 뒤&lt;/hl&gt;, y의 root parent의 &lt;hl&gt;parent값을 x의 root parent로 변경&lt;/hl&gt;해준다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Union_Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Find_Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Find_Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

\[\text{Complexity} : T_{\text{union}}(n) = 2T_{\text{find}}(n)+Θ(1) = O(h)\]

&lt;p&gt;※ 이때 &lt;hl&gt;height값이 큰 집합을 x로 사용&lt;/hl&gt;하면 height값을 줄여 complexity를 작게 만들 수 있다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;example&quot;&gt;Example&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912002930085.png&quot; alt=&quot;image-20210912002930085&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ union-set(1,3)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912003029896.png&quot; alt=&quot;image-20210912003029896&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ find-set(1) and find-set(3)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912003123190.png&quot; alt=&quot;image-20210912003123190&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ union-set(3,5)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912003216458.png&quot; alt=&quot;image-20210912003216458&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ union-set(5,7)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912003302864.png&quot; alt=&quot;image-20210912003302864&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;worst-case-scenario&quot;&gt;Worst-Case Scenario&lt;/h3&gt;

&lt;p&gt;&lt;hl&gt;같은 높이&lt;/hl&gt;를 갖는 tree x, y를 &lt;hl&gt;union-set(x,y)&lt;/hl&gt;하는 경우 tree x의 &lt;hl&gt;height가 커진다.&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912004030362.png&quot; alt=&quot;image-20210912004030362&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이때 각 level에서 nodes의 개수를 세 보면 &lt;hl&gt;Pascal's triangle의 수와 같다&lt;/hl&gt;는 걸 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;pascals-triangle&quot;&gt;Pascal’s Triangle&lt;/h4&gt;

\[\begin{pmatrix} n\\ m \end{pmatrix}
=
\begin{cases}
	1 &amp;amp; \text{$m=0$ or $m=n$}\\
	\begin{pmatrix} n-1\\ m \end{pmatrix} +
		\begin{pmatrix} n-1\\ m-1 \end{pmatrix}
		&amp;amp; \text{$0&amp;lt;m&amp;lt;n$}
\end{cases}\\
=\frac{n!}{m!(n-m)!}\]

&lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912004650630.png&quot; alt=&quot;image-20210912004650630&quot; style=&quot;width:150px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 Pascal’s Triangle을 알면 &lt;hl&gt;평균적으로 어느 정도 높이를 올라가야 root parent를 만날 수 있는지를 분석&lt;/hl&gt;할 수 있다.&lt;/p&gt;

\[\sum_{k=0}^h{_nC_k} = \sum_{k=0}^h{\binom{h}{k}} = \sum_{k=0}^h{\frac{h!}{k!(h-k)!}} = 2^h = n\\
\sum_{k=0}^h{k\binom{h}{k}} = h2^{h-1}\\
∴ \text{average depth} = \frac{h·2^{h-1}}{2^h}=\frac{h}{2}=\frac{\lg(n)}{2}\]

&lt;p&gt;따라서 &lt;hl&gt;worst case&lt;/hl&gt;일 때, &lt;hl&gt;height와 average depth의 complexity는 $O(\lg(n))$&lt;/hl&gt;이다.&lt;br /&gt;⇒ Disjoint Set이 &lt;hl&gt;효율적&lt;/hl&gt;인 자료구조임을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;서로소-집합과-크루스칼-알고리즘&quot;&gt;서로소 집합과 크루스칼 알고리즘&lt;/h1&gt;

&lt;p&gt;&lt;hl&gt;크루스칼 알고리즘에서의 연결요소 관리를 서로소 집합으로 변경&lt;/hl&gt;했을 때 &lt;hl&gt;복잡도를 개선&lt;/hl&gt;할 수 있는 원리에 대해 이해하고 코딩으로 구현해본다.&lt;/p&gt;

&lt;h2 id=&quot;kruskals-algorithm-using-disjoint-sets&quot;&gt;Kruskal’s Algorithm using Disjoint Sets&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Kruskal’s Algorithm&lt;/b&gt;은 &lt;hl&gt;minimum spanning tree 문제&lt;/hl&gt;에서 사용할 수 있는 알고리즘 중 하나이다&lt;gray&gt;(이전 시간에 배웠다.)&lt;/gray&gt;. 이전에 배운 Kruskal’s Algorithm에서는 &lt;hl&gt;cycle을 체크하는 방식으로 DFS나 BFS와 같은 traversal algorithm을 사용&lt;/hl&gt;했다. 이때 &lt;hl&gt;Disjoint Set의 자료구조&lt;/hl&gt;를 Kruskal’s algorithm에 도입하면, algorithm의 complexity를 줄일 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Example&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-12-cs-algorithm-algo12/image-20210912005007068.png&quot; alt=&quot;image-20210912005007068&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;analysis&quot;&gt;Analysis&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;■ Worst Case&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;아래 알고리즘에 대해 전체 edges 개수 $E$만큼 반복&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;두 vertex가 &lt;hl&gt;같은 집합에 포함되었는지를 확인&lt;/hl&gt; (find-set) : $O(\lg(V))$&lt;/li&gt;
  &lt;li&gt;서로 다른 집합에 속하는 경우
    &lt;ol&gt;
      &lt;li&gt;두 집합을 &lt;hl&gt;합집합&lt;/hl&gt;화 (union-set): $O(\lg(V))$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서 Complexity는 다음과 같다.&lt;/p&gt;

\[E·(\lg(V) + \lg(V)) = 2E\lg(V)\\
\text{Complexity} = O(E\lg(V))\]

&lt;p&gt;한편, Kruskal’s Algorithm을 적용하기에 앞서 &lt;hl&gt;모든 edges의 weights를 정렬하는 데에 필요한 비용&lt;/hl&gt;은 $O(E\lg(E))=O(E\lg(V))$이다. &lt;gray&gt;(∵$E=V^2$)&lt;/gray&gt;&lt;/p&gt;

&lt;p&gt;따라서 최종 complexity는 다음과 같다.&lt;/p&gt;

\[\text{overall complexity} = O(E\lg(V) + E\lg(V)) = O(E\lg(V))\]

&lt;p&gt;※ DFS(or BFS)를 이용한 Kruskal’s algorithm의 complexity $O(EV)$에 비해 &lt;hl&gt;효율적&lt;/hl&gt;임을 알 수 있다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 12 Sep 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo12/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo12/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-11] Lect11. 최소신장트리 문제에 대한 프림, 크루스칼 두 알고리즘</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이번 시간에는 &lt;hl&gt;최소신장트리 문제&lt;/hl&gt;에 대한 &lt;hl&gt;프림&lt;/hl&gt;, &lt;hl&gt;크루스칼&lt;/hl&gt; 두 알고리즘에 대해 배운다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;최소신장트리-문제와-프림-알고리즘&quot;&gt;최소신장트리 문제와 프림 알고리즘&lt;/h1&gt;

&lt;p&gt;&lt;hl&gt;그래프에서 발생하는 여러가지 문제&lt;/hl&gt;들을 해결하는 방법에 대해 알아본다.&lt;/p&gt;

&lt;h2 id=&quot;minimum-spanning-trees&quot;&gt;Minimum Spanning Trees&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;■ 정의&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo11/image-20210911225434559.png&quot; alt=&quot;image-20210911225434559&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Minimum Spanning Tree&lt;/b&gt;란 Graph의 &lt;hl&gt;모든 nodes가 최소한의 weights로 연결되는 Tree 자료구조&lt;/hl&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 쓰임&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;각 도시를 연결짓는 도로를 건설하려고 할 때, &lt;hl&gt;건설 비용을 최소화하면서 모든 도시를 연결할 수 있는 길을 찾는 문제&lt;/hl&gt;에서 Minimum Spanning Tree 자료구조를 사용할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;spanning-tree&quot;&gt;Spanning Tree&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo11/image-20210911225919799.png&quot; alt=&quot;image-20210911225919799&quot; style=&quot;width:250px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Spanning Tree&lt;/b&gt;란 Graph의 &lt;hl&gt;모든 vertex가 연결되도록 구성된 Tree&lt;/hl&gt;이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;vertex의 개수가 $V$개일 때, Tree는 언제나 $V-1$개의 edges를 가지므로, &lt;hl&gt;Graph로부터 $V-1$개의 edges를 고르는 문제&lt;/hl&gt;라 생각할 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Spanning Tree가 &lt;hl&gt;Weighted Tree인 경우&lt;/hl&gt;, Spanning Tree의 weight는 &lt;hl&gt;모든 edges의 weight 합&lt;/hl&gt;으로 정의된다.&lt;/p&gt;

\[\text{Spanning Tree의 weight} = \sum_{i=1}^{V}{w_i}\]

    &lt;p&gt;※ Spanning Tree의 weigth를 최소화하는 문제를 &lt;hl&gt;Minimum Spanning Tree&lt;/hl&gt;라고 한다. ※&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Spanning Tree가 &lt;hl&gt;Unweighted Tree&lt;/hl&gt;인 경우, 모든 edges의 $\text{weigth}=1$이라 가정할 수 있다. 따라서 이 경우 &lt;hl&gt;Minimum Spanning Tree의 weight는 언제나 $V-1$&lt;/hl&gt;이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;Unique하지 않다.&lt;/hl&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;$V-1$개의 edges를 선택했을 때, &lt;hl&gt;어떤 vertex를 root로 보느냐에 따라 다른 Tree가 된다.&lt;/hl&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;algorithms&quot;&gt;Algorithms&lt;/h3&gt;

&lt;p&gt;&lt;hl&gt;Minimum Spanning Tree문제&lt;/hl&gt;를 해결하는 대표적인 알고리즘은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;hl&gt;Prim's Algorithm&lt;/hl&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;hl&gt;Kruskal's Algorithm&lt;/hl&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;hl&gt;두 알고리즘 모두 Greedy Algorithm&lt;/hl&gt;임에도 불구하고 최적의 값을 도출할 수 있음이 증명되었다.&lt;/p&gt;

&lt;div class=&quot;callout&quot;&gt;
    &lt;b&gt;Greedy Algorithm&lt;/b&gt;이란?&lt;br /&gt;
    &lt;hl&gt;매 순간 최선의 선택&lt;/hl&gt;을 취하는 알고리즘
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;prims-algorithm&quot;&gt;Prim’s Algorithm&lt;/h2&gt;

&lt;h3 id=&quot;개념&quot;&gt;개념&lt;/h3&gt;

&lt;p&gt;&lt;hl&gt;$N$개의 vertex를 갖는 graph로부터 minimum spanning tree를 결정하는 방법&lt;/hl&gt;은 다음과 같다. $k$개의 vertex로 이루어진 minimum spanning tree가 주어졌다면, &lt;hl&gt;기존의 minimum spanning tree에 $k+1$번째 vertex를 추가해나가는 방식&lt;/hl&gt;으로 $N$개의 vertex를 갖는 minimum spanning tree를 결정할 수 있다.&lt;/p&gt;

&lt;p&gt;이때 최소한의 weight를 갖도록 $k+1$번째 vertex를 선택하는 방법은, &lt;hl&gt;기존의 tree에 직접적으로 연결된 edges 중 최소한의 weight를 갖는 edge를 선택&lt;/hl&gt;하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo11/image-20210911231030263.png&quot; alt=&quot;image-20210911231030263&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;hl&gt;$e_k$&lt;/hl&gt;를 minimum spanning tree와 $v_{k+1}$을 연결하는 &lt;hl&gt;최소의 weight를 가진 edge&lt;/hl&gt;라고 할 때, 위 예시에서 $v_{k+1}$을 연결하기 위한 edge로 $e_k$를 선택하는 것이 가장 합리적인지를 생각해보자. &lt;/p&gt;

&lt;p&gt;$e_k$를 선택하지 않는다면 다른 어떤 경로($\tilde{e}$)를 통해 $v_{k+1}$으로 연결되어야 한다. 이때 $e_k$가 $v_k$를 연결하는 최소의 weight를 가졌으므로 다른 경로 $\tilde{e}$는 반드시 $e_k$보다 weight값이 클 수밖에 없다. 따라서 &lt;hl&gt;$e_k$를 선택하는 것이 가장 합리적인 결정&lt;/hl&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;특징&quot;&gt;특징&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;hl&gt;어떤 vertex에서 시작하든지 상관 없다.&lt;/hl&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;■ 배경 지식&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;distance&lt;/code&gt;: &lt;hl&gt;현재 minimum spanning tree로부터 어떤 특정 vertex까지의 거리&lt;/hl&gt; &lt;gray&gt;※초기값은 ∞. 단, root vertex의 distance=0 ※&lt;/gray&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;visit&lt;/code&gt;: 해당 vertex가 &lt;hl&gt;이미 minimum spanning tree에 포함되었는지&lt;/hl&gt;를 체크하는 flag &lt;gray&gt;※ 초기값=0 ※&lt;/gray&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parent&lt;/code&gt;: 어떤 vertex가 minimu spanning tree에 포함될 때, &lt;hl&gt;어떤 node와 직접적으로 연결되었는가&lt;/hl&gt;를 체크하는 flag &lt;gray&gt;※ 초기값=NULL ※&lt;/gray&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 알고리즘&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;minimum distance를 갖는 unvisited vertex를 선택&lt;/hl&gt;한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;해당 vertex를 &lt;hl&gt;visited상태로&lt;/hl&gt; 바꿔준다.&lt;/li&gt;
  &lt;li&gt;각 인접 vertex에 대한 distance를 고려하여 &lt;hl&gt;기존의 distance값을 업데이트&lt;/hl&gt;해준다.&lt;/li&gt;
  &lt;li&gt;①모든 vertex가 visited상태이거나 ②모든 unvisited vertex의 distance값이 ∞가 될 때 까지 &lt;hl&gt;1~3의 과정을 반복&lt;/hl&gt;한다. &lt;gray&gt;※ ②의 경우 unconnected graph ※&lt;/gray&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예제&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo11/image-20210911231907393.png&quot; alt=&quot;image-20210911231907393&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;복잡도&quot;&gt;복잡도&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo11/image-20210904010142704.png&quot; alt=&quot;image-20210904010142704&quot; style=&quot;width:700px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;최종적으로 Prim’s Algorithm의 복잡도는 $Θ(V^2)$가 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;최적화&quot;&gt;최적화&lt;/h3&gt;

&lt;p&gt;minimum distance를 구하기에 가장 적합한 자료구조는 min-heap 자료구조이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;min-heap의 nodes의 개수는 $V$개가 된다. ⇒ $Θ(V)$의 memory and run time&lt;/li&gt;
  &lt;li&gt;최소값을 구하는 데(Pop)에 걸리는 비용은 $\lg(V)$이다.&lt;/li&gt;
  &lt;li&gt;따라서 최소값을 찾는 데에 드는 총 $(V-1)\lg(V)$가 소요된다. (∵ V-1번 위 과정을 반복) ⇒ $O(V\lg(V))$&lt;/li&gt;
  &lt;li&gt;최소값을 구한 뒤, neighbors에 대해 distance를 update하는 데에 $E\lg(V)$가 소요된다. (∵ edge만큼 연산되며, 각 update는 $\lg(V)$가 소요됨) ⇒ $O(E\lg(V))$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서 total run time은 다음과 같다.
\(O(V\lg(V) + E\lg(V)) = O(E\lg(V))\)
※ 이떄 edges의 개수 $E$는 최대 $V^2$의 값을 가질 수 있다. 따라서 edges의 개수가 많은 경우 오히려 min-heap(priority queue)를 사용하지 않는 것이 더 효율적이다. ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;kruskals-algorithm&quot;&gt;Kruskal’s Algorithm&lt;/h1&gt;

&lt;p&gt;크루스칼 알고리즘은 프림 알고리즘 보다 간단한 알고리즘이다.&lt;/p&gt;

&lt;p&gt;크루스칼 알고리즘은 weight에 따라 edges를 정렬하고, 최소 weight부터 차례로 minimum spanning graph에 추가시켜 나간다. 이때 tree에 cycle이 생기지 않도록(∵ cycle이 생기면 tree가 아니라 graph가 되니까) 추가하는 것이 핵심이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo11/image-20210911232526642.png&quot; alt=&quot;image-20210911232526642&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;graph 내에 존재하는 모든 edges를 weight 순서대로 정렬한다.&lt;/li&gt;
  &lt;li&gt;위에서부터 하나씩 순회하며 minimum spanning tree에 삽입 여부를 결정한다.
    &lt;ol&gt;
      &lt;li&gt;해당 edge를 포함시킬 때 cycle이 생성되면 무시한다. (skip)&lt;/li&gt;
      &lt;li&gt;해당 edge를 포함시킬 때 cycle이 생성되지 않으면 추가한다. (addition)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;2번 과정을 $V-1$개의 edges를 고를 때 까지 반복한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;복잡도-1&quot;&gt;복잡도&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;edges를 정렬하기 위한 merge sort(or heap sort) : $O(E\lg(E))$&lt;/li&gt;
  &lt;li&gt;cycle 체크를 위한 DFS(or BFS) : $O(E)=O(V)$ (∵$ E&amp;lt;V$이므로)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서 최종적인 run-time은 다음과 같다.
\(O(E\lg(E) + E·V)\\
= O(E·V)\ \ (∵ E=O(V^2)\text{이므로 }\lg(E)=\lg(V))\)
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;특징-1&quot;&gt;특징&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;프림알고리즘($O(E\lg(V))$ 혹은 $O(V^2)$)에 비해 비효율적($O(E·V)$)이다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;알고리즘이 단순하다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;cycle check를 위해 DFS(or BFS) 대신 disjoint set을 사용하면 complexity를 낮출 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 06 Sep 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo11/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo11/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-10] Lect10. Heap 정렬, Quick-Sort의 알고리즘 및 최선/평균/최악 복잡도</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이번 시간에는 &lt;hl&gt;Heap 정렬, Quick-Sort의 알고리즘 및 최선/평균/최악 복잡도&lt;/hl&gt;에 대해 배운다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;힙-정렬&quot;&gt;힙 정렬&lt;/h1&gt;

&lt;p&gt;주어진 &lt;hl&gt;배열을 힙으로 변환하는 방법&lt;/hl&gt;과, &lt;hl&gt;힙 정렬 알고리즘 구동 원리 및 복잡도 분석&lt;/hl&gt;에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Min Heap Sort&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;hl&gt;Min Heap의 item을 오름차순으로 정렬하는 방법&lt;/hl&gt;은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;주어진 min heap에서 root node의 값을 Pop한다.&lt;/li&gt;
  &lt;li&gt;재정렬된 min heap에서 root node의 값을 Pop한다.&lt;/li&gt;
  &lt;li&gt;모든 nodes가 Pop될 때 까지 2번 과정을 반복한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위의 알고리즘을 사용하여 Min Heap Sort를 수행할 때, &lt;hl&gt;Complexity&lt;/hl&gt;는 다음과 같다.&lt;/p&gt;

\[\sum_{k=1}^{n}{\lg(k)} = 
\lg(\prod_{k=1}^{n}{k}) =
\lg(n!) \approx n\lg(n) \\ 
∴ \text{Complexity} = O(n\lg(n))\]

&lt;p&gt;※ Pop operation의 complexity=$O(\lg(n))$ ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 문제점&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;n nodes를 갖는 min heap을 sort하기 위해서는, 기존의 n길이 array 이외에 추가적인 n 길이 array가 필요하다. 즉, 위와 같은 min heap sort 알고리즘은 &lt;hl&gt;in-place로 동작하지 않는다.&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;※ in-place: 추가적인 메모리 공간이 거의 요구되지 않는 특성 ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;max-heap-sort&quot;&gt;Max Heap Sort&lt;/h2&gt;

&lt;p&gt;위와 같은 (in-place로 동작하지 않는) 문제점을 해결하기 위해 &lt;hl&gt;max heap sort 알고리즘&lt;/hl&gt;을 생각해볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;heap-sort-algorithm&quot;&gt;Heap Sort Algorithm&lt;/h3&gt;

&lt;p&gt;&lt;hl&gt;Heap Sort Algorithm 과정&lt;/hl&gt;은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;주어진 Array를 Complete Tree형태로 표현&lt;/hl&gt;한다.&lt;br /&gt;&lt;gray&gt;※ min-heap도 max-heap도 아닌 그냥 binary-tree 형태이다. ※&lt;/gray&gt;&lt;br /&gt;&lt;gray&gt;※ array index=0에서 부터 시작함 (∴ $\text{자식 node}=2k+1 \text{and} 2k+2$; $\text{부모 node}=(k-1)/2$) ※&lt;/gray&gt;&lt;br /&gt;
  &lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo10/image-20210903150754347.png&quot; alt=&quot;image-20210903150754347&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;뒤에서부터 sub-tree별로 max-heap을 만족하도록 재정렬&lt;/hl&gt;해준다.&lt;br /&gt;
  &lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo10/image-20210903154140672.png&quot; alt=&quot;image-20210903154140672&quot; style=&quot;width:900px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;complexity&quot;&gt;Complexity&lt;/h4&gt;

&lt;p&gt;$\text{depth}=k$인 &lt;hl&gt;node의 값이 이동할 수 있는 최대 경로의 길이는 $h-k$&lt;/hl&gt;와 같으며, &lt;hl&gt;depth=k에서 nodes의 개수는 $2^k$개&lt;/hl&gt;이다. 따라서 &lt;hl&gt;최악의 경우 heap sort algorithm을 수행하는 데에 필요한 node의 움직임 수&lt;/hl&gt;는 다음과 같다.&lt;/p&gt;

\[\sum_{k=0}^{h}{2^k(h-k)} = 
(2^{h+1}-1)-(h+1)\]

&lt;p&gt;이때 node의 개수 $n$에 대해 위 식을 고쳐 쓰면 다음과 같다.&lt;/p&gt;

\[n=2^{h+1}-1\text{이므로}\\
\text{complexity}=O{((2^{h+1}-1)-(h+1))}=O(n-\lg(n+1))\\
∵\ \lg(n+1)=h+1\]

&lt;p&gt;최종적으로 &lt;hl&gt;in-place heapify algorithm은 $O(n)$의 복잡도&lt;/hl&gt;를 갖게 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;max-heap-2-min-heap&quot;&gt;Max-heap 2 Min-heap&lt;/h4&gt;

&lt;p&gt;위의 과정을 통해 구한 &lt;hl&gt;max-heap을 min-heap으로 변환하는 과정&lt;/hl&gt;은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;root node의 값을 Pop한다. (⇒ 마지막 leaf node가 그 자리를 채운 뒤, percolation down된다.)&lt;/li&gt;
  &lt;li&gt;기존의 leaf node값이 존재하던 자리를 Pop된 값이 채운다.&lt;/li&gt;
  &lt;li&gt;위 1~2과정을 min-heap이 될 때 까지 반복한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Complexity&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이전에 구했듯 &lt;hl&gt;Heapification을 수행하는 데에는 $Θ(n)$이 소요&lt;/hl&gt;된다 (complete tree→max heap). 이후 max heap을 min heap으로 변환하는 과정에서는 n번의 pop과 insert가 수행되므로&lt;hl&gt; $Θ(n\lg(n))$이 소요&lt;/hl&gt;된다.&lt;br /&gt;(∵ $\text{pop의 copmlexity}=\lg(n)$ )&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Case&lt;/th&gt;
      &lt;th&gt;Run Time&lt;/th&gt;
      &lt;th&gt;Comments&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Worst&lt;/td&gt;
      &lt;td&gt;$Θ(n\lg(n))$&lt;/td&gt;
      &lt;td&gt;No worst case&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Average&lt;/td&gt;
      &lt;td&gt;$Θ(n\lg(n))$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Best&lt;/td&gt;
      &lt;td&gt;$Θ(n)$&lt;/td&gt;
      &lt;td&gt;All or most entries are same&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;퀵-정렬의-평균최악-복잡도-분석&quot;&gt;퀵 정렬의 평균/최악 복잡도 분석&lt;/h1&gt;

&lt;p&gt;&lt;hl&gt;퀵 정렬 알고리즘 구동 원리&lt;/hl&gt; 파악 후 &lt;hl&gt;평균/최악의 자료 분포 도출 및 각 상황에서의 복잡도를 분석&lt;/hl&gt;한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 복습&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;현재까지 $Θ(n\lg(n))$으로 동작하는 sorting algorithms 두 가지에 대해 알아보았다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;heap sort&lt;/strong&gt; : in-place에 동작함&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;merge sort&lt;/strong&gt; : heap sort보다 빠르지만 in-place에 동작하지 않음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이번에는 &lt;hl&gt;in-place에 가까우며 heap sort보다 속도가 빠른 Quick Sort 알고리즘&lt;/hl&gt;에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;quick-sort&quot;&gt;Quick Sort&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;hl&gt;in-place에 가깝다.&lt;/hl&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;hl&gt;평균적으로 Heap Sort Algorithm보다 속도가 빠르다.&lt;/hl&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;Complexity&lt;/hl&gt;는 다음과 같다.&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt;Case&lt;/th&gt;
          &lt;th&gt;Time&lt;/th&gt;
          &lt;th&gt;Memory&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;Best&lt;/td&gt;
          &lt;td&gt;$Θ(n\lg(n))$&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;Average&lt;/td&gt;
          &lt;td&gt;$Θ(n\lg(n))$&lt;/td&gt;
          &lt;td&gt;$Θ(\lg(n))$&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;Worst&lt;/td&gt;
          &lt;td&gt;$Θ(n^2)$&lt;/td&gt;
          &lt;td&gt;$Θ(n)$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

    &lt;gray&gt;※ Quick Sort에서는 최대한 데이터의 분포를 worst case가 안되도록 만드는 것이 핵심! ※&lt;br /&gt;
※ pivot값을 median값으로 고를 수 있다면 Best Case가 된다. ※&lt;br /&gt;
※ pivot값이 최소/최대값이라면 Worst Case가 된다. ⇒ $T(n)=T(n-1)+Θ(n)=Θ(n^2)$ ※&lt;/gray&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;Divide-and-Conquer&lt;/hl&gt; 방식으로 동작한다.&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt;Merge Sort&lt;/th&gt;
          &lt;th&gt;Quick Sort&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;middle point를 기준으로 왼쪽-오른쪽 array로 둘을 나눈다.&lt;/td&gt;
          &lt;td&gt;특정 값(pivot)을 기준으로 작은 값을 왼쪽, 큰 값을 오른쪽 array에 나눈다 (partitioning).&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

    &lt;p&gt;또한, Merge Sort와 마찬가지로 array size가 굉장히 작은 경우 partitioning을 수행하는 대신에 Insertion Sort와 같은 알고리즘을 수행하는 것이 일반적이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;median-of-three&quot;&gt;Median-of-Three&lt;/h3&gt;

&lt;p&gt;pivot값으로 median값을 선택하는 경우가 가장 이상적인 Case이다. 일반적으로 Quick Sort에서 median값을 지향하는 pivot 선택 방법은 &lt;hl&gt;Median-of-Three 방법&lt;/hl&gt;이다. Median-of-Three 방법은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;주어진 Array에서 첫 번째, 마지막, 가운데 위치의 값(총 3개)의 median값을 pivot으로 고른다.&lt;/li&gt;
  &lt;li&gt;pivot값을 기준으로 partitioning을 한다. ⇒ sub-array 생성&lt;/li&gt;
  &lt;li&gt;각각의 sub-array에 대해 첫 번째, 마지막, 가운데 위치의 값의 median값을 pivot으로 고른다.&lt;/li&gt;
  &lt;li&gt;각각의 sub-array에 대해 pivot값을 기준으로 partitioning을 한다.&lt;/li&gt;
  &lt;li&gt;3~4의 과정을 반복한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이때 in-place로 동작하기 위해 다음과 같이 위의 과정이 수행된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo10/image-20210903171423839.png&quot; alt=&quot;image-20210903171423839&quot; style=&quot;width:900px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● partitioning 과정&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;3개의 값(index=&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;first&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;middle&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;last&lt;/code&gt;) 중 pivot값(=median)을 구한 뒤 따로 값을 저장한다.&lt;/li&gt;
  &lt;li&gt;3개의 값 중 최소값은 index &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;first&lt;/code&gt;에, 최대값은 index &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;middle&lt;/code&gt; 위치에 저장한다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;first&lt;/code&gt;에서부터 오른쪽으로 값을 탐색하며 pivot값보다 큰 값을 찾는다.&lt;br /&gt;동시에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;last&lt;/code&gt;에서부터 왼쪽으로 값을 탐색하며 pivot값보다 작은 값을 찾는다.&lt;/li&gt;
  &lt;li&gt;pivot값을 기준으로 큰 값과 작은 값이 감지되면 두 값을 swap한다.&lt;/li&gt;
  &lt;li&gt;두 pointer가 만날 때 까지 3~4번의 과정을 반복한다.&lt;/li&gt;
  &lt;li&gt;반복 과정이 종료되면 두 개의 pointer 중 더 큰 값을 마지막 array index에 넣고, 빈 자리에 pivot값을 넣어준다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위의 partitioning과정을 재귀적으로 수행하면 아래와 같이 Quick Sort의 결과를 얻을 수 있다.&lt;/p&gt;

&lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css&quot; /&gt;

&lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;button type=&quot;button&quot; class=&quot;btn btn-info&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#demo&quot;&gt;과정 자세히 보기&lt;/button&gt;&lt;/p&gt;

&lt;div id=&quot;demo&quot; class=&quot;collapse&quot;&gt;
  &lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo10/image-20210903173927009.png&quot; alt=&quot;image-20210903173927009&quot; style=&quot;width:900px;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo10/image-20210903174249895.png&quot; alt=&quot;image-20210903174249895&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;code-implementation&quot;&gt;Code Implementation&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Quicksort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NUM_THRESHOLD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Insertion_Sort&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Find_Pivot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Quicksort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Quicksort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;memory-requirement&quot;&gt;Memory Requirement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pivot&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start_index&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;last_index&lt;/code&gt;를 stack에 저장해야한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서 &lt;hl&gt;Quicksort의 Memory Requirement는 $Θ(\lg(n))$&lt;/hl&gt;이다.&lt;br /&gt;한편, &lt;hl&gt;Worst-case에서는 memory requirment는 $Θ(n)$&lt;/hl&gt;가 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;run-time-summary&quot;&gt;Run-time Summary&lt;/h1&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Average Run Time&lt;/th&gt;
      &lt;th&gt;Worst-case Run Time&lt;/th&gt;
      &lt;th&gt;Average Memory&lt;/th&gt;
      &lt;th&gt;Worst-case Memory&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Heap Sort&lt;/td&gt;
      &lt;td&gt;$Θ(n\lg(n))$&lt;/td&gt;
      &lt;td&gt;$Θ(n\lg(n))$&lt;/td&gt;
      &lt;td&gt;$Θ(1)$&lt;/td&gt;
      &lt;td&gt;$Θ(1)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Merge Sort&lt;/td&gt;
      &lt;td&gt;$Θ(n\lg(n))$&lt;/td&gt;
      &lt;td&gt;$Θ(n\lg(n))$&lt;/td&gt;
      &lt;td&gt;$Θ(n)$&lt;/td&gt;
      &lt;td&gt;$Θ(n)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Quicksort&lt;/td&gt;
      &lt;td&gt;$Θ(n\lg(n))$&lt;/td&gt;
      &lt;td&gt;$Θ(n^2)$&lt;/td&gt;
      &lt;td&gt;$Θ(\lg(n))$&lt;/td&gt;
      &lt;td&gt;$Θ(n)$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 06 Sep 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo10/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo10/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-09] Lect9. 힙 자료구조의 정의와 연산 및 복잡도 분석</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이번 시간에는 &lt;hl&gt;힙 자료구조의 정의와 연산 및 복잡도 분석&lt;/hl&gt;에 대해 배운다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;힙-자료구조의-정의와-연산&quot;&gt;힙 자료구조의 정의와 연산&lt;/h1&gt;

&lt;p&gt;&lt;hl&gt;이진힙 자료구조&lt;/hl&gt;의 정의와 &lt;hl&gt;Push, Pop 연산 방법&lt;/hl&gt;에 대해 학습한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;binary-heap&quot;&gt;Binary Heap&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Binary Heap&lt;/b&gt;은 &lt;hl&gt;Tree에서 최소값, 최대값을 빠르게 찾을 수 있도록 하는 자료구조&lt;/hl&gt;이다. Binary Heap은 특성에 따라 다음과 같이 분류될 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Min Heap&lt;/strong&gt;: (sub) tree의 모든 root node가 자손보다 언제나 값이 작은 tree&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Max Heap&lt;/strong&gt;: (sub) tree의 모든 root node가 자손보다 언제나 값이 큰 tree&lt;/li&gt;
&lt;/ul&gt;

&lt;gray&gt;※ sibling끼리는 어떠한 관계성이 존재하지 않는다. ※&lt;/gray&gt;

&lt;p&gt;&lt;strong&gt;■ (예시) Binary Min Heap&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo09/image-20210902221741019.png&quot; alt=&quot;image-20210902221741019&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;operations&quot;&gt;Operations&lt;/h2&gt;

&lt;p&gt;&lt;hl&gt;Binary Heap에서 수행할 수 있는 연산들&lt;/hl&gt;은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Top&lt;/strong&gt;: 가장 root node의 값을 찾음&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pop&lt;/strong&gt;: root에 있는 값을 제거&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Push&lt;/strong&gt;: 어떤 임의의 값을 Heap의 특성을 깨지 않으면서 삽입&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;top&quot;&gt;Top&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;■ 방법&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;tree의 &lt;hl&gt;root node의 값에 접근&lt;/hl&gt;한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;pop&quot;&gt;Pop&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;■ 방법&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo09/image-20210902222851134.png&quot; alt=&quot;image-20210902222851134&quot; style=&quot;width:900px;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Root Node의 값을 제거 (그 결과, Root Node의 값이 비게 됨)&lt;/li&gt;
  &lt;li&gt;Root Node의 자식 Nodes 중 더 작은 값을 Root Node로 Promotion (그 결과, 자식 Node의 값이 비게 됨)&lt;/li&gt;
  &lt;li&gt;2번 과정을 반복&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;push&quot;&gt;Push&lt;/h3&gt;

&lt;p&gt;Heap에 어떤 값을 삽입하는 방법에는 크게 두 가지가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Leaf Node에 삽입하는 방법 (item이 적당한 위치를 찾을 때까지 올라가는 방법)&lt;/li&gt;
  &lt;li&gt;Root Node에 삽입하는 방법 (item이 적당한 위치를 찾을 때까지 내려가는 방법)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;우리는 이 중 일반적으로 자주 사용되는 &lt;hl&gt;① Leaf Node에 삽입하는 방법&lt;/hl&gt;에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 방법&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo09/image-20210902224039160.png&quot; alt=&quot;image-20210902224039160&quot; style=&quot;width:700px;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;아무 Leaf Node에 item을 삽입한다.&lt;/li&gt;
  &lt;li&gt;Binary Heap의 조건을 만족할 때까지 해당 item을 Promotion한다. (부모와 자리를 바꿈)&lt;/li&gt;
  &lt;li&gt;2번 과정을 반복하여 Binary Heap의 조건을 만족하도록 만든다.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;callout&quot;&gt;
    &lt;b&gt;Percolation(삼투압 작용)&lt;/b&gt;&lt;br /&gt;
    Binary Min Heap에서 Push를 할 때, 규칙에 의해 Node가 아래로 내려오지는 못하고 위로만 올라가는 현상을 의미
    (혹은 위로 올라가지는 못하고 밑으로만 내려가는 현상을 의미)
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;힙-자료구조의-구현-및-복잡도&quot;&gt;힙 자료구조의 구현 및 복잡도&lt;/h1&gt;

&lt;p&gt;&lt;hl&gt;완전 이진트리 기반의 힙 자료구조&lt;/hl&gt;의 구현 방법 및 &lt;hl&gt;각 연산의 평균/최악의 복잡도&lt;/hl&gt;에 대해 학습한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;perfect-binary-trees&quot;&gt;Perfect Binary Trees&lt;/h2&gt;

&lt;p&gt;Min Heap을 구현할 때, Perfect Binary Tree에 가깝도록 유지하면, 보다 효율적인 구현이 가능해진다. 따라서 Binary Heap을 구현하기에 앞서, &lt;hl&gt;Perfect Binary Tree에 대한 개념&lt;/hl&gt;을 알아본다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 정의&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo09/image-20210902224913408.png&quot; alt=&quot;image-20210902224913408&quot; style=&quot;width:700px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Binary Tree의 $\text{height}=h$일 때, &lt;hl&gt;모든 leaf nodes가 다음의 조건을 만족한다.&lt;/hl&gt;&lt;/p&gt;

\[\text{depth}=h\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 재귀적인 정의&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo09/image-20210902225246461.png&quot; alt=&quot;image-20210902225246461&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$h=0$인 경우: node가 1개만 있는 tree&lt;/li&gt;
  &lt;li&gt;$h≥1$인 경우: sub-tree가 모두 perfect binary tree여야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Perfect Binary Tree의 $\text{height}=h$일 때, &lt;hl&gt;nodes의 총 개수 $n$&lt;/hl&gt;은 다음과 같다.&lt;/p&gt;

\[n=2^h-1\]

&lt;p&gt;∵ 1, 3, 7, 15, 31, 63, …&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;complete-binary-trees&quot;&gt;Complete Binary Trees&lt;/h2&gt;

&lt;p&gt;min heap을 perfect binary tree에 가깝게 구현하면 좋지만, 데이터가 삽입/삭제되는 과정에 의해 nodes의 개수를 언제나 $n=2^h-1$에 맞출 수 없다. 따라서 &lt;hl&gt;perfect binary tree에 가까운 Complete Binary Tree 형태로 min heap을 구현하는 대안&lt;/hl&gt;을 떠올려볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo09/image-20210902225342423.png&quot; alt=&quot;image-20210902225342423&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Complete Binary Tree&lt;/b&gt;는 &lt;hl&gt;Perfect Binary Tree를 지향&lt;/hl&gt;하는, Perfect Binary Tree로 변해가는 과정이라 생각할 수 있다. 즉, &lt;hl&gt;왼쪽부터 Leaf Node를 채워 나가며 Perfect Binary Tree가 되도록하는 Tree 형태&lt;/hl&gt;를 Complete Binary Tree라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;hl&gt;Complete Binary Tree의 heigth $h$&lt;/hl&gt;는 다음과 같다.&lt;/p&gt;

\[h=[\log(n)]\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;operations-1&quot;&gt;Operations&lt;/h3&gt;

&lt;p&gt;&lt;hl&gt;Complete Binary Tree의 형태를 최대한 유지하며 연산을 수행하는 방법&lt;/hl&gt;에 대해 알아본다.&lt;/p&gt;

&lt;h4 id=&quot;push-1&quot;&gt;Push&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo09/image-20210911221817909.png&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;가장 왼쪽에 위치한 leaf node의 sibling node의 위치에 item을 삽입한다.&lt;/li&gt;
  &lt;li&gt;item이 적당한 자리를 찾을 때 까지 promotion한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;pop-1&quot;&gt;Pop&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo09/image-20210911222740861.png&quot; alt=&quot;image-20210911222740861&quot; style=&quot;width:400px;;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Root Node의 값을 제거 (그 결과, Root Node의 값이 비게 됨)&lt;/li&gt;
  &lt;li&gt;가장 마지막 leaf node의 값을 root node 자리에 채워 넣음&lt;/li&gt;
  &lt;li&gt;기존의 leaf node를 올바른 위치로 percolation down한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;heap의-구현&quot;&gt;Heap의 구현&lt;/h2&gt;

&lt;p&gt;&lt;hl&gt;Tree를 BFS order로 순회를 한 결과를 array에 채워나가는 방법으로 Heap를 구현&lt;/hl&gt;한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 예시&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo09/image-20210911223837945.png&quot; alt=&quot;image-20210911223837945&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;※ 편의상 array 0은 비웠다. (Trivial Cost)※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;hl&gt;자식 node에 접근하는 방법&lt;/hl&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo09/image-20210911223801704.png&quot; alt=&quot;image-20210911223801704&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

\[\begin{cases}
	\text{(왼쪽 자식 node의 index)} = \text{(본인 index)}*2\\
	\text{(오른쪽 자식 node의 index)} = \text{(본인 index)}*2 + 1
\end{cases}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;hl&gt;부모 node에 접근하는 방법&lt;/hl&gt;

\[\text{(부모 node의 index)} = [\text{(본인 index)}/2]\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;operations-2&quot;&gt;Operations&lt;/h3&gt;

&lt;h4 id=&quot;search&quot;&gt;Search&lt;/h4&gt;

&lt;p&gt;※ $\text{Time Complexity}=Θ(1)$ ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;pop-2&quot;&gt;Pop&lt;/h4&gt;

&lt;p&gt;※ $\text{Time Complexity}=Θ(\lg(n))$ (∵ 최악의 경우, height만큼 올라갔다 height만큼 내려온다.) ※&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;push-2&quot;&gt;Push&lt;/h4&gt;

&lt;p&gt;※ $\text{Time Complexity}=Θ(1) \sim Θ(\lg(n))$ (∵ 최선의 경우 움직이지 않고, 최악의 경우 heigth만큼 움직인다.)  ※&lt;/p&gt;

&lt;p&gt;※ 평균적인 Time Complexity는 다음과 같다. ※&lt;/p&gt;

\[\text{Time Complexity} = 
\frac{1}{n}\sum_{k=1}^{h}{(h-k)2^k} = 
\frac{2^{h+1}-h-2}{n} = 
\frac{n-h-1}{n} = Θ(1)\]

&lt;p&gt;따라서 &lt;hl&gt;Push는 평균적으로 Constant Time안에 수행&lt;/hl&gt;된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;remove&quot;&gt;Remove&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;해당 item이 존재하는지 여부를 판단하는 Time Complexity = $O(n)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;가장 큰 값을 갖는 item을 제거하는 Time Complexity = $O(n)$&lt;/p&gt;

    &lt;p&gt;※ 큰 item은 대부분 leaf level에 존재하므로 $\frac{n}{2}$만큼의 탐색 시간이 걸린다. ※&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;run-time-analysis&quot;&gt;Run-time Analysis&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Average&lt;/th&gt;
      &lt;th&gt;Worst&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Search&lt;/td&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Push&lt;/td&gt;
      &lt;td&gt;$O(1)$&lt;/td&gt;
      &lt;td&gt;$O(\lg(n))$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td&gt;$O(\lg(n))$&lt;/td&gt;
      &lt;td&gt;$O(\lg(n))$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Remove&lt;/td&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 06 Sep 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo09/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo09/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-07] Lect7. 삽입 정렬과 합병 정렬의 비교 분석을 통한 재귀적 알고리즘 복잡도 도출 방법</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이번 시간에는 &lt;hl&gt;삽입 정렬과 합병 정렬의 비교 분석을 통한 재귀적 알고리즘 복잡도 도출 방법&lt;/hl&gt;에 대해 배운다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;삽입-정렬-insertion-sort&quot;&gt;삽입 정렬, Insertion Sort&lt;/h1&gt;

&lt;p&gt;&lt;b&gt;Insertiong Sort&lt;/b&gt;란, 정렬된 array가 주어졌을 때, 새로운 element를 &lt;hl&gt;정렬 규칙이 깨지지 않도록 올바른 위치를 찾아서 삽입해주는 알고리즘&lt;/hl&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sorting-problem&quot;&gt;Sorting Problem&lt;/h2&gt;

&lt;p&gt;Sorting 문제는 다음과 같이 구성된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: A sequence of $n$ numbers $&amp;lt;a_1, a_2, …, a_n&amp;gt;$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: A permutation (reordering) of the input sequence, $&amp;lt;b_1, b_2, …, b_n&amp;gt;$, such that $b_1 ≤ b_2 ≤ … ≤ b_n$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;&lt;hl&gt;Swap 알고리즘을 사용하는 방식&lt;/hl&gt;과 &lt;hl&gt;사용하지 않는 방식&lt;/hl&gt;, 두 가지 방식으로 Insertion Sort Algorithm 코드를 작성해볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;insertion-sort-with-swap&quot;&gt;Insertion Sort with Swap&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Insertion_Sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;코드 설명:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo07/image-20210826172800334.png&quot; alt=&quot;image-20210826172800334&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Complexity:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Swap Operation&lt;/strong&gt; : $Θ(1)$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;for Loop (j)&lt;/strong&gt; : $O(1 + \sum_{j=0}^{i}{1}) = O(1+i) = O(i)$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;for Loop (i)&lt;/strong&gt; : $O(1 + \sum_{i=1}^{n}{i}) = O(1+\frac{n(n+1)}{2}) = O(n^2)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;결과적으로 Insertion Sort는 &lt;hl&gt;$O(n^2)$의 Time Complexity&lt;/hl&gt;를 갖는다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;insertion-sort-without-swap&quot;&gt;Insertion Sort without Swap&lt;/h3&gt;

&lt;p&gt;Swap Algorithm은 세 줄의 코드로 구성되어 있다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;Swap Algorithm:
	tmp = a;
	a = b;
	b = tmp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;이러한 &lt;hl&gt;Swap 알고리즘을 사용하지 않으면 보다 최적화된 알고리즘을 얻을 수 있다.&lt;/hl&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Insertion_Sort_without_Swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 오름차순이 아닌 경우&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;코드 설명&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo07/image-20210826174751795.png&quot; alt=&quot;image-20210826174751795&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;합병-정렬-merge-sort&quot;&gt;합병 정렬, Merge Sort&lt;/h1&gt;

&lt;p&gt;&lt;b&gt;Merge Sort Algorithm&lt;/b&gt;은 &lt;hl&gt;divide and conquer 알고리즘&lt;/hl&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;divide-and-conquer-알고리즘&quot;&gt;Divide and Conquer 알고리즘&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Divide and Conquer 알고리즘&lt;/b&gt;이란, &lt;hl&gt;문제를 쪼개어 각각을 따로 처리한 다음 결과를 하나로 합치는 알고리즘&lt;/hl&gt;을 말한다.&lt;/p&gt;

&lt;p&gt;Merge Sort 알고리즘에서는, 주어진 하나의 array를 반으로 나누어 각각을 sort한 다음 merge operation을 통해 합쳐준다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;merging-example&quot;&gt;Merging Example&lt;/h3&gt;

&lt;p&gt;아래의 예시를 통해 &lt;hl&gt;Merging을 수행하는 방법&lt;/hl&gt;에 대해 살펴보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo07/image-20210826190803166.png&quot; alt=&quot;image-20210826190803166&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;두 array의 포인터가 가리키는 index 값을 비교한다.&lt;/li&gt;
  &lt;li&gt;더 작은 값을 갖는 array의 값을 result array에 넣고, 해당 array의 포인터를 오른쪽으로 이동시킨다.&lt;/li&gt;
  &lt;li&gt;위 과정을 반복한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css&quot;&amp;gt;
&amp;lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script src=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;

&amp;lt;button type=&quot;button&quot; class=&quot;btn btn-info&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#demo&quot;&amp;gt;Code Run&amp;lt;/button&amp;gt;
&amp;lt;div id=&quot;demo&quot; class=&quot;collapse&quot;&amp;gt;
  &amp;lt;iframe src=&quot;https://code.sololearn.com/ct5m9F874CWD&quot; style=&quot;width:100%; heigth:100%; resize: vertical;&quot;&amp;gt;&amp;lt;/iframe&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;단점&quot;&gt;단점&lt;/h3&gt;

&lt;p&gt;Merging Algorithm은 &lt;hl&gt;in-place로 동작하지 않는다는 단점&lt;/hl&gt;이 있다. 즉, 병합하기 전에 언제나 병합된 결과를 담는 array를 memory allocation해야한다.&lt;/p&gt;

&lt;div class=&quot;callout&quot;&gt;
    &lt;b&gt;in-place란?&lt;/b&gt;&lt;br /&gt;
    새로운 메모리 없이 구동될 수 있는 알고리즘을 의미한다.&lt;br /&gt;
    &lt;hl&gt;in-place가 아닌 알고리즘은, 언제나 memory allocation을 해야만 한다.&lt;/hl&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;code-1&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo07/image-20210826195102050.png&quot; alt=&quot;image-20210826195102050&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;알고리즘은 대략 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;list를 절반 정도씩 쪼개어 두 개로 나눈다.&lt;/li&gt;
  &lt;li&gt;두 sub lists에 대해 merge sort를 재귀적으로 호출한다(Recursively call).&lt;/li&gt;
  &lt;li&gt;sorted lists 결과를 merge한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Problem: Recursively Call&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이론상으로는 sub-list를 하나의 element가 남을 때 까지 재귀적으로 merge sort하여 합치는 것이 맞다. 하지만 이러한 방식은 &lt;hl&gt;function call에 의해 발생하는 overhead 문제&lt;/hl&gt;가 있다. 따라서 실질적으로는 어떠한 threshold를 이용하여 &lt;hl&gt;threshold 이하의 element가 남을 때 다른 sorting algorithm(e.g., insertion sort)을 사용&lt;/hl&gt;하기도 한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;merge-sort&quot;&gt;Merge Sort&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Merge_Sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NUM_THRESHOLD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Insertion_Sort&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;midpoint&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;Merge_Sort&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;midpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;Merge_Sort&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;midpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;midpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Complexity&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;if절(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_last-_first&amp;lt;=NUM_THRESHOLD&lt;/code&gt;)&lt;/strong&gt; : $T(\text{NUM_THRESHOLD})=Θ(1)$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;else절&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int midpoint&lt;/code&gt;&lt;/strong&gt; : $Θ(1)$&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Merge_Sort()&lt;/code&gt;&lt;/strong&gt; : $T(\frac{n-1}{2})$&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Merge_Sort()&lt;/code&gt;&lt;/strong&gt; : $T(\frac{n-1}{2})$&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Merge()&lt;/code&gt;&lt;/strong&gt; : $Θ(n)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;hl&gt;Merge Sort의 Time Complexity $T$는 아래와 같이 recursive하게 나타낼 수 있다.&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;\(T(n) = 
\begin{cases}
	Θ(1) &amp;amp; \text{if $n=1$}\\
	2T(\frac{n}{2})+Θ(n) &amp;amp; \text{if $n&amp;gt;1$}
\end{cases}\)
위의 재귀적으로 표현된 Time Complexity는 &lt;hl&gt;recursion tree 기법을 이용하여 Asymptotic Analysis&lt;/hl&gt;를 할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Recursion Tree&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;hl&gt;Recursion Tree를 이용하여 Asymptotic Analysis를 하는 방법&lt;/hl&gt;은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;위의 재귀적으로 표현된 Time Complexity를 아래와 같이 표현한다.&lt;/p&gt;

\[T(n)=
\begin{cases}
	c &amp;amp; \text{if $n=1$}\\
	2T(\frac{n}{2})+cn &amp;amp; \text{if $n&amp;gt;1$}
\end{cases}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;위의 표현식을 recursion tree로 표현한다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-09-06-cs-algorithm-algo07/image-20210827011809452.png&quot; alt=&quot;image-20210827011809452&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;recursion tree의 모든 nodes의 합을 구한다.
\(T(n)=cn·\log(n)+cn\)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;결과적으로 &lt;hl&gt;Time Complexity가 $O(n^2)$인 Insertion Sort보다 $O(n\log(n))$인 Merge Sort 알고리즘이 더 효율적&lt;/hl&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 06 Sep 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo07/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo07/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-06] Lect6. 코드블록 단위의 복잡도 분석</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이번 시간에는 알고리즘에서 &lt;hl&gt;블록 단위의 시간/공간 복잡도를 분석하는 방법&lt;/hl&gt;에 대해 배운다.&lt;/p&gt;

&lt;h1 id=&quot;코드-블록-단위의-복잡도-분석&quot;&gt;코드 블록 단위의 복잡도 분석&lt;/h1&gt;

&lt;hl&gt;Instruction&lt;/hl&gt;
&lt;p&gt;의 개념을 배우고, Code Sequence가 있을 때 &lt;hl&gt;전체 Complexity를 표현하는 방법&lt;/hl&gt;에 대해 배운다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;알고리즘을 분석하는 목적은 코드 블록 단위로 다양한 파라미터에 대한 asymptotic run time 혹은 asymptotic memory requirements를 결정하기 위함이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Asymptotic Behavior of Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;알고리즘의 &lt;b&gt;Asymptotic Behavior&lt;/b&gt;이란, &lt;hl&gt;scale(n)에 따라 알고리즘이 어떻게 동작하는지에 관한 것&lt;/hl&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;● 예시&lt;/strong&gt;: 알고리즘 A, B에 대한 complexitiy가 각각 $f_A(n)=Θ(n^2)$, $f_B(n)=Θ(n\log_2n))$인 경우&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$n=2k$일 때&lt;/p&gt;

\[f_A(n) = (2k)^2 = 4k^2\\
f_B(n) = (2k)\log_2(2k) = 2k(\log_2(k)+\log_2(2)) = 2k\log(k)+2k\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$n=10k$일 때&lt;/p&gt;

\[f_A(n) = (10k)^2 = 100k^2\\
f_B(n) = (10k)\log_2(10k) = 10k(\log_2(k)+\log_2(10)) = 10k\log(k)+33.2k\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 예시를 통해 n이 5배가 될 때, $f_A$는 25배로 증가하고 $f_B$는 약 5배로 증가함을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;machine-instructions&quot;&gt;Machine Instructions&lt;/h2&gt;

&lt;div class=&quot;quote-box&quot;&gt;
    프로그램이 어떤 Instruction으로 구성되었는지를 알면 프로세서에 따라 어느정도의 시간이 소요되는지를 계산할 수 있다. 
&lt;/div&gt;

&lt;p&gt;프로세서(CPU,GPU)는 한정된 숫자의 연산(e.g., 덧셈, 뺄셈, 곱셈, 나눗셈)만을 수행할 수 있다. 이러한 연산을 &lt;b&gt;Instruction&lt;/b&gt;이라고 하며, &lt;hl&gt;Instruction set은 프로세서에 따라 다르다.&lt;/hl&gt; 따라서 프로그램을 컴파일 할 때는 꼭 타겟(e.g., ARM, x86)을 지정해주어야 machine이 이해할 수 있는 언어로 번역되어 동작가능해진다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;constant-수행시간을-갖는-연산&quot;&gt;Constant 수행시간을 갖는 연산&lt;/h3&gt;

&lt;p&gt;아래의 연산들은 Machine Instruction에 매핑되어있기 때문에 fixed number of cycle에 수행될 수 있다. 즉, Constant 수행시간$Θ(1)$)을 갖는다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Retieving/Storing variables from memory&lt;/strong&gt; : 메모리로부터 변수 검색 및 저장&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Variable assignment (=)&lt;/strong&gt; : 메모리에 특정값 할당&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Integer Operations (+, -, *, /, %, ++, –)&lt;/strong&gt; : 덧셈/뺄셈/곱셈/나눗셈 등의 산술 연산&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Logical Operations (&amp;amp;&amp;amp;, ││, !)&lt;/strong&gt;: AND/OR와 같은 논리 연산&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Bitwise Operations (&amp;amp;, │, ^, ~)&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Relational Operations (==, !=, &amp;lt;, &amp;lt;=, =&amp;gt;, &amp;gt;)&lt;/strong&gt; : 값의 비교&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Memory Allocation and Deallocation (new, delete)&lt;/strong&gt; : 메모리 할당&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시1. Swap Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 알고리즘은 3줄의 코드로 구성되어있지만 &lt;hl&gt;각각이 Θ(1)의 수행시간을 가지므로, 알고리즘이 Θ(1) 시간을 갖는다&lt;/hl&gt;고 말할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시2. AVL 트리의 노드 재정렬&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Tree_node&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lrl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Tree_node&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lrl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lrl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lrr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 알고리즘 역시 &lt;hl&gt;모든 코드가 Θ(1)의 수행시간을 가지므로 Θ(1)의 시간을 갖는 알고리즘&lt;/hl&gt;이라고 말할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;시간복잡도의-dominant-term&quot;&gt;시간복잡도의 Dominant Term&lt;/h3&gt;

&lt;p&gt;가령 어떤 알고리즘이 Θ(1)의 시간복잡도를 갖는 코드와 Θ(n)의 시간복잡도를 갖는 코드로 구성되어있을 때, 이 알고리즘은 Θ(1+n)=Θ(n)의 복잡도를 갖는다고 말한다. 즉, 다항식으로 구성된 시간복잡도는 &lt;hl&gt;dominant term으로 구성된 시간복잡도&lt;/hl&gt;라고 말할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시: Θ(n) 시간 복잡도를 갖는 알고리즘&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;Increase_Capacity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_old&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    
   	&lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_old&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 알고리즘은 위의 두 줄과 아래의 두 줄은 모두 Θ(1)의 시간복잡도를 갖지만, 3~4번째 줄(for문)이 machine instruction을 _n회 반복하므로 Θ(n)의 시간복잡도를 갖는다. 따라서 $Θ(1+n+1)=Θ(n)$의 시간복잡도를 갖는 알고리즘이라고 말할 수 있다.&lt;/p&gt;

&lt;p&gt;이전에 배웠던 &lt;hl&gt;Weak Ordering&lt;/hl&gt;을 이용하면 코드 시퀀스의 복잡도를 dominant term만으로 이용하여 설명할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;알고리즘-복잡도-분석&quot;&gt;알고리즘 복잡도 분석&lt;/h1&gt;

&lt;p&gt;프로그램 언어에서 사용하는 &lt;hl&gt;Control Statement&lt;/hl&gt;(e.g., if, for)에 대해 Complexity를 계산하는 방법에 대해 배운다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Control Statement의 종류&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Conditional Statements&lt;/strong&gt; : if, switch&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Condition-controlled Loops&lt;/strong&gt; : for, while, do-while&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Count-controlled Loops&lt;/strong&gt; : for i from 1 to 10 do … end do;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Collection-controlled Loops&lt;/strong&gt; : foreach (int i in array) {…}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conditional-statements&quot;&gt;Conditional Statements&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Conditional Statement&lt;/b&gt;의 대표적인 예로는 &lt;hl&gt;`if`, `switch`&lt;/hl&gt;가 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-29-cs-algorithm-algo06/image-20210824180909051.png&quot; alt=&quot;image-20210824180909051&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;hl&gt;Conditional Statement의 runtime&lt;/hl&gt;은 다음과 같이 구성된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;condition (test)&lt;/hl&gt;에 대한 runtime&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;실행될 body&lt;/hl&gt;에 대한 runtim&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;일반적인 condition(e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&amp;gt;5&lt;/code&gt;)에 대한 runtime은 Θ(1)이지만, &lt;hl&gt;condition에서 function call을 하는 경우는 constant runtime을 가지지 않을 수도 있다.&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시1. Factorial Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Factorial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Factorial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;_n=0인 조건을 제외하면 언제나 recursive하게 함수를 호출한다는 것을 명확하게 알 수 있다. 따라서 complexity를 비교적 쉽게 계산할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시2. Find Max Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Find_Max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxVal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;maxVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;// ·······ⓐ&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxVal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 코드에서 &lt;hl&gt;ⓐ 코드가 몇 번 실행되는지는 데이터 _array에 따라 달라진다.&lt;/hl&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;_array가 작은 것부터 큰 것까지 순서대로 정렬된 경우&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;for문을 돌 때마다 매번 최대값이 갱신되어 &lt;hl&gt;ⓐ가 언제나 실행된다.&lt;/hl&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;_array가 큰 것부터 작은 것까지 순서대로 정렬된 경우&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;_array[0]이 최대값이므로 &lt;hl&gt;ⓐ는 한 번도 실행되지 않는다.&lt;/hl&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;예시2와 같이 데이터의 분포에 따라 코드의 시행횟수가 달라지는 경우가 있기 때문에 이러한 변수까지 고려하여 알고리즘의 complexity를 계산하기는 어렵다. 따라서 &lt;hl&gt;이러한 경우는 최악의 경우(모든 데이터에 대해서 한 번씩 수행될 때)와 평균적인 경우(일반적인 데이터가 주어졌을때, 혹은 랜덤 분포를 갖는 데이터일 때)에 대해서 논할 수 있다.&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;switch&quot;&gt;Switch&lt;/h3&gt;

&lt;p&gt;switch문에서 case는 변수가 아닌 상수에 대해서만 프로그래밍이 가능하다. 따라서 미리 정의된 machine instruction에 의해, 바로 해당 값의 binary로 점프할 수 있도록 최적화 되어있기때문에 &lt;hl&gt;if-else문에 비해 switch문이 조금 더 빠르다.&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conditional-controlled-loops&quot;&gt;Conditional-controlled Loops&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Conditional-controlled Loop(반복문)&lt;/b&gt;의 대표적인 예로는 &lt;hl&gt;for, while, do-while&lt;/hl&gt;가 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-29-cs-algorithm-algo06/image-20210824182833365.png&quot; alt=&quot;image-20210824182833365&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Conditional-controlled Loop의 runtime은 다음과 같이 구성된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;Initialization&lt;/hl&gt;에 대한 runtime&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;Condition Check&lt;/hl&gt;에 대한 runtime&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;Increment&lt;/hl&gt;에 대한 runtime&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;for(i=0; i&amp;lt;n; i++)&lt;/code&gt;인 Conditional-controlled Loop에 대한 runtime은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;Initialization, Condition Check, Increment&lt;/b&gt;에 대한 runtime은 모두 Θ(1)이다.&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;body&lt;/b&gt;에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;break&lt;/code&gt;이나 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;return&lt;/code&gt;이 없다면, Conditional-controlled Loop는 Ω(n)의 runtime을 갖는다(즉, 최소 n이 소비됨).&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;body&lt;/b&gt;의 runtime이 Θ(f(n))이면, Loop는 Θ(nf(n))의 runtime을 갖는다.&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;body&lt;/b&gt;의 runtime이 O(f(n))이면, Loop는 O(nf(n))의 runtime을 갖는다 (즉, 최악의 경우 nf(n)이 소비됨)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시1.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// Θ(1)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 코드는 $Θ(n·1)=Θ(n)$의 run time을 갖는다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시2.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;// Θ(1)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 코드는 $Θ(n·n·1)=Θ(n^2)$의 run time을 갖는다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시3.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// search through an array of size m&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// O(m);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 코드는 $O(n·m)$의 run time을 갖는다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시4.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;// code which is Θ(f(i,n))}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 코드의 run time은 다음과 같다.&lt;/p&gt;

\[Θ(1 + \sum_{i=0}^{n-1}{(1+f(i,n))})\]

&lt;gray&gt;※ 첫번째 항은 initialization(i=0)에 대한 run time Θ(1)을 의미한다.&lt;/gray&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시5.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Θ(1)    } // Θ(1+i·1)} // Θ(1+SUMⁿ(1+i·1))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 코드의 run time은 다음과 같다.&lt;/p&gt;

&lt;p&gt;\(Θ(1+\sum_{i=0}^{n-1}{(1+i)}) \\
= Θ(1+n+\sum_{i=0}^{n-1}{i}) \\
= Θ(1+n+\frac{n(n-1)}{2}) \\
= Θ(n^2)\)
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;serial-statements&quot;&gt;Serial Statements&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;■ 예시1. 적합한 Notation&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\text{size}=n$인 random list로부터 maximum entry를 찾는 경우에는&lt;br /&gt;&lt;hl&gt;모든 값을 조사해야하므로 $\text{run time}=Θ(n)$&lt;/hl&gt;이다.&lt;/li&gt;
  &lt;li&gt;$\text{size}=n$인 random list로부터 특정값(particular entry)를 찾는 경우&lt;br /&gt;&lt;hl&gt;최악의 경우 모든 값을 조사해야하므로 $\text{run time}=O(n)$&lt;/hl&gt;이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시2. leading term 기준으로 Notation 결정&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$O(n) + O(n^2) + O(n^4) = O(n+n^2+n^4) = O(n^4)$&lt;/li&gt;
  &lt;li&gt;$O(n) + Θ(n^2) = Θ(n^2)$&lt;/li&gt;
  &lt;li&gt;$O(n^2) + Θ(n) = O(n^2)$&lt;/li&gt;
  &lt;li&gt;$O(n^2) + Θ(n^2) = Θ(n^2)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;코드에 따라서는 Notation이 섞여있을 수 있다. 이런 경우 &lt;hl&gt;leading term 기준으로 Notation을 표기&lt;/hl&gt;한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;functions&quot;&gt;Functions&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;■ 개념&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Function(혹은 subroutine)&lt;/b&gt;이란 &lt;hl&gt;별도로 분리되어 존재하는 코드&lt;/hl&gt;를 의미한다. 가령, mathmatical functions과 같이 &lt;hl&gt;반복적인 연산&lt;/hl&gt;을 하거나 initialization과 같이 &lt;hl&gt;연결된 tasks&lt;/hl&gt;를 모아서 함수로 만들 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Function Call&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Function Call을 하면, 어떤 프로그램의 binary로 점프를 하여 해당 부분을 수행한 뒤, 다시 본래의 프로그램 binary위치로 복귀하게 된다. 즉, &lt;hl&gt;Function Call의 과정&lt;/hl&gt;은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;함수를 구동하기 위한 적절한 환경을 구축한다.&lt;/li&gt;
  &lt;li&gt;함수에 넣어 줄 파라미터를 처리한다.&lt;/li&gt;
  &lt;li&gt;subroutine으로 점프한다.&lt;/li&gt;
  &lt;li&gt;subroutine을 실행한다.&lt;/li&gt;
  &lt;li&gt;return value를 처리한다.&lt;/li&gt;
  &lt;li&gt;clean up&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위의 일련의 과정은 어떻게보면 Overhead이지만, 최신 프로세서는 이러한 과정을 constant time에 해줄 수 있는 instruction을 가지고 있다. 따라서 &lt;hl&gt;function call에 대한 overhead는 Ω(1)이다 (즉, 최대 constant time)&lt;/hl&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 함수 안에서 발생하는 run time&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;함수 안에서 발생하는 run time은 $T_{f(n)}$ 혹은 $T(N)$이라고 표현한다.&lt;/p&gt;

&lt;gray&gt;※ 이때 N은 함수가 받아들이는 파라미터의 사이즈를 의미한다. ※&lt;/gray&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 29 Aug 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo06/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo06/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-05] Lect5. 함수의 점근적 분석</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이번 시간에는 주어진 함수를 점근적으로 분석하는 방법에 대해 배운다.&lt;/p&gt;

&lt;h1 id=&quot;함수의-점근적-분석법&quot;&gt;함수의 점근적 분석법&lt;/h1&gt;

&lt;p&gt;함수의 &lt;hl&gt;점근적 분석(Asymptotic Analysis)&lt;/hl&gt;과 이를 통한 함수들 사이의 &lt;hl&gt;점근적 대소 관계&lt;/hl&gt;에 대해 학습한다.&lt;/p&gt;

&lt;h2 id=&quot;배경&quot;&gt;배경&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;알고리즘을 직접 구현하여 비교하는 방식은 비용소모적이다.&lt;/li&gt;
  &lt;li&gt;점근적 분석법은 수학적으로 분석하여, &lt;hl&gt;두 알고리즘의 성능을 비교하기 위한 분석법&lt;/hl&gt;이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Motivation&lt;/strong&gt;: Linear Search vs. Binary Search&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Linear Search&lt;/strong&gt; : Array에서 특정 값을 찾고자 할 때, &lt;u&gt;앞에서부터 순차적으로 테스트&lt;/u&gt;하는 방법&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Binary Search&lt;/strong&gt; : 정렬된 Array에서 특정 값을 찾고자 할 때, 중간값과 찾고자하는 값을 비교하여 &lt;u&gt;범위를 좁혀가며 테스트&lt;/u&gt;하는 방법&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/files/2021-08-22-cs-algorithm-algo05/image-20210821164918636.png&quot; alt=&quot;image-20210821164918636&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그래프는 Array의 사이즈에 대한 Searching Algorithm의 연산량 그래프이다. Array Size가 증가함에 따라 Linear Search 알고리즘에 비해 Binary Search 알고리즘의 연산량이 더 적은 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;특징&quot;&gt;특징&lt;/h2&gt;

&lt;h3 id=&quot;quadratic-growth&quot;&gt;Quadratic Growth&lt;/h3&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/files/2021-08-22-cs-algorithm-algo05/image-20210821170237342.png&quot; alt=&quot;image-20210821170237342&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$f(n)=n^2$​, $g(n)=n^2-3n+2$​일 때, $N$​을 $(0,3)$​의 범위에서 관찰하면 $f(n)$​과 $g(n)$​ 그래프의 차이가 없지만,  $N$을 $(0,100)$​의 범위에서 관찰하면 두 그래프가 거의 비슷해진다.&lt;/p&gt;

&lt;p&gt;따라서 Asymptotic Analysis에서는 &lt;hl&gt;&lt;b&gt;N이 무한히 커졌을 때&lt;/b&gt; 두 알고리즘의 차이가 얼마나 심한지가 주요 관심사&lt;/hl&gt;이자 핵심이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;counting-instructions&quot;&gt;Counting Instructions&lt;/h3&gt;

&lt;div class=&quot;quote-box&quot;&gt;
	&lt;hl&gt;A보다 B 알고리즘이 더 느릴 때, 더 좋은 컴퓨터를 사용함으로써 B가 A보다 빨라질수도, 혹은 그렇지 않을 수도 있다.&lt;/hl&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;■ 두 알고리즘 모두 Leading Term이 $n^k$​인 경우&lt;/strong&gt; (Polynomial)&lt;/p&gt;

\[f(n)=a_kn^k + a_{k-1}n^{k-1} + ...\\
g(n)=b_kn^k + b_{k-1}n^{k-1} + ...\]

&lt;p&gt;$M=\frac{a_k}{b_k}+1$일 때, n이 충분히 큰 경우 다음이 언제나 성립한다.&lt;/p&gt;

\[f(n)&amp;lt;Mg(n)\]

&lt;p&gt;$f(n)$​​ 알고리즘을 구동시키는 컴퓨터보다 &lt;u&gt;$M$​​배 빠른 컴퓨터로 $g(n)$​​알고리즘을 구동시킨다면, $g(n)$​​알고리즘이 더 빠르게 동작&lt;/u&gt;한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 두 알고리즘의 승수가 다른 경우&lt;/strong&gt;&lt;/p&gt;

\[f(n)=a·n^2\\
g(n)=b·n·\log{(n)}\]

&lt;p&gt;g(n)은 &lt;u&gt;절대 f(n)보다 빨라질 수 없다.&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;weak-ordering&quot;&gt;Weak Ordering&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;■ Equivalent&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;아래와 같이 f(n)과 g(n)의 비율이 상수로 수렴한다면, f와 g는 &lt;strong&gt;동일한 복잡도&lt;/strong&gt;를 갖는다: &lt;strong&gt;$f \sim g$​​.&lt;/strong&gt;&lt;/p&gt;

\[\lim_{n→∞}{\frac{f(n)}{g(n)}}=c \ \ \ \ \text{where, 0&amp;lt;c&amp;lt;∞}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 대소관계&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;아래와 같이 f(n)과 g(n)의 비율이 0에 수렴한다면, g의 복잡도가 더 크다: &lt;strong&gt;$f &amp;lt; g$​​​​​.&lt;/strong&gt;&lt;/p&gt;

\[\lim_{n→∞}{\frac{f(n)}{g(n)}}=0\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;심볼-기반-함수의-점근적-바운드&quot;&gt;심볼 기반 함수의 점근적 바운드&lt;/h1&gt;

&lt;p&gt;함수의 점근적 바운드를 표현하는 &lt;hl&gt;5가지 심볼&lt;/hl&gt;에 대해 이해한다.&lt;/p&gt;

&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt;

&lt;p&gt;함수의 증가 양상을 다른 함수와의 비교로 표현하는 &lt;hl&gt;점근 표기법(Asymptotic Notation)은 대표적으로 다섯 가지 표기법&lt;/hl&gt;이 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Θ-Notation&lt;/strong&gt; (Theta Notation, 대문자 세타 표기법)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Big-O Notation&lt;/strong&gt; (Big O Notation, 대문자 O 표기법)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Big-Ω Notation&lt;/strong&gt; (Big Omega Notation, 대문자 오메가 표기법)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;o Notation&lt;/strong&gt; (O Notation, 소문자 o 표기법)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ω Notation&lt;/strong&gt; (Omega Notation, 소문자 오메가 표기법)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;θ-notation&quot;&gt;Θ-Notation&lt;/h3&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/files/2021-08-22-cs-algorithm-algo05/image-20210822124652179.png&quot; alt=&quot;image-20210822124652179&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 정의&lt;/strong&gt;&lt;/p&gt;

\[f(n)=Θ(g(n))\\
\text{$0≤c_1g(n)≤f(n)≤c_2g(n)$, for all $n≥n_0$}\]

&lt;p&gt;위 조건을 만족하는 $c_1, c_2$가 존재할 때, 다음과 같이 표현할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$f(n)$: $g(n)$과 &lt;hl&gt;같은 증가 속도&lt;/hl&gt;를 갖는다.&lt;/li&gt;
  &lt;li&gt;$f(n)$: $Θ(g(n))$에 속한다.&lt;/li&gt;
  &lt;li&gt;$g(n)$: $f(n)$에 대한 &lt;hl&gt;Aymptotically Tight Bound&lt;/hl&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시&lt;/strong&gt;&lt;/p&gt;

\[f(n)=Θ(g(n))\text{: a polynomial of degree k}\\
⇔ f(n)=Θ(n^k)\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$\frac{1}{2}n^2 - 3n = Θ(n^2)$&lt;/p&gt;

    &lt;p&gt;∵ $\frac{1}{14}n^2 ≤ \frac{1}{2}n^2 - 3n ≤ n^2$, for all $n≥7$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$an^2 + bn +c = Θ(n^2), a&amp;gt;0$​​​&lt;/p&gt;

    &lt;p&gt;∵ $\frac{a}{4}n^2 ≤ an^2 + bn +c ≤ \frac{7a}{4}n^2$, for all $n≥2\max{(\frac{│b│}{a}, \sqrt{\frac{│c│}{a}})}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;big-o-notation&quot;&gt;Big-O Notation&lt;/h3&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/files/2021-08-22-cs-algorithm-algo05/image-20210822125417266.png&quot; alt=&quot;image-20210822125417266&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 정의&lt;/strong&gt;&lt;/p&gt;

\[f(n)=O(g(n))\\
\text{$0≤f(n)≤cg(n)$, for all $n≥n_0$}\]

&lt;p&gt;위 조건을 만족하는 양의 실수 $c, n_0$​​가 존재할 때, 다음과 같이 표현할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$g(n)$: $f(n)$​​에 대한 &lt;hl&gt;Aymptotically Upper Bound&lt;/hl&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$f(n)=Θ(g(n)) ⇒ f(n)=O(g(n))$​&lt;/li&gt;
  &lt;li&gt;$O$: &lt;hl&gt;worst-case&lt;/hl&gt; running time을 bound하기에 적합하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$ 2n^2 + 8n - 2 = \text{$O(n^2)$, $O(n^3)$, $O(n^4)$, …}$​​​​&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;big-ω-notation&quot;&gt;Big-Ω Notation&lt;/h3&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/files/2021-08-22-cs-algorithm-algo05/image-20210822125811504.png&quot; alt=&quot;image-20210822125811504&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 정의&lt;/strong&gt;&lt;/p&gt;

\[f(n)=Ω(g(n))\\
\text{$0≤cg(n)≤f(n)$, for all $n≥n_0$}\]

&lt;p&gt;위 조건을 만족하는 양의 실수 $c, n_0$​​가 존재할 때, 다음과 같이 표현할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$g(n)$: $f(n)$​​에 대한 &lt;hl&gt;Aymptotically Lower Bound&lt;/hl&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$f(n)=Θ(g(n)) ⇒ f(n)=Ω(g(n))$​​&lt;/li&gt;
  &lt;li&gt;$Ω$: &lt;hl&gt;best-case&lt;/hl&gt; running time을 bound하기에 적합하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;o-notation&quot;&gt;o-Notation&lt;/h3&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/files/2021-08-22-cs-algorithm-algo05/image-20210822135612432.png&quot; alt=&quot;image-20210822135612432&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■정의&lt;/strong&gt;&lt;/p&gt;

\[f(n)=o(g(n))\\
\text{$0≤f(n)≤cg(n)$, for all $n≥n_0$ and all constant $c&amp;gt;0$}\]

&lt;p&gt;위 조건을 만족하는 양의 실수 $c, n_0$​​가 존재할 때, 다음과 같이 표현할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$g(n)$: $f(n)$​​​​​에 대한 &lt;hl&gt;Upper Bound이면서 Asymptotically Tight하지 않다&lt;/hl&gt;.&lt;/p&gt;

    &lt;gray&gt;※ Asymptotically Tight하다는 것은, &lt;b&gt;하한과 상한 경계가 모두 존재&lt;/b&gt;함을 의미한다. ※ &lt;/gray&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Big-O Notation은 Aysmptotically Tight할 수도, 안 할 수도 있지만
    &lt;hl&gt;Little-o Notation은 언제나 Aysmptotically Tight하지 않다.&lt;/hl&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$2n=O(n^2), o(n^2)$​​​​​&lt;/li&gt;
  &lt;li&gt;$2n^2=O(n^2)$  but $2n^2 ≠o(n^2)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ω-notation&quot;&gt;ω-Notation&lt;/h3&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/files/2021-08-22-cs-algorithm-algo05/image-20210822135529174.png&quot; alt=&quot;image-20210822135529174&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 정의&lt;/strong&gt;&lt;/p&gt;

\[f(n)=ω(g(n))\\
\text{$0≤f(n)≤cg(n)$, for all $n≥n_0$ and all constant $c&amp;gt;0$}\]

&lt;p&gt;위 조건을 만족하는 양의 실수 $c, n_0$​​가 존재할 때, 다음과 같이 표현할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$g(n)$: $f(n)$에 대한 &lt;hl&gt;Lower Bound이면서 Asymptotically Tight하지 않다.&lt;/hl&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ω-Notation은 Aysmptotically Tight할 수도, 안 할 수도 있지만
    &lt;hl&gt;ω-Notation은 언제나 Aysmptotically Tight하지 않다.&lt;/hl&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;■ 예시&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$2n^2=Ω(n), ω(n)$​​​&lt;/li&gt;
  &lt;li&gt;$2n^2=Ω(n)$​​​  but $2n^2 ≠ω(n)$​​​&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;정리&quot;&gt;정리&lt;/h3&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/files/2021-08-22-cs-algorithm-algo05/image-20210822140700223.png&quot; alt=&quot;image-20210822140700223&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;일반적으로는 tight boundary를 갖는 &lt;b&gt;Θ-Notation&lt;/b&gt;과 worst-case를 가정하는 &lt;b&gt;Big-O Notation&lt;/b&gt;을 주로 사용한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Notations Defined with Limit&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/files/2021-08-22-cs-algorithm-algo05/image-20210822142124620.png&quot; alt=&quot;image-20210822142124620&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

\[\begin{cases}
f(n)=o(g(n)) : \lim_{n→∞}{\frac{f(n)}{g(n)}} = 0\\
f(n)=O(g(n)) : \lim_{n→∞}{\frac{f(n)}{g(n)}} &amp;lt; ∞\\
f(n)=Θ(g(n)) : 0 &amp;lt; \lim_{n→∞}{\frac{f(n)}{g(n)}} &amp;lt; ∞\\
f(n)=Ω(g(n)) : 0 &amp;lt; \lim_{n→∞}{\frac{f(n)}{g(n)}}\\
f(n)=ω(g(n)) : \lim_{n→∞}{\frac{f(n)}{g(n)}} = ∞\\
\end{cases}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Transitivity&lt;/strong&gt;&lt;/p&gt;

\[\text{$f(n)=Θ(g(n))$ and $g(n)=Θ(h(n))$}\\
\text{⇔ $f(n)=Θ(h(n))$ for Θ, O, o, Ω, ω}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Reflexivity&lt;/strong&gt;&lt;/p&gt;

\[\text{$f(n)=Θ(f(n))$ for Θ, O, Ω}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Symmetry&lt;/strong&gt;&lt;/p&gt;

\[\text{$f(n)=Θ(g(n))$ ⇔ $g(n)=Θ(f(n))$}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Transpose Symmetry&lt;/strong&gt;&lt;/p&gt;

\[\text{$f(n)=Θ(g(n))$ ⇔ $g(n)=Ω(h(n))$}\\
\text{$f(n)=o(g(n))$ ⇔ $g(n)=ω(h(n))$}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;common-classes&quot;&gt;Common Classes&lt;/h2&gt;

&lt;p&gt;일반적으로 자주 사용되는 몇몇 Notation은 다음과 같이 이름붙여진다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$Θ(1)$ : constant&lt;/li&gt;
  &lt;li&gt;$Θ(\log(n))$​​ : logarithmic&lt;/li&gt;
  &lt;li&gt;$Θ(n)$​​ : linear&lt;/li&gt;
  &lt;li&gt;$Θ(n\log(n))$​ : n log n&lt;/li&gt;
  &lt;li&gt;$Θ(n^2)$​​ : quadratic&lt;/li&gt;
  &lt;li&gt;$Θ(n^3)$​​ : cubic&lt;/li&gt;
  &lt;li&gt;$Θ(2^n, e^n, 4^n, …)$​​ : exponential&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ Weak Ordering&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/files/2021-08-22-cs-algorithm-algo05/image-20210822143409082.png&quot; alt=&quot;image-20210822143409082&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

\[Θ(1) &amp;lt; Θ(\log(n)) &amp;lt; Θ(n) &amp;lt; Θ(n\log(n)) &amp;lt; Θ(n^2) &amp;lt; Θ(n^3) &amp;lt; Θ(4^n)\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;응용&quot;&gt;응용&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;n개의 floating point value들이 있는 list를 합하는 경우:&lt;/p&gt;

    &lt;p&gt;n번의 operation이 필요하므로 $Θ(n)$ algorithm&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;run-time이 $O(n^d)$인 경우, polynomial time complexity를 갖는다고 말할 수 있다.&lt;/p&gt;

    &lt;gray&gt;※ 일반적으로, polynomial time complexity를 가지면 Efficient(효율적)이다. ※&lt;/gray&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;polynomial-time을 갖지 않는다고 알려진 (혹은 모르는) algorithms을 Intractable하다고 한다.&lt;/p&gt;

    &lt;gray&gt;e.g., : Traveling salesman problem(TSP)는 아직 polynomial-tim을 갖는 알고리즘이 알려지지 않았다. 지금까지 알려진 best algorithm은 $Θ(n^2 2^n)$이다.&lt;/gray&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 22 Aug 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo05/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo05/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-04] Lect4. 그래프 탐색 알고리즘: DFS, BFS</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-14-cs-algorithm-algo04/image-20210814144512492.png&quot; alt=&quot;image-20210814144512492&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 시간에는 주어진 &lt;hl&gt;그래프에서 노드를 순회(traversal)하는 방법&lt;/hl&gt;에 대해 배운다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-그래프-탐색-dfs와-bfs&quot;&gt;1. 그래프 탐색: DFS와 BFS&lt;/h1&gt;

&lt;h2 id=&quot;graph-traversalsearch-그래프-순회&quot;&gt;Graph Traversal(Search), 그래프 순회&lt;/h2&gt;

&lt;p&gt;&lt;hl&gt;그래프 순회&lt;/hl&gt;란, 그래프의 &lt;hl&gt;각 vertex 하나씩 방문하는 것&lt;/hl&gt;을 말한다. 대표적인 그래프 순회 방식은 아래와 같이 크게 두 가지로 나눌 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Breadth-First Search, BFS: 너비 우선 순회&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-08-14-cs-algorithm-algo04/image-20210804162950665.png&quot; alt=&quot;image-20210804162950665&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Depth-First Search, DFS: 깊이 우선 순회&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-08-14-cs-algorithm-algo04/image-20210804163515713.png&quot; alt=&quot;image-20210804163515713&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;BFS와 DFS은 유사한 알고리즘을 공유하지만 &lt;hl&gt;BFS는 queue을, DFS는 stack를 사용&lt;/hl&gt;하여 알고리즘이 구현된다는 점에서 차이가 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;대략적인 알고리즘 요약&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;하나의 vertex를 선택한 뒤, &lt;hl&gt;visit이라 표시하고 queue(혹은 stack)에 삽입&lt;/hl&gt;한다.&lt;/li&gt;
  &lt;li&gt;queue(혹은 stack)이 빌 때까지 아래 동작을 반복한다.
    &lt;ul&gt;
      &lt;li&gt;queue(혹은 stack)에서 &lt;hl&gt;하나의 vertex를 꺼낸다.&lt;/hl&gt;&lt;/li&gt;
      &lt;li&gt;꺼낸 vertex와 &lt;hl&gt;인접한 vertex 중, visit이라 표시되지 않은 vertex를 queue(혹은 stack)에 삽입&lt;/hl&gt;한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;callout&quot;&gt;이때, queue(혹은 stack)이 비었는데 &lt;b&gt;not visited 상태&lt;/b&gt;인 vertex가 존재한다면, 해당 그래프는 &lt;b&gt;unconnnected graph&lt;/b&gt;라고 판별할 수 있다.&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;bfs-알고리즘&quot;&gt;BFS 알고리즘&lt;/h2&gt;

&lt;p&gt;아래 예시를 통해 &lt;b&gt;BFS 알고리즘&lt;/b&gt;을 살펴본다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-14-cs-algorithm-algo04/image-20210804170717830.png&quot; alt=&quot;image-20210804170717830&quot; style=&quot;width:700px;&quot; /&gt;&lt;/p&gt;

&lt;div style=&quot;text-align:center; font-weight:bold;&quot;&gt;최종 순회 순서: A-B-C-E-D-F-G-H-I&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;BFS 알고리즘&lt;/b&gt;은 &lt;hl&gt;Queue를 이용하여 visited vertices를 push &amp;amp; pop&lt;/hl&gt;한다. (그래프를 tree구조로 해석했을 때) FIFO 성질에 의해 같은 부모를 공유하는 sibling 순서대로 순회함을 알 수 있다. 이러한 논리에 따라 BFS 알고리즘을 구현할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dfs-알고리즘&quot;&gt;DFS 알고리즘&lt;/h2&gt;

&lt;p&gt;BFS 알고리즘이 queue를 사용하여 FIFO 순서로 그래프를 순회하였다면, DFS 알고리즘은 stack을 사용하여 LIFO 순서로 그래프를 순회한다.&lt;/p&gt;

&lt;p&gt;아래 예시를 통해 DFS 알고리즘을 살펴보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-14-cs-algorithm-algo04/image-20210804172421604.png&quot; alt=&quot;image-20210804172421604&quot; style=&quot;width:700px;&quot; /&gt;&lt;/p&gt;

&lt;div style=&quot;text-align:center; font-weight:bold;&quot;&gt;최종 순회 순서: A-B-D-C-F-E-G-H-I&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;DFS 알고리즘&lt;/b&gt;은 &lt;hl&gt;Stack를 이용하여 visited vertices를 push &amp;amp; pop&lt;/hl&gt;한다. (그래프를 tree구조로 해석했을 때)  LIFO 성질에 의해 자식 vertex를 향해 깊어지는 방향으로 순회함을 알 수 있다. 이러한 논리에 따라 DFS 알고리즘을 구현할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;bfs와-dfs&quot;&gt;BFS와 DFS&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;특징&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;BFS와 DFS는 인접 vertex를 삽입하는 순서에 따라 최종 순회 순서가 달라진다. &lt;hl&gt;즉, ordering은 unique하지 않다&lt;/hl&gt;는 특징이 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;응용&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;BFS 혹은 DFS 알고리즘을 이용하여 그래프로부터 connected component를 구할 수 있다. &lt;hl&gt;&quot;box가 비었음에도 불구하고 visited 마크되지 않은 vertex가 존재한다면, 해당 그래프는 unconnected graph라는 특성&quot;&lt;/hl&gt;을 이용하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-그래프-탐색의-stl-활용-구현&quot;&gt;2. 그래프 탐색의 STL 활용 구현&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-14-cs-algorithm-algo04/image-20210812163243171.png&quot; alt=&quot;image-20210812163243171&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 그래프를 BFS와 DFS 알고리즘을 이용하여 정렬하는 코드를 작성해보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;bfs-및-dfs-구현&quot;&gt;BFS 및 DFS 구현&lt;/h2&gt;

&lt;script src=&quot;https://gist.github.com/dazory/3cddc72a607073696b6966dd4e3fdc49.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;실행 결과&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./main.exe graph.txt 
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Graph/Init] start
&lt;span class=&quot;c&quot;&gt;# of vertices=13, # of edges = 12&lt;/span&gt;
... edges[A] : B
... edges[A] : H
... edges[B] : C
... edges[B] : E
... edges[C] : D
... edges[E] : F
... edges[E] : G
... edges[H] : I
... edges[H] : M
... edges[I] : J
... edges[I] : K
... edges[I] : L
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Graph/Init] end

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;BFS] start
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt; The result of Breadth-First Searching &lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
A B H C E I M D F G J K L
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;BFS] end

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;DFS] start
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt; The result of Depth-First Searching &lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
A B C D E F G H I J K L M
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;DFS] end
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sat, 14 Aug 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo04/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo04/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[CS-01] Lect1. 컴퓨터구조 개요</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: 컴퓨터구조&lt;br /&gt;
        교수: 상명대학교 전자공학과 박병수 교수님, 상명대학교 시스템반도체공학과 홍대길 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SMUCk+CK.SMUC03k+2017_T6/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SMUCk+CK.SMUC03k+2017_T6/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-computer-system-구조&quot;&gt;1. Computer System 구조&lt;/h1&gt;

&lt;h2 id=&quot;전반적-구조&quot;&gt;전반적 구조&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-01-cs-computerStructure-kmooc01/image-20210805150605581.png&quot; alt=&quot;image-20210805150605581&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;구성요소 설명&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt;, Central Processing Unit, 중앙 처리 장치&lt;br /&gt;
:&lt;hl&gt;산술/논리연산 및 제어&lt;/hl&gt;를 수행하는 부품&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Main Memory&lt;/strong&gt;&lt;br /&gt;
:주로 &lt;hl&gt;RAM(Random Access Memory)&lt;/hl&gt;이 이에 해당한다.&lt;br /&gt;&lt;gray&gt;※ 일반적으로 ROM은 main memory에 포함되지 않음 ※&lt;/gray&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;SSD&lt;/strong&gt;, Solid State Drive, Secondary Storage Device, &lt;hl&gt;보조 저장 장치&lt;/hl&gt;&lt;br /&gt;
:e.g.,  하드디스크, CD ROM, ...&lt;br /&gt;&lt;gray&gt;※ SSD는 I/O의 일부로 분류하기도 함 ※&lt;br /&gt;※ Main Memory와 SSD 모두 데이터를 저장하는 기능을 가졌으나 쓰임이 다르다.&lt;/gray&gt;

&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;I/O&lt;/strong&gt;, Input/Output Device&lt;br /&gt;
:e.g., Input device: 키보드, 마우스&lt;br /&gt;
 e.g., Ouput device: 모니터, 프린터&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;System Bus&lt;/strong&gt;&lt;br /&gt;
:위와 같은 부품들을 연결한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;cpu내부&quot;&gt;CPU내부&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt;의 내부는 크게 세 가지 sub-block으로 구성되어 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;ALU&lt;/strong&gt;, Arithmetic Logic Unit, 산술 논리 장치&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Register Set&lt;/strong&gt;&lt;br /&gt;
:&lt;hl&gt;데이터를 임시로 저장&lt;/hl&gt;하는 register의 집합&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Control Unit&lt;/strong&gt;&lt;br /&gt;
:&lt;hl&gt;제어 신호를 전송&lt;/hl&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이때 위 세 가지 sub-block을 연결하는 경로를 Internal Bus라고 한다.&lt;br /&gt;
&lt;gray&gt;※ system bus와 다르다 ※&lt;/gray&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-01-cs-computerStructure-kmooc01/image-20210805152002678.png&quot; alt=&quot;image-20210805152002678&quot; style=&quot;width:700px;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;alu-산술-논리-장치&quot;&gt;ALU, 산술 논리 장치&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;ALU&lt;/strong&gt;는 &lt;hl&gt;산술 연산을 수행하는 AU&lt;/hl&gt;와 &lt;hl&gt;논리 연산을 수행하는 LU&lt;/hl&gt;로 구성되어 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;AU, Arithmatic Unit&lt;/strong&gt;: +, -, *, %&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LU, Logic Unit&lt;/strong&gt;: OR, AND, NOT, EX-OR&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;register-set&quot;&gt;Register Set&lt;/h3&gt;

&lt;p&gt;크게 &lt;hl&gt;일반목적 Register&lt;/hl&gt;와 &lt;hl&gt;특수목적 Register&lt;/hl&gt;로 구성된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;일반 목적의 Register&lt;/strong&gt;: &lt;hl&gt;임시로 데이터를 저장&lt;/hl&gt;하는 장치&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Shift Register&lt;/strong&gt;: Bit의 자리를 옮김&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Status Register&lt;/strong&gt;: 현재 상태를 저장&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;특수목적 Register&lt;/strong&gt;: &lt;hl&gt;특수한 목적&lt;/hl&gt;을 갖는 저장 장치&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;ACC, Accumulator&lt;/strong&gt;: &lt;hl&gt;처리할 데이터가 누적&lt;/hl&gt;되는 저장 장치 (연산의 최종 결과가 저장됨)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;SP, Stack Pointer&lt;/strong&gt;: 개념 난해. 나중에 알려드림&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;PC, Program Counter&lt;/strong&gt;: &lt;hl&gt;실행할 프로그램이 저장되어있는 메모리 주소&lt;/hl&gt;가 저장됨&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;MAR, Memroy Adrress Register&lt;/strong&gt;: &lt;hl&gt;Address bus로 전송될 주소값&lt;/hl&gt;이 저장됨&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;MBR, Memory Buffer Register&lt;/strong&gt;: &lt;hl&gt;Data bus로부터 받은 데이터&lt;/hl&gt;가 저장됨&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;IR, Instruction Register&lt;/strong&gt;: MBR에 저장된 데이터가 명령인 경우, IR에 해당 &lt;hl&gt;명령&lt;/hl&gt;이 저장됨&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;control-unit&quot;&gt;Control Unit&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;ID, Instruction Decoder&lt;/strong&gt;&lt;br /&gt;
:&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Register Set&lt;/code&gt;의 &lt;hl&gt;`IR(Instruction Register)`에 저장된 데이터&lt;/hl&gt;가 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Control Unit&lt;/code&gt;의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ID(Instruction Decoder)&lt;/code&gt;로 전달됨.&lt;br /&gt;
 명령어로 구성된 프로그램을 기계가 이해할 수 있도록 &lt;hl&gt;디코딩하여 번역&lt;/hl&gt;하는 역할&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;CAR, Control Address Register&lt;/strong&gt;&lt;br /&gt;
:&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ID&lt;/code&gt;에서 디코딩의 결과로 출력된 &lt;hl&gt;'μ-명령어의 주소'&lt;/hl&gt;가 저장됨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;CM, Control Memory&lt;/strong&gt; (μ-instruction)&lt;br /&gt;
:&amp;lt;p&amp;gt;&lt;hl&gt;'μ-명령어로 이루어진 μ-프로그램'&lt;/hl&gt;이 저장된 기억장치&amp;lt;/p&amp;gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;CBR, Control Buffer Register&lt;/strong&gt;&lt;br /&gt;
:&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CM&lt;/code&gt;으로부터 읽은 &lt;hl&gt;'μ-명령어'를 저장 및 control bus로 전달&lt;/hl&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot;&gt;
    &lt;b&gt;μ-instruction&lt;/b&gt;&lt;br /&gt;
    사람이 짠 프로그램을 컴파일하면 기계어가 출력되는데, &lt;hl&gt;ID에 의해 번역된 주소가 CM을 거쳐 μ-instruction(마이크로 명령어)가 된다.&lt;/hl&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;callout&quot;&gt;
    &lt;b&gt;S/W&lt;/b&gt;&lt;br /&gt;
    S/W는 아래와 같이 크게 System S/W, Application S/W, F/W로 구성된다.&lt;br /&gt;
    &lt;ul&gt;
    &lt;li&gt;&lt;b&gt;System S/W&lt;/b&gt;: e.g.,window, linux, ...&lt;/li&gt;
    &lt;li&gt;&lt;b&gt;Application S/W&lt;/b&gt;: e.g., Adobe Photoshop, ...&lt;br /&gt;&lt;/li&gt;
    &lt;li&gt;&lt;b&gt;F/W&lt;/b&gt;: H/W를 구동하기 위한 μ-instruction으로 구성됨&lt;/li&gt;
    &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;system-bus&quot;&gt;System Bus&lt;/h2&gt;

&lt;p&gt;System bus는 컴퓨터 시스템의 주요 구성 요소를 연결하여 신호를 전달한다. 아래와 같이 크게 세 가지 bus로 구성된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Address Bus&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;hl&gt;CPU→memory(단방향)&lt;/hl&gt;으로 &lt;hl&gt;주소값&lt;/hl&gt;을 전달 (Unidirectional)&lt;/p&gt;

    &lt;div class=&quot;callout&quot;&gt;
	&lt;b&gt;Address Bus에서 width의 의미&lt;/b&gt;&lt;br /&gt;
	Address Bus의 폭은 &lt;hl&gt;최대 Memory Capacity를 결정&lt;/hl&gt;한다. 가령 Address Bus가 2Bits로 구성된 경우, 2²개의 주소를 만들 수 있다(00, 01, 10, 11). 13Bits로 구성된 Address Bus는 2¹³=8K개의 주소를 만들 수 있다.
&lt;/div&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Bus&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;hl&gt;데이터값&lt;/hl&gt;을 전달 (Bidirectional)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;CPU→memory : 연산 결과를 메모리에 저장하는 경우&lt;/li&gt;
      &lt;li&gt;memory → CPU : 메모리의 데이터를 CPU로 가져오는 경우&lt;/li&gt;
    &lt;/ul&gt;

    &lt;div class=&quot;callout&quot;&gt;
    &lt;b&gt;Data Bus에서 width의 의미&lt;/b&gt;&lt;br /&gt;
    가령 Data Bus의 Width가 32Bits인 경우, 한 번에 32Bits만큼씩 데이터 전송이 가능하다.
&lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Control Bus&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;hl&gt;명령어 시퀀스&lt;/hl&gt;를 전달 (Bidirectional)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;CPU→memory : &lt;hl&gt;READ/WRITE&lt;/hl&gt;와 같은 명령어 시퀀스를 전달하는 경우&lt;/li&gt;
      &lt;li&gt;memory→CPU : 주변 장치가 CPU에게 &lt;hl&gt;interrupt&lt;/hl&gt;를 걸며 제어 신호를 전달하는 경우&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-memory-ssd-io의-구성&quot;&gt;Main Memory, SSD, I/O의 구성&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-computerStructure-kmooc01/image-20210805205557246.png&quot; alt=&quot;image-20210805205557246&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;main-memory&quot;&gt;Main Memory&lt;/h3&gt;

&lt;p&gt;Main memory는 아래와 같이 세 가지로 구성된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Addressing Mode&lt;/strong&gt;, 주소 지정 방식&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;명령어에서 &lt;hl&gt;피연산자(Operand)의 주소가 지정되는 방식&lt;/hl&gt;을 지정한다.&lt;br /&gt;이에 따라 여러가지 형태로 데이터를 가져오게 된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;RAM, Random Access Memory&lt;/strong&gt; (READ/WRITE)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;주 메모리(또는 기본 메모리)라고도 불린다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;전원이 꺼지면 데이터가 손실되는 &lt;hl&gt;휘발성 메모리&lt;/hl&gt;이다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;m-instruction(machine instruction)들이 저장되어 있다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;ROM, Read Only Memory&lt;/strong&gt; (READ)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;필수 프로그램과 같은 &lt;hl&gt;시스템 운영에 필수적인 정보를 저장&lt;/hl&gt;한다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;항상 데이터를 유지하는 &lt;hl&gt;비휘발성 메모리&lt;/hl&gt;이다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;m-instruction(machine instruction)들이 저장되어 있다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;cache&quot;&gt;Cache&lt;/h4&gt;

&lt;p&gt;Main memory에 저장된 데이터 중 &lt;hl&gt;빠른 시일 내에 CPU가 읽을만한 데이터를 저장&lt;/hl&gt;한다.
CPU는 데이터를 읽기 위해 Main memory에 접근하기 전에, Cache를 먼저 거친다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;반도체를 이용하여 만들기 때문에 RAM/ROM에 비해 &lt;hl&gt;빠르다.&lt;/hl&gt;&amp;gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ssd-solid-state-drive&quot;&gt;SSD, &lt;strong&gt;S&lt;/strong&gt;olid &lt;strong&gt;S&lt;/strong&gt;tate &lt;strong&gt;D&lt;/strong&gt;rive&lt;/h3&gt;

&lt;p&gt;SSD는 &lt;hl&gt;고형 상태의 보조 기억 장치&lt;/hl&gt;로, Disk, CD-ROM이 이에 해당한다.&lt;/p&gt;

&lt;p&gt;main memory는 system bus에 직접 연결된 반면, SSD는 &lt;hl&gt;SSD controller를 거쳐 system bus에 연결&lt;/hl&gt;된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SSD가 System bus와 통신하는 방법&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;SSD의 데이터를 &lt;hl&gt;SSD Controller&lt;/hl&gt;에 임시로 가져온 뒤, system bus에 싣는다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main Memory vs. SSD&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;비교&lt;/th&gt;
      &lt;th&gt;Main Memory&lt;/th&gt;
      &lt;th&gt;SSD&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Access 방식&lt;/td&gt;
      &lt;td&gt;표준이 있어 CPU가 register를 통해 &lt;hl&gt;직접 접근&lt;/hl&gt;&lt;/td&gt;
      &lt;td&gt;CPU가 &lt;hl&gt;별도의 Controller&lt;/hl&gt;를 통해 접근&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Device 종류&lt;/td&gt;
      &lt;td&gt;RAM, ROM, Cache&lt;/td&gt;
      &lt;td&gt;Magnetic Disk, Optical Disk, RAID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;특징&lt;/td&gt;
      &lt;td&gt;● 빠른 속도&lt;br /&gt;● 비싼 가격&lt;br /&gt;● 용량대비 면적이 큼&lt;br /&gt;● 휘발성&lt;/td&gt;
      &lt;td&gt;● 느린 속도&lt;br /&gt;● 저렴한 가격&lt;br /&gt;● 용량대비 면적이 작음&lt;br /&gt;● 비휘발성&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;io-inputoutput-device&quot;&gt;I/O, Input/Output Device&lt;/h3&gt;

&lt;p&gt;Input/Output device는 SSD와 마찬가지로 System bus와의 통신을 담당하는 &lt;hl&gt;I/O Controller&lt;/hl&gt;가 존재한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I/O vs. SSD&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;공통점&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CPU입장에서 I/O와 SSD는 차이가 없다.&lt;/li&gt;
  &lt;li&gt;둘 다 &lt;hl&gt;Device Controller&lt;/hl&gt;를 통해 System bus에 접근할 수 있다.&lt;/li&gt;
  &lt;li&gt;Controller 내부에 존재하는 &lt;hl&gt;Status Register와 Data Register에 별도의 Address가 할당&lt;/hl&gt;되어 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;차이점&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;hl&gt;I/O(keyboard, printer..)는 bytes(8Bits) 단위&lt;/hl&gt;
    &lt;p&gt;로 전송하지만&lt;/p&gt;
    &lt;hl&gt;SSD는 block(512/ 1024/ 4096 Bytes) 단위&lt;/hl&gt;
    &lt;p&gt;로 전송한다.&lt;/p&gt;

    &lt;p&gt;∴ SSD는 Controller 내에 한 block 이상을 임시 저장할 수 있는 &lt;hl&gt;Data Buffer&lt;/hl&gt;가 존재한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-컴퓨터-구성품의-연결&quot;&gt;2. 컴퓨터 구성품의 연결&lt;/h1&gt;

&lt;h2 id=&quot;cpu와-컴퓨터-구성품의-통신-과정&quot;&gt;CPU와 컴퓨터 구성품의 통신 과정&lt;/h2&gt;

&lt;p&gt;CPU와 컴퓨터 구성품이 통신하는 방법에 대해서 알아본다. 이러한 연결 방식은 크게 아래와 같이 네 가지로 구분할 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Memory read&lt;/li&gt;
  &lt;li&gt;memory write&lt;/li&gt;
  &lt;li&gt;I/O read&lt;/li&gt;
  &lt;li&gt;I/O write&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;memory-read&quot;&gt;Memory Read&lt;/h3&gt;

&lt;p&gt;Main Memory로부터 데이터를 읽는 과정은 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-computerStructure-kmooc01/image-20210805213526887.png&quot; alt=&quot;image-20210805213526887&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CPU에서 main memory에 있는 데이터를 읽어오는 과정&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;CPU → Main Memory&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;CPU → Address Bus → Cache → (Main Memory)&lt;/p&gt;

        &lt;p&gt;메모리의 어느 주소를 읽을지에 대한 &lt;hl&gt;주소 정보(Address)&lt;/hl&gt;가 전달된다.&lt;br /&gt;
&lt;gray&gt;※ Cache에 원하는 데이터가 있는 경우 Main Memory까지 신호가 전달되지 않음 ※&lt;/gray&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;CPU → Control Bus → Cache → (Main Memory)&lt;/p&gt;

        &lt;p&gt;메모리로부터 데이터를 읽을 것이라는 &lt;hl&gt;READ 신호&lt;/hl&gt;가 전달된다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;CPU ← Main Memory&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;hl&gt;주소를 읽은 결과에 해당하는 데이터&lt;/hl&gt;
        &lt;p&gt;가 Data Bus를 통해 CPU로 전달된다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;신호선 타이밍도&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-computerStructure-kmooc01/image-20210805213623171.png&quot; alt=&quot;image-20210805213623171&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;memory-write&quot;&gt;Memory Write&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-computerStructure-kmooc01/image-20210805213813276.png&quot; alt=&quot;image-20210805213813276&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CPU에서 Main Memory로 데이터를 작성하는 과정&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;CPU → Main Memory&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;CPU → Address Bus → Main Memory : &lt;hl&gt;Address&lt;/hl&gt;가 전달&lt;/li&gt;
      &lt;li&gt;CPU → Data Bus → Main Memory : &lt;hl&gt;연산할 Data&lt;/hl&gt;가 전달&lt;/li&gt;
      &lt;li&gt;CPU → Control Bus → Main Memory : &lt;hl&gt;WRITE enable 신호&lt;/hl&gt;가 전달&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;신호선 타이밍도&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-computerStructure-kmooc01/image-20210805214411327.png&quot; alt=&quot;image-20210805214411327&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;io-read&quot;&gt;I/O Read&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-computerStructure-kmooc01/image-20210805214707218.png&quot; alt=&quot;image-20210805214707218&quot; style=&quot;width:600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I/O Device Controller&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I/O Device Controller 내부에는 다음의 &lt;hl&gt;두 가지 Register&lt;/hl&gt;가 존재한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Status Register, 상태 레지스터&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;CPU에게 데이터를 잘 수신했음을 확인시켜주기 위한 Register이다.&lt;br /&gt;
&lt;hl&gt;새로운 데이터가 업데이트되면 status가 0에서 1로 업데이트&lt;/hl&gt;된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Register, 데이터 레지스터&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;hl&gt;임시로 데이터가 저장&lt;/hl&gt;되는 Register이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I/O Device로부터 I/O Device Controller로 데이터를 읽어오는 과정&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-computerStructure-kmooc01/image-20210805215908983.png&quot; alt=&quot;image-20210805215908983&quot; style=&quot;width:250px;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;CPU → I/O Device Controller&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;hl&gt;Address와 READ enable 신호&lt;/hl&gt;를 받는다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;I/O Device Controller ←→ I/O Device&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;I/O Device로부터 데이터를 읽어와 &lt;hl&gt;Data Register에 데이터를 저장&lt;/hl&gt;한다.
이후 &lt;hl&gt;Status Register의 값을 0에서 1로 업데이트&lt;/hl&gt;한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;CPU ← I/O Device Controller&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;hl&gt;Status Register&lt;/hl&gt;의 값을 읽어서, &lt;hl&gt;In_RDY Bit의 값이 1인지&lt;/hl&gt;를 판단한다.&lt;br /&gt;
&lt;gray&gt;In_RDY Bit는 Status Register에 존재한다.&lt;/gray&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;CPU ← I/O Device Controller&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;4.1. &lt;strong&gt;In_RDY Bit == 1인 경우&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Controller가 데이터를 잘 읽어온 경우이다.
Data Register의 값을 읽어 &lt;hl&gt;Data Bus를 통해 CPU에게 전달&lt;/hl&gt;한다.&lt;/p&gt;

    &lt;p&gt;4.2. &lt;strong&gt;In_RDY Bit != 1인 경우&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Controller가 데이터를 아직 읽어오지 못한 경우이다.&lt;/p&gt;
    &lt;hl&gt;다시 3의 과정으로 돌아간다.&lt;/hl&gt;
    &lt;blockquote&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;io-write&quot;&gt;I/O Write&lt;/h3&gt;

&lt;p&gt;유사한 방식으로 I/O Device로부터 데이터를 읽을 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;결정-방식&quot;&gt;결정 방식&lt;/h2&gt;

&lt;h3 id=&quot;bus-arbitration&quot;&gt;Bus Arbitration&lt;/h3&gt;

&lt;p&gt;System Bus는 &lt;hl&gt;한 순간에 하나의 Address나 Data만 전송시킬 수 있다.&lt;/hl&gt; 이때 동시에 System Bus를 사용해야 하는 경우, &lt;hl&gt;우선권을 주는 방식&lt;/hl&gt;에 대해서 다룬다. 이러한 방식을 &lt;hl&gt;Bus Arbitration&lt;/hl&gt;이라고 하며 아래와 같이 크게 3가지 방식으로 나눌 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-computerStructure-kmooc01/image-20210805221125057.png&quot; alt=&quot;image-20210805221125057&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PA, Parallel Arbitration&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Bus Priority를 &lt;hl&gt;병렬&lt;/hl&gt;로 부여하는 방식&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Serial Arbitration&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Bus Priority를 &lt;hl&gt;직렬&lt;/hl&gt;로 부여하는 방식&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Polling&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;요구한 디바이스에게 우선적으로 Bus Priority를 부여하는 방식 (&lt;hl&gt;선착순&lt;/hl&gt;)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;io-device-접속-방식&quot;&gt;I/O Device 접속 방식&lt;/h3&gt;

&lt;p&gt;I/O Device를 설계할 때, 주로 기존의 프로세서를 이용하여 원하는 동작을 수행하는 어플리케이션을 만든다. 이때 외부의 데이터 입출력을 어떤 방식으로 CPU화할 것인지(인터페이스)를 결정해야 한다. 이러한 개념은 I/O Device 접속 방식에 해당한다. 아래와 같이 크게 세 가지로 분류할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-computerStructure-kmooc01/image-20210805221623680.png&quot; alt=&quot;image-20210805221623680&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Programmed I/O&lt;/strong&gt; a.k.a. Polling
    &lt;ul&gt;
      &lt;li&gt;주기적으로 데이터가 입력되었는지를 확인하는 방식&lt;/li&gt;
      &lt;li&gt;마이크로프로세서를 이용하여 코딩하는 경우 많이 사용됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interrupt Driven I/O&lt;/strong&gt;, 현재 대부분의 방식에서 사용됨
    &lt;ul&gt;
      &lt;li&gt;데이터가 왔을때 신호를 받아 데이터가 온 것을 알아차리는 방식&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;I/O with DMA&lt;/strong&gt;, Direct Memory Access
    &lt;ul&gt;
      &lt;li&gt;데이터가 왔을때 처리하는 장치가 별도로 존재하는 방식&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 세가지 방식에도 하위 방식이 여러가지 존재하는데, 추후에 자세히 다루도록 한다.&lt;/p&gt;

&lt;h2 id=&quot;컴퓨터의-전체-구조&quot;&gt;컴퓨터의 전체 구조&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-computerStructure-kmooc01/image-20210805222454604.png&quot; alt=&quot;image-20210805222454604&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-program과-data의-처리-반도체-개요&quot;&gt;3. Program과 Data의 처리, 반도체 개요&lt;/h1&gt;

&lt;p&gt;read와 write가 이루어지는 과정에 대해서 다룬다.&lt;/p&gt;

&lt;p&gt;1차시. 컴퓨터의 구성품&lt;/p&gt;

&lt;p&gt;2차시. 구성품간의 연결&lt;/p&gt;

&lt;p&gt;3차시. 어떻게 오가게하고 처리할 것인지&lt;/p&gt;

&lt;h2 id=&quot;readwrite-데이터를-처리하는-과정&quot;&gt;READ/WRITE 데이터를 처리하는 과정&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-01-cs-computerStructure-kmooc01/image-20210803165222621.png&quot; alt=&quot;image-20210803165222621&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High-Level Language Program&lt;/strong&gt;, (C/C++, Python 등등..)&lt;/p&gt;

    &lt;p&gt;e.g., Z=X+Y&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;컴파일&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Assembly Program&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Assembly Instruction&lt;/p&gt;

            &lt;p&gt;목적지가 앞에, 출발지가 뒤에 나옴&lt;/p&gt;

            &lt;p&gt;e.g.,&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LOAD A, X&lt;/code&gt;: A(accumulator) register로 X memory에 저장된 값을 LOAD(가져가라).&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ADD A, Y&lt;/code&gt; : A register로 Y에 저장된 값을 더해라.&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STOR Z, A&lt;/code&gt;: Z 메모리에 A의 값을 저장해라.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Machine Program&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;기계만 이해할 수 있는 내용.&lt;/li&gt;
          &lt;li&gt;이게 메모리에 저장되며, 메모리에 있는 machine program이 한 줄씩 주소에 따라서 CPU로 가서, CPU에서 어떤 코딩과정을 거쳐, 제어신호로 바뀜.&lt;/li&gt;
          &lt;li&gt;e.g.
            &lt;ul&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LOAD&lt;/code&gt;: 001&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt;: 00101 = 5번지.   거기 읽었더니 00011011이 써있음&lt;/li&gt;
              &lt;li&gt;A는 도마야. 지정할필요없어. 당연히 A에 저장하겄지. 그래서 따로 binary 없어&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ADD&lt;/code&gt;: 100&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Y&lt;/code&gt;: 00110 = 6번지.   거기 읽었더니 11010111 써있음&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STOR&lt;/code&gt;: 010&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Z&lt;/code&gt;: 00111 = 7번지.   거기에 저장을 하는겨&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;반도체-부품의-발전&quot;&gt;반도체 부품의 발전&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Vaccum Tube, 진공관 (1세대)
    &lt;ul&gt;
      &lt;li&gt;다이오드의 역할 ~~ 스위치. 끄면 0, 키면 1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;TR, Transistor (2세대)
    &lt;ul&gt;
      &lt;li&gt;Vacuum Tube가 발전된 형태.&lt;/li&gt;
      &lt;li&gt;컴퓨터로 구성하는 모든 반도체의 기본 단위&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;IC (3세대: SSI, Small Scale Integration)
    &lt;ul&gt;
      &lt;li&gt;transistor를 기판 안에 집적시켜, 수백만개의 transistor를 하나의 칩 안에 집적시키게 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MSI, Medium Scale Integration
    &lt;ul&gt;
      &lt;li&gt;단위면적당 transistor수 증가: 집적이 더 많이 됨.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LSI, Large Scale Integration&lt;/li&gt;
  &lt;li&gt;VLSI, Very Large Scale Integration
    &lt;ul&gt;
      &lt;li&gt;현재 이 정도.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ULSI,
    &lt;ul&gt;
      &lt;li&gt;아직 세대는 아님&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optical/Neural Computer
    &lt;ul&gt;
      &lt;li&gt;앞으로 나올 것들.&lt;/li&gt;
      &lt;li&gt;신경망을 이용한 컴퓨터&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AI
    &lt;ul&gt;
      &lt;li&gt;인공지능 형태의 반도체&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ic-제조-과정&quot;&gt;IC 제조 과정&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Silicon&lt;/strong&gt;: IC 만드는 대표적 재료 (실리콘 혹은 저메니움,갈마스나이드 여러 종류가 있음)&lt;/p&gt;

&lt;p&gt;실리콘 이용하여 &lt;strong&gt;wafer&lt;/strong&gt;를 만듦 ~~ 웨하스의 얇은 판떼기&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-01-cs-computerStructure-kmooc01/image-20210803165717350.png&quot; alt=&quot;image-20210803165717350&quot; /&gt;&lt;/p&gt;

&lt;p&gt;정사각형 하나가 칩 하나. 짜투리는 버림.&lt;/p&gt;

&lt;p&gt;여기에 &lt;strong&gt;Integrate&lt;/strong&gt;(직접)함.&lt;/p&gt;

&lt;p&gt;이후 쪼갬. &lt;strong&gt;다이아몬드 칼로 하나를 쪼개서 이 패키지 안에 넣음.=&amp;gt; PAckaging&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;img src=&quot;/files/2021-08-01-cs-computerStructure-kmooc01/image-20210803165811472.png&quot; alt=&quot;image-20210803165811472&quot; /&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pin도 부착&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PCB에 연결&lt;/strong&gt;: 녹색 판에 저항, 케페시터 등 부착하면 보드를 얻게됨.&lt;/p&gt;

&lt;h2 id=&quot;컴퓨터-시스템의-분류&quot;&gt;컴퓨터 시스템의 분류&lt;/h2&gt;

&lt;p&gt;가장 작은&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Embedded Computer (두 번째로 많이 사용). 일에 종사하면 자판기 등에서 이런걸로 됨.&lt;/p&gt;

    &lt;p&gt;시스템 내에 내장되어 있다는 의미에서 내장 컴퓨터&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;PC, Personal Computer (가장 많이 사용)&lt;/p&gt;

    &lt;p&gt;최근 모든 기능은 PC로 집약되는 느낌.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WS, Work Station, Super-Mini Computer&lt;/p&gt;

    &lt;p&gt;크고 성능이 파워풀한 것.&lt;/p&gt;

    &lt;p&gt;최근 쓰임새가 줄어듬. 왜냐면 PC에서 구현 가능&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Main-Frame Computer&lt;/p&gt;

    &lt;p&gt;작은 방안을 채울 정도로 큰 규모를 갖는 컴퓨터&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Super Computer, Cluster Computer (Alpha-Go)&lt;/p&gt;

    &lt;p&gt;최근 경향: 하나의 머신을 만들지 않고 cluster computer라해서 네트워크를 연결된 여러 대의 PC를 컴퓨터 파워 모아서 비슷하게 만들어주는 그런걸로 됨.&lt;/p&gt;

    &lt;p&gt;alpha-go도 Cluster computer. 수천개의 pc가 네트워크로 연결되어 바둑을 둚&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;kmooc-컴퓨터구조: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SMUCk+CK.SMUC03k+2017_T6/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SMUCk+CK.SMUC03k+2017_T6/course/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 14 Aug 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-computerstructure/cs-computerStructure-kmooc01/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-computerstructure/cs-computerStructure-kmooc01/</guid>
        
        <category>computer science</category>
        
        <category>computer structure</category>
        
        <category>CS</category>
        
        
        <category>computerScience-computerStructure</category>
        
      </item>
    
      <item>
        <title>[ALGO-03] Lect3. 비선형 자료구조</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802140417579.png&quot; alt=&quot;image-20210802140417579&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 시간에는 대표적인 &lt;hl&gt;비선형 자료구조&lt;/hl&gt;인 &lt;hl&gt;트리와 그래프&lt;/hl&gt;에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;트리-자료구조&quot;&gt;트리 자료구조&lt;/h1&gt;

&lt;p&gt;트리는 &lt;hl&gt;노드들의 집합체&lt;/hl&gt;로, &lt;hl&gt;계층적인 관계&lt;/hl&gt;를 나타내는 자료구조이다. 디렉토리 구조는 대표적인 tree구조이다..&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802141043317.png&quot; alt=&quot;image-20210802141043317&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;특징&quot;&gt;특징&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;정보는 nodes에 저장된다.&lt;/li&gt;
  &lt;li&gt;첫 번째 노드는 &lt;hl&gt;root&lt;/hl&gt;라고 불린다 &lt;gray&gt;(위 그림에서 A에 해당)&lt;/gray&gt;&lt;/li&gt;
  &lt;li&gt;각 노드는 &lt;hl&gt;자손 노드(children)&lt;/hl&gt;들을 가질 수 있다.&lt;/li&gt;
  &lt;li&gt;root를 제외한 각 노드는 하나의 &lt;hl&gt;부모 노드(parent)&lt;/hl&gt;를 가진다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;용어&quot;&gt;용어&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Degree&lt;/code&gt;: 해당 노드가 갖는 &lt;hl&gt;자식 노드의 개수&lt;/hl&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Leaf 노드&lt;/code&gt;: degree = 0인 노드&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Internal 노드&lt;/code&gt;: degree != 0인 노드&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sibling&lt;/code&gt;: 같은 부모 노드를 공유하는 &lt;hl&gt;자매 노드&lt;/hl&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Unordered tree&lt;/code&gt;: &lt;hl&gt;자식 노드의 순서가 무시&lt;/hl&gt;될 수 있는 트리 구조&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ordered tree&lt;/code&gt;: &lt;hl&gt;자식 노드간에 순서가 존재&lt;/hl&gt;하는 트리 구조&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Path&lt;/code&gt;: &lt;hl&gt;노드들의 시퀀스&lt;/hl&gt; $(a_0, a_1, …, a_n)$​​​​ (이때, $a_{k+1}$: $a_k$​​​​의 자식노드)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;예시: path(B,E,G)의 길이는 2이다.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802142050795.png&quot; alt=&quot;image-20210802142050795&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depth&lt;/code&gt;: &lt;hl&gt;루트노드부터 해당 노드까지의 경로의 길이&lt;/hl&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;예시: depth(B)=1, depth(E)=2, depth(F)=3&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802142602411.png&quot; alt=&quot;image-20210802142602411&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;height&lt;/code&gt;: 트리에 존재하는 &lt;hl&gt;가장 큰 depth값&lt;/hl&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;예시: &lt;br /&gt;
⒜ 루트 노드만 존재하는 경우 height=0&lt;br /&gt;
⒝ 아무 노드도 존재하지 않는 경우 heigth=-1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ancestor&lt;/code&gt;: a는 b의 ancestor (노드 a에서 노드 b로 가는 path가 존재할 때)&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;descendant&lt;/code&gt;: b는 a의 descendant (노드 a에서 노드 b로 가는 path가 존재할 때)&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;strictly descendant&lt;/code&gt;: 자기 자신이 자기 자신의 자손이나 부모가 되는 것을 금지시키는 조건 &lt;gray&gt;※ 일반적인 경우, 자기 자신은 자기 자신의 ancestor이자 descendant이다. ※&lt;/gray&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;표현&quot;&gt;표현&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802151123625.png&quot; alt=&quot;image-20210802151123625&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;각 노드가 &lt;hl&gt;자식 노드를 참조&lt;/hl&gt;하고 있는 구조로 표현된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;그래프-자료구조&quot;&gt;그래프 자료구조&lt;/h1&gt;

&lt;p&gt;그래프는 &lt;hl&gt;데이터 사이의 인접한 정보를 저장&lt;/hl&gt;하는 자료구조이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802201823939.png&quot; alt=&quot;image-20210802201823939&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 응용&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SNS상에서 친구 관계&lt;/li&gt;
  &lt;li&gt;회로 사이의 component간의 연결성&lt;/li&gt;
  &lt;li&gt;선수과목 정보 표현&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;용어-1&quot;&gt;용어&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vertex&lt;/code&gt;: &lt;hl&gt;정점(node)&lt;/hl&gt;, $V$라 표현&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Objects&lt;/code&gt;: 저장하고자 하는 객체. 유한개의 &lt;hl&gt;nodes(혹은 vertices)의 집합&lt;/hl&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;edge&lt;/code&gt;: &lt;hl&gt;vertex 간의 연결&lt;/hl&gt;, $E$라 표현&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Relationship&lt;/code&gt;: 유한개의 &lt;hl&gt;edges(혹은 arcs, links)의 집합&lt;/hl&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;degree&lt;/code&gt;: &lt;hl&gt;이웃 vertex의 개수&lt;/hl&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;예시: degree($v_1$)=3&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802202941366.png&quot; alt=&quot;image-20210802202941366&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;neighbor&lt;/code&gt;: &lt;hl&gt;인접한(adjacent) vertex의 집합&lt;/hl&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub-graph&lt;/code&gt;: original graph에서 &lt;hl&gt;일부 vertex와 edge를 sampling&lt;/hl&gt;하여 얻을 수 있는 그래프&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;path&lt;/code&gt;: &lt;hl&gt;vertex간의 연결&lt;/hl&gt;: $(v_0, v_1, …, v_k)$&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trivial path&lt;/code&gt;: &lt;hl&gt;length=0&lt;/hl&gt;인 path&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;simple path&lt;/code&gt;: 경로상에, 처음과 마지막을 제외하고 중복이 없는 경우&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;simple cycle&lt;/code&gt;: &lt;hl&gt;처음과 마지막 vertex가 일치&lt;/hl&gt;하는 &lt;hl&gt;simple path&lt;/hl&gt;
        &lt;ul&gt;
          &lt;li&gt;예시&lt;br /&gt;
case1) A-B-C : simple path&lt;br /&gt;
case2) A-B-C-A : simple path &amp;amp;&amp;amp; simple cycle&lt;br /&gt;
case3) A-B-C-B-A : simple path아님&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;connectedness(연결성)&lt;/code&gt;: graph의 vertex끼리 어떤 path로든 &lt;hl&gt;연결되어있는지&lt;/hl&gt; 여부&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;예시: 아래 그래프에서 흰색 영역에 존재하는 path가 끊어지면 conntedness 성질이 사라진다.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Network_Community_Structure.svg/220px-Network_Community_Structure.svg.png&quot; alt=&quot;img&quot; style=&quot;width:200px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;유형&quot;&gt;유형&lt;/h2&gt;

&lt;p&gt;그래프의 속성에 따라 다양한 종류의 그래프가 존재한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Undirected graph&lt;/code&gt;: edge에 &lt;hl&gt;방향이 없는&lt;/hl&gt; 무향 그래프&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;directed graph&lt;/code&gt;: edge에 &lt;hl&gt;방향성이 존재&lt;/hl&gt;하는 유향 그래프&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Weighted graph&lt;/code&gt;: &lt;hl&gt;가중치&lt;/hl&gt;가 있는 그래프&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tree&lt;/code&gt;: &lt;hl&gt;unique path&lt;/hl&gt;를 갖는 conncted graph&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forest&lt;/code&gt;: &lt;hl&gt;tree들의 모음&lt;/hl&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-undirected-graphs&quot;&gt;1. Undirected Graphs&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802204012542.png&quot; alt=&quot;image-20210802204012542&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;vertices의 모음으로 표현되는 &lt;hl&gt;방향이 없는 그래프&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$V={v_1, v_2, …, v_n}$일 때&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;vertices의 개수 $│V│=n$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;vertices를 연결하는 edges E = ${v_i, v_j}$​   &lt;gray&gt;※ 순서가 없는 쌍 ※&lt;/gray&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;연결성을 표현하기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;adjacency matrix&lt;/code&gt;나 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;adjacency list&lt;/code&gt;를 사용할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802152506507.png&quot; alt=&quot;image-20210802152506507&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;9개의 vertex가 존재: $V={v_1, v_2, …, v_9}$​​, $│V│=9$​​&lt;/li&gt;
  &lt;li&gt;5개의 edge가 존재: $E={{v_1, v_2}, {v_3,v_5}, {v_4, v_8}, {v_4, v_9}, {v_6, v_9}}$, $│E│=5$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 최대 edge개수&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;자기 자신으로 가는 edge는 없다고 가정하면, &lt;hl&gt;undirected graph에서 최대한 많은 edge의 개수&lt;/hl&gt;는 다음과 같다.&lt;/p&gt;

&lt;p&gt;\(|E|≤
_VC_2 =
\binom{|V|}{2} =
\frac{|V|(|V|-1|)}{2} =
O(|V|^2)\)
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-directed-graphs&quot;&gt;2. Directed Graphs&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802210956503.png&quot; alt=&quot;image-20210802210956503&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;vertices의 모음으로 표현되는 &lt;hl&gt;방향이 있는 그래프&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 용어&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;in_degree&lt;/code&gt;: 방향이 해당 &lt;hl&gt;node로 향하는&lt;/hl&gt; edge의 개수&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;out_degree&lt;/code&gt;: 해당 &lt;hl&gt;node에서 나오는 방향&lt;/hl&gt;인 edge의 개수&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;예시: in_degree($v_1$)=1, out_degree($v_1$)=2&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802211314168.png&quot; alt=&quot;image-20210802211314168&quot; style=&quot;width:200px;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source&lt;/code&gt;: &lt;hl&gt;in_degree=0&lt;/hl&gt;인 node&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sink&lt;/code&gt;: &lt;hl&gt;out_degree=0&lt;/hl&gt;인 node&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;strongly connected&lt;/code&gt;: &lt;hl&gt;방향성을 고려&lt;/hl&gt;했을 때 &lt;hl&gt;모든 pair간에 경로가 존재&lt;/hl&gt;하는 경우&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;weakly connected&lt;/code&gt;: &lt;hl&gt;방향성을 무시&lt;/hl&gt;할 때 &lt;hl&gt;모든 pair간에 경로가 존재&lt;/hl&gt;하는 경우&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;directed acyclic graph (DAG, 유향 비순환 그래프)&lt;/code&gt;: &lt;hl&gt;순환하지 않는 유향 그래프&lt;/hl&gt;&lt;/p&gt;

    &lt;p&gt;예시:&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/220px-Topological_Ordering.svg.png&quot; alt=&quot;img&quot; style=&quot;width:150px;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-weighted-graphs&quot;&gt;3. Weighted Graphs&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802204557687.png&quot; alt=&quot;image-20210802204557687&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;edge가 &lt;hl&gt;연결성&lt;/hl&gt;뿐만 아니라 &lt;hl&gt;가중치&lt;/hl&gt;도 표현하는 그래프&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 응용&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;vertex간의 거리나 에너지 소모등을 표현하는 경우에 주로 사용되며, 이러한 그래프를 통해 &lt;hl&gt;shortest path 문제&lt;/hl&gt;를 해결할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ path length&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;단순히 연결된 path의 개수로 표현되는 un-weighted graphs와 달리, weighted graph는 &lt;hl&gt;path의 가중치의 합으로 length&lt;/hl&gt;가 표현된다.&lt;/p&gt;

&lt;p&gt;예시: $\text{length of }(v_1, v_3, v_5, v_4)=5.1+1.3+1.1=7.5$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802204836778.png&quot; alt=&quot;image-20210802204836778&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;4-tree&quot;&gt;4. Tree&lt;/h3&gt;

&lt;p&gt;graph가 &lt;hl&gt;다음의 조건을 만족&lt;/hl&gt;하는 경우, tree라고 부를 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;cycle이 없음&lt;/li&gt;
  &lt;li&gt;연결 그래프&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;unique path&lt;/hl&gt;를 갖는다. ⇒ 따라서 $│E│=│V│-1$&lt;/p&gt;

    &lt;div class=&quot;callout&quot;&gt;root를 제외한 모든 node가 하나의 parent와 연결된 path를 가지며, parent가 누구냐에 따라 구조가 달라진다고 생각하면 |E|=|V|-1의 결과를 쉽게 얻을 수 있다.&lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;하나의 edge를 추가하면 cycle&lt;/hl&gt;이 생긴다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;하나의 edge를 제거하면&lt;/hl&gt; disconnected graph가 되며, &lt;hl&gt;두 개의 tree로 분리&lt;/hl&gt;된다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;5-forest&quot;&gt;5. Forest&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802210313744.png&quot; alt=&quot;image-20210802210313744&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;hl&gt;하나 이상의 tree로 이루어진 집합&lt;/hl&gt;을 forest라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;cycle이 없다&lt;/hl&gt; ⇒ 따라서 $│E│&amp;lt;│V│$ &lt;/p&gt;

    &lt;div class=&quot;callout&quot;&gt;forest에서 root를 하나만 남도록 연결하면(edge를 추가하면) 하나의 tree가 된다. 즉, │E│=│V│-1인 tree의 edge 개수보다 forest의 edge개수가 더 적으니 언제나 │E│&amp;lt;│V│&lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;tree의 개수 = $│V│-│E│$&lt;/hl&gt;&lt;/p&gt;

    &lt;div class=&quot;callout&quot;&gt;|V|에서 |E|를 빼면 root의 개수가 되므로 tree의 개수는 |V|-|E|&lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;hl&gt;edge를 제거하면 tree가 하나 더 생성&lt;/hl&gt;된다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;표현-1&quot;&gt;표현&lt;/h2&gt;

&lt;p&gt;그래프를 표현하는 대표적인 방식은 다음과 같다.&lt;/p&gt;

&lt;h3 id=&quot;1-binary-relation-list&quot;&gt;1. Binary-relation list&lt;/h3&gt;

&lt;p&gt;&lt;hl&gt;edge를 나열&lt;/hl&gt;하여 graph를 표현한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802212242204.png&quot; alt=&quot;image-20210802212242204&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;${(1,2), (71,4), (3,5), (4,2), (4,5), (5,2), (5,3), (6,9), (7,9), (8,4)}$&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;메모리 사용량 = $│E│$​&lt;/li&gt;
  &lt;li&gt;어떤 두 vertex간에 edge가 존재하는지 확인하기 위한 계산량 $≤│E│$​​​​​&lt;/li&gt;
  &lt;li&gt;어떤 vertex의 neighbor를 구하기 위한 계산량 $≤│E│$​​&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-adjacency-matrix&quot;&gt;2. Adjacency Matrix&lt;/h3&gt;

&lt;p&gt;vertex $v_j$​​에서 $v_k$​​로 가는 &lt;hl&gt;edge가 존재할 때 $(v_j, v_k)$​​의 값을 True로&lt;/hl&gt; 지정한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;메모리 사용량 = $│V│^2$&lt;/li&gt;
  &lt;li&gt;어떤 두 vertex 간에 edge가 존재하는지 확인하기 위한 계산량 = 1&lt;/li&gt;
  &lt;li&gt;어떤 vertex의 neighbor를 구하기 위한 계산량 = $O(│V│)$
    &lt;gray&gt;※ computational overhead ※&lt;/gray&gt;
  &lt;/li&gt;
  &lt;li&gt;weighted graph인 경우, true/false 대신 weight 값으로 표현&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-adjacency-list&quot;&gt;3. Adjacency List&lt;/h3&gt;

&lt;p&gt;일반적인 알고리즘에서 가장 많이 사용되는 그래프 표현법으로, &lt;hl&gt;각 노드를 기준으로 자기 자신과 연결된 vertex를 list형태&lt;/hl&gt;로 표현한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 예시&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-08-cs-algorithm-algo03/image-20210802213201489.png&quot; alt=&quot;image-20210802213201489&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;th style=&quot;background-color:black; color:white; text-align:center;&quot;&gt;vertex&lt;/th&gt;
    &lt;th style=&quot;background-color:black; color:white; text-align:center;&quot;&gt;adjacent to&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v1&lt;/td&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v2, v4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v2&lt;/td&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt; &lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v3&lt;/td&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v4&lt;/td&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v2, v5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v5&lt;/td&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v2, v3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v6&lt;/td&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v7&lt;/td&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v8&lt;/td&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v4, v5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt;v9&lt;/td&gt;&lt;td style=&quot;width:100px; text-align:center; border: 1px solid #333;&quot;&gt; &lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;■ 특징&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;메모리 사용량 = $│V│ &amp;lt; │E│$​&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고-비선형 자료구조, K-MOOC: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/courseware/1158909c057347478d7386a2b7e97214/e9d736660cad4f76bcb9a05974fe0bf2/1?activate_block_id=block-v1%3ASKKUk%2BSKKU_46%2B2021_T1%2Btype%40vertical%2Bblock%4052b62ff6da2147c9a8a65a6056681d91&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 08 Aug 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo03/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo03/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-02] Lect2. 기본적인 선형 자료구조의 종류, 구동 원리 및 활용법 이해</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;선수지식&quot;&gt;선수지식&lt;/h1&gt;

&lt;h2 id=&quot;메모리-접근-오퍼레이터&quot;&gt;메모리 접근 오퍼레이터&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;용어&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Pointer&lt;/strong&gt;&lt;br /&gt;
어떤 variable은 크게 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Address&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Value&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Name&lt;/code&gt;으로 표현된다고 할 때, pointer를 이용하여 value가 아닌 &lt;hl&gt;address로 값을 받아올 수 있다.&lt;/hl&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&amp;amp;(Ampersand) Operator&lt;/strong&gt;&lt;br /&gt;
&amp;amp; 연산자는 reference 연산자라고도 불리며, &lt;hl&gt;변수의 주소값을 반환&lt;/hl&gt;한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;*(Asterisk) Operator&lt;/strong&gt;&lt;br /&gt;
*연산자는 dereference 연산자라고도 불리며, 포인터 앞에서 &lt;hl&gt;포인터가 가르키는 값에 접근&lt;/hl&gt;한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;예제1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;변수 n의 주소를 변수 pn에 초기화&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-algorithm-algo02/image-20210801152358545.png&quot; alt=&quot;image-20210801152358545&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;예제2&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;'A'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c=%c, *pc=%c &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;'C'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c=%c, *pc=%c &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;출력결과:&lt;/p&gt;

&lt;div class=&quot;language-tex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;c=A, *pc=A
c=C, *pc=C
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;function-call&quot;&gt;Function Call&lt;/h2&gt;

&lt;p&gt;함수를 호출할 때 &lt;hl&gt;argument를 넣어주는 방식&lt;/hl&gt;에 따라 크게 두 가지로 나뉜다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Call by value&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;hl&gt;값 자체&lt;/hl&gt;
    &lt;p&gt;를 넣어준다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;원래 변수의 값을 수정할 수 없다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. Call by reference&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;hl&gt;포인터/주소값&lt;/hl&gt;
    &lt;p&gt;으로 함수를 호출한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;원래 변수의 값을 수정할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;예제&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;swap1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;swap2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a=%d, b=%d&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;swap1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;swap1: a=%d, b=%d&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;swap2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;swap2: a=%d, b=%d&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;    

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;swap1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;swap2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;px&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;출력결과:&lt;/p&gt;

&lt;div class=&quot;language-tex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a=5, b=7
swap1: a=5, b=7
swap2: a=7, b=5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;배열과-리스트&quot;&gt;배열과 리스트&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-algorithm-algo02/image-20210801143659915.png&quot; alt=&quot;image-20210801143659915&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;선형 자료 구조 중 대표적인 &lt;hl&gt;array와 list&lt;/hl&gt;에 대해 알아본다.&lt;/p&gt;

&lt;h2 id=&quot;array&quot;&gt;Array&lt;/h2&gt;

&lt;p&gt;array는 어떤 자료의 값들을 모아둔 자료 구조이다.&lt;/p&gt;

&lt;h3 id=&quot;1-기본개념&quot;&gt;1. 기본개념&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;구분자&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;array &lt;hl&gt;index를 통해 구분&lt;/hl&gt;되며, 프로그래밍 언어에 따라 인덱스는 0부터 시작할 수도, 1부터 시작할 수도 있다. C/C++, JAVA 프로그래밍 언어에서는 array index가 0부터 시작한다.&lt;/p&gt;

&lt;p&gt;가령 아래와 같이 size가 10인 int형 array score를 선언할 때 array index는 0~9이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-algorithm-algo02/image-20210801144312550.png&quot; alt=&quot;image-20210801144312550&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;저장&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;프로그래밍 언어를 이용하여 array를 선언하면, &lt;hl&gt;physical memory인 RAM에 연속적인 공간을 할당받는다.&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;선언&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;array를 선언하는 방법은 크게 두 가지가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;hl&gt;정적 생성&lt;/hl&gt;

    &lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;hl&gt;동적 생성&lt;/hl&gt;

    &lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;동적으로 생성한 array를 다 사용한 경우, 할당을 삭제해야 한다: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delete [] d;&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;접근&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;인덱스를 이용하여 접근할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-array-생성-in-c&quot;&gt;2. Array 생성 (in C++)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1차원 array&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// 혹은 &lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-algorithm-algo02/image-20210801145529448.png&quot; alt=&quot;image-20210801145529448&quot; style=&quot;width:500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2차원 array&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// 혹은 &lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-algorithm-algo02/image-20210801145906817.png&quot; alt=&quot;image-20210801145906817&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-array-in-memory&quot;&gt;3. Array in Memory&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;52&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;61&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위와 같이 score array를 생성했을 때, physical memory에는 &lt;hl&gt;아래 이미지&lt;hl&gt;와 같이 값이 쓰여진다.&lt;/hl&gt;&lt;/hl&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-algorithm-algo02/image-20210801150840947.png&quot; alt=&quot;image-20210801150840947&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;4-insertion-and-deletion&quot;&gt;4. Insertion and Deletion&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Insertion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;array 요소의 중간에 어떤 값을 삽입하고싶을 때, &lt;hl&gt;이후 값을 모두 1칸씩 뒤로 밀고 생긴 빈자리에 값을 넣어&lt;/hl&gt;주어야 한다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,};&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printIntArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;// 3(arr[0])과 10(arr[1]) 사이에 5를 삽입&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// array 결과를 출력&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printIntArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;출력결과:&lt;/p&gt;

&lt;div class=&quot;language-tex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3 10 7 6 0 
3 5 10 7 6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deletion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;마찬가지로 어떤 요소값을 삭제할 때도 &lt;hl&gt;이후 값을 모두 1칸씩 앞으로 밀어주어야&lt;/hl&gt; 한다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hello World!&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printIntArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;// 3(arr[0])를 삭제&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// array 결과를 출력&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printIntArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;출력결과:&lt;/p&gt;

&lt;div class=&quot;language-tex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3 5 10 7 6 
5 10 7 6 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;list&quot;&gt;List&lt;/h2&gt;

&lt;p&gt;insertion 혹은 deletion을 수행할 때 해당 인덱스를 기점으로 뒤의 값을 모두 밀어주어야하는 &lt;hl&gt;array의 단점을 해결&lt;/hl&gt;하기위해 제안된 자료구조가 바로 linked list이다. physical memory상에서 순서대로 작성되지 않으므로 &lt;hl&gt;삽입과 삭제가 한 번의 동작으로 가능&lt;/hl&gt;하다.&lt;/p&gt;

&lt;h3 id=&quot;1-기본개념-1&quot;&gt;1. 기본개념&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;_Node&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;_Node&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;    
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinkedList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-algorithm-algo02/image-20210801160842809.png&quot; alt=&quot;image-20210801160842809&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;스택과-큐&quot;&gt;스택과 큐&lt;/h1&gt;

&lt;hl&gt;stack과 queue&lt;/hl&gt;
&lt;p&gt;는 빈번히 사용되는 자료구조이다. C++에서는 STL(standard template library) 소프트웨어 패키지를 이용하여 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pair&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vector&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;list&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;queue&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stack&lt;/code&gt;과 같은 다양한 종류의 container를 제공한다.&lt;/p&gt;

&lt;h2 id=&quot;stack&quot;&gt;Stack&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-algorithm-algo02/image-20210801161404482.png&quot; alt=&quot;image-20210801161404482&quot; style=&quot;width:100px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;삽입과 삭제가 &lt;hl&gt;LIFO(last-in first-out)&lt;/hl&gt; 순서로 진행된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;용어&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Top&lt;/code&gt;: stack의 최상단 지점&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Push&lt;/code&gt;: top 위치에 아이템을 삽입&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pop&lt;/code&gt;: top에 위치한 아이템을 제거&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;응용&lt;/strong&gt;&lt;/p&gt;

&lt;hl&gt;괄호매칭 문제&lt;/hl&gt;
&lt;p&gt;에서 stack이 사용된다. 열린 괄호가 나타날 때 stack에 열린괄호를 push하고, 닫힌 괄호가 나타날 때 열린 괄호를 pop한다. 최종적으로 stack이 비어있다면 괄호매칭이 잘 된 것이라 판단한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;queue&quot;&gt;Queue&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-algorithm-algo02/image-20210801161957500.png&quot; alt=&quot;image-20210801161957500&quot; style=&quot;width:120px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;삽입과 삭제가 &lt;hl&gt;FIFO(first-in first-out)&lt;/hl&gt; 순서대로 진행된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;용어&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Front (head)&lt;/code&gt;: queue의 앞 부분 (deletion이 발생)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Rear (back, tail)&lt;/code&gt;: queue의 뒷 부분 (insertion이 발생)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Enqueue&lt;/code&gt;: rear에서 아이템이 삽입됨&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dequeue&lt;/code&gt;: front에서 아이템이 삭제됨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;linear-queue-vs-circular-queue&quot;&gt;Linear Queue vs. Circular Queue&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-05-cs-algorithm-algo02/image-20210801163040665.png&quot; alt=&quot;image-20210801163040665&quot; style=&quot;width:400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;linear queue는 enqueue/dequeue를 반복하다보면 rear가 주어진 array의 마지막을 가리키게 된다. 따라서 front의 앞 부분에 분명 공간이 남아있음에도 사용할 수 없는 &lt;hl&gt;공간 낭비 문제&lt;/hl&gt;가 발생한다.&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해 linear queue에서 할당된 array의 양끝을 이어붙인 &lt;hl&gt;circular queue&lt;/hl&gt; 구조가 제안되었다. circular queue는 rear가 array의 마지막을 간 경우, 다시 0 index로 돌아오는 형태로 되어있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;kmooc-[집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Aug 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo02/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo02/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[ALGO-01] Lect1. 자료구조/알고리즘의 정의</title>
        <description>&lt;div class=&quot;info-dialog&quot;&gt;
    &lt;div class=&quot;title&quot;&gt;강의 정보&lt;/div&gt;
    &lt;div class=&quot;content&quot;&gt;
        강의명: [집콕]인공지능을 위한 알고리즘과 자료구조: 이론, 코딩, 그리고 컴퓨팅 사고&lt;br /&gt;
        교수: 성균관대학교 소프트웨어학과 허재필 교수님&lt;br /&gt;
        사이트: &lt;a href=&quot;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&quot;&gt;http://www.kmooc.kr/courses/course-v1:SKKUk+SKKU_46+2021_T1/course/&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;-자료구조-data-structure&quot;&gt;👉 자료구조, Data Structure&lt;/h1&gt;

&lt;p&gt;자료구조는 말 그대로 “자료의 구조”를 의미한다. 자료의 구조를 표현하기 위해서는 일반적으로 다음의 &lt;span class=&quot;hl&quot;&gt;3가지 요소&lt;/span&gt;가 필요하다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;자료의 값 (Data values)&lt;/li&gt;
  &lt;li&gt;자료들 간의 관계 (Relationships among data values)&lt;/li&gt;
  &lt;li&gt;자료에 적용될 수 있는 연산 (Operations that can be applied to the data)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;fundamental-data-structures&quot;&gt;Fundamental data structures&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-01-cs-algorithm-algo01/image-20210731210126795.png&quot; alt=&quot;image-20210731210126795&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;데이터의 구조는 크게 &lt;span class=&quot;hl&quot;&gt;기본 데이터 유형(Primitive)과 기본이 아닌 데이터 유형(Non-primitive)&lt;/span&gt;으로 나눌 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;1-기본-데이터-유형primitive&quot;&gt;1. 기본 데이터 유형(Primitive)&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;hl&quot;&gt;프로그래밍 언어에 의해 미리 정의된 데이터 타입&lt;/span&gt;을 의미한다. 변수 값의 크기와 유형이 미리 정의되어있다. 가령 C에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;integer&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;character&lt;/code&gt;와 같은 primitive data가 존재한다.&lt;/p&gt;

&lt;h3 id=&quot;2-기본이-아닌-데이터-유형non-primitive&quot;&gt;2. 기본이 아닌 데이터 유형(Non-primitive)&lt;/h3&gt;

&lt;p&gt;프로그래밍 언어에 의해 정의되지 않고 &lt;span class=&quot;hl&quot;&gt;프로그래머에 의해 생성된 데이터 타입&lt;/span&gt;이다. 데이터를 저장하는 메모리 위치를 참조하기 때문에 “참조 변수(reference varaibels)” 또는 객체 참조(object references)”라고도 불린다. Non-primitive data는 다음과 같이 &lt;span class=&quot;hl&quot;&gt;세 가지로 분류&lt;/span&gt;할 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;21-선형-데이터-구조-linear&quot;&gt;2.1. 선형 데이터 구조, Linear&lt;/h4&gt;

&lt;p&gt;선형 데이터 구조는 &lt;span class=&quot;hl&quot;&gt;데이터 요소가 순차 또는 선형으로 배열된 구조&lt;/span&gt;를 의미한다. 컴퓨터 메모리가 선형 방식으로 배열되어 있기 때문에 구현하기 쉽다는 특징이 있다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stack&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;queue&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linked list&lt;/code&gt;는 선형 구조를 갖는다.&lt;/p&gt;

&lt;div class=&quot;callout&quot;&gt;
       &lt;span style=&quot;font-weight:bold;&quot;&gt;Stack vs. Queue&lt;/span&gt;&lt;br /&gt;
       &lt;img src=&quot;/files/2021-08-01-cs-algorithm-algo01/image-20210731211825340.png&quot; alt=&quot;image-20210731211825340&quot; style=&quot;width:200px; display:block; margin:0px auto; &quot; /&gt;
       stack과 queue 모두 차례로 쌓인 구조이지만, stack은 들어온 역 순으로 데이터가 읽히고 queue는 들어온 순서대로 데이터가 읽힌다는 점에 차이가 있다.
   &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;22-비선형-데이터-구조-non-linear&quot;&gt;2.2. 비선형 데이터 구조, Non-linear&lt;/h4&gt;

&lt;p&gt;비선형 데이터 구조는 &lt;span class=&quot;hl&quot;&gt;순차적 또는 선형으로 배열되지 않은 데이터 구조&lt;/span&gt;를 의미한다. 선형 데이터 구조에 비해 구현하기 쉽지 않지만, 컴퓨터 메모리를 더 효율적으로 사용할 수 있는 장점을 갖는다. 예로는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tree&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;graph&lt;/code&gt;가 있다.&lt;/p&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
       &lt;span style=&quot;font-weight:bold;&quot;&gt;Tree vs. Graph&lt;/span&gt;&lt;br /&gt;
       &lt;img src=&quot;/files/2021-08-01-cs-algorithm-algo01/image-20210731212436322.png&quot; alt=&quot;image-20210731212436322&quot; style=&quot;width:200px; display:block; margin:0px auto; &quot; /&gt;
       tree는 두 정점 사이에 하나의 path만 존재하는 반면, graph는 둘 이상의 path가 허용된다. graph는 tree를 일반화한 것이라고 볼 수 있다.
   &lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;23-파일-데이터-구조-files&quot;&gt;2.3. 파일 데이터 구조, Files&lt;/h4&gt;

&lt;p&gt;파일 구조는 &lt;span class=&quot;hl&quot;&gt;서로 관련있는 필드로 구성된 레코드 집합인 파일에 대한 자료 구조&lt;/span&gt;로, 보조 기억 장치에 데이터가 실제로 기록되는 형태이다. 예로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sequential file&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index file&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;direct file&lt;/code&gt;이 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;graph-traversal-algorithms&quot;&gt;Graph traversal algorithms&lt;/h2&gt;

&lt;p&gt;선형 데이터 구조에서 데이터 요소는 단일 실행에서만 순회할 수 있는 반면, 비선형 데이터 구조에서 데이터 요소는 다중 실행을 통해 순회가 가능하다. graph나 tree와 같은 비선형 데이터를 순회하는 대표적인 알고리즘에는 DFS와 BFS가 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;🗣  뒤에서 더욱 자세히 다루도록 한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;-추상화-abstraction&quot;&gt;👉 추상화, Abstraction&lt;/h1&gt;

&lt;p&gt;컴퓨터 과학에서 추상화(abstraction)는 &lt;span class=&quot;hl&quot;&gt;복잡한 자료, 모듈, 시스템 등으로부터 핵심적인 개념 또는 기능을 간추려 내는 것&lt;/span&gt;을 말한다. 가령 경복궁을 찾아간다고 할 때, 아래의 왼쪽 그림(위성사진)보다는 오른쪽 그림(약도)이 더 보기 쉽다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-08-01-cs-algorithm-algo01/image-20210731204348879.png&quot; alt=&quot;image-20210731204348879&quot; style=&quot;width:300px; height:150px&quot; /&gt;&lt;img src=&quot;https://www.royalpalace.go.kr/content/images/guide/guide05_02.png&quot; alt=&quot;주차장 안내&quot; style=&quot;width:300px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이처럼 필수 정보만 제공하고 세부 사항을 숨기는 프로세스를 추상화라고 하며, 컴퓨팅 사고(computational thinking)에서 중요한 능력 중 하나이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;-알고리즘-algorithm&quot;&gt;👉 알고리즘, Algorithm&lt;/h1&gt;

&lt;p&gt;알고리즘은 &lt;span class=&quot;hl&quot;&gt;연산의 시퀀스&lt;/span&gt;이다. 적절한 알고리즘은 다음의 두 조건을 만족해야 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모든 가능한 input &lt;u&gt;instance&lt;/u&gt;*에 대해서 정답을 도출해야 한다.&lt;/li&gt;
  &lt;li&gt;반드시 알고리즘은 종료되어야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;margin-bottom:10px;&quot;&gt;
    &lt;span style=&quot;font-weight:bold;&quot;&gt;*인스턴스(instance)란?&lt;/span&gt;&lt;br /&gt;
 인스턴스는 알고리즘의 입력이 되는 데이터로, 필요한 정보를 모두 포함한다.
&lt;/div&gt;

&lt;p&gt;다양한 알고리즘 중에 더 좋은 알고리즘을 판별하기 위해서는 다양한 요소에 대해 분석해보아야 한다. 대표적으로 &lt;span class=&quot;hl&quot;&gt;점근적 표기법(Asymptotic notation)&lt;/span&gt;을 이용하여 알고리즘의 복잡도(complexity)를 분석할 수 있다. 복잡도는 크게 시간 복잡도(time complexity)와 공간 복잡도(space complexity)로 나눌 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;fundamental-algorithms&quot;&gt;Fundamental Algorithms&lt;/h2&gt;

&lt;p&gt;여기서는 대략적으로 어떤 알고리즘이 있는지만 살펴보고, 각 알고리즘에 대한 자세한 내용은 후에 다루도록 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;정렬 알고리즘(Sorting Algorithm)&lt;/strong&gt;: 원소들을 일정한 순서대로 열거하는 알고리즘&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;최단 경로 알고리즘(Shortest Path Algorithm)&lt;/strong&gt;: graph에서 한 노드에서 다른 노드로 가는 가장 빠른 길을 찾는 알고리즘&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;최소 신장 트리 알고리즘(Minimum Spanning Tree Algorithm)&lt;/strong&gt;: graph를 tree로 변환하는 알고리즘&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;위상 정렬 알고리즘(Topological Sort Algorithm)&lt;/strong&gt;: 방향을 가진 유향 그래프의 꼭짓점들(vertex)을 변의 방향을 거스르지 않도록 나열하는 알고리즘&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;동적 계획법 알고리즘(Dynamic Programming Algorithm)&lt;/strong&gt;: 복잡한 문제를 간단한 sub-problem으로 나누어 푸는 알고리즘&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;-실습-환경&quot;&gt;👉 실습 환경&lt;/h1&gt;

&lt;p&gt;앞으로의 실습은 C++ 프로그래밍 언어를 이용하여 진행된다. &lt;a href=&quot;https://sourceforge.net/projects/orwelldevcpp/&quot;&gt;Dev-c++&lt;/a&gt;(경량 프로그램) 혹은 &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;vscode&lt;/a&gt; 코드 편집기를 사용하여 실습을 진행한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;references&quot;&gt;👉References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;위키백과-추상화: &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%B6%94%EC%83%81%ED%99%94_(%EC%BB%B4%ED%93%A8%ED%84%B0_%EA%B3%BC%ED%95%99)&quot;&gt;https://ko.wikipedia.org/wiki/%EC%B6%94%EC%83%81%ED%99%94&lt;em&gt;(%EC%BB%B4%ED%93%A8%ED%84%B0&lt;/em&gt;%EA%B3%BC%ED%95%99)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;edureka-Know All About the Various Data Types in Java: &lt;a href=&quot;https://www.edureka.co/blog/data-types-in-java/&quot;&gt;https://www.edureka.co/blog/data-types-in-java/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Geeksforgeeks-Difference between Linear and Non-linear Data Structures: &lt;a href=&quot;https://www.geeksforgeeks.org/difference-between-linear-and-non-linear-data-structures/&quot;&gt;https://www.geeksforgeeks.org/difference-between-linear-and-non-linear-data-structures/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 01 Aug 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//computerscience-algorithm/algo01/</link>
        <guid isPermaLink="true">https://dazory.github.io//computerscience-algorithm/algo01/</guid>
        
        <category>computer science</category>
        
        <category>algorithm</category>
        
        <category>ALGO</category>
        
        
        <category>computerScience-algorithm</category>
        
      </item>
    
      <item>
        <title>[Environment] Anaconda 가상환경 생성 및 VSCode 연동 방법</title>
        <description>&lt;p&gt;2021.07.25 최초 작성&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;anaconda-설정&quot;&gt;Anaconda 설정&lt;/h1&gt;

&lt;p&gt;아나콘다는 &lt;a href=&quot;https://www.anaconda.com/products/individual#download-section&quot;&gt;여기&lt;/a&gt;에서 다운로드받을 수 있다. (전부 Next 클릭)&lt;/p&gt;

&lt;h2 id=&quot;세팅&quot;&gt;세팅&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;아나콘다 버전 확인&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda &lt;span class=&quot;nt&quot;&gt;--version&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;아나콘다 업데이트&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda update conda
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;가상환경-생성&quot;&gt;가상환경 생성&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;가상환경 생성&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;문법:&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda create &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; 가상환경명 설치할패키지
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;예시: project01라는 가상환경에 python 3.5 버전을 설치하는 경우&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda create &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; project01 &lt;span class=&quot;nv&quot;&gt;python&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3.5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
나는 보통 가상환경명을 프로젝트명으로 한다. 그래야 프로젝트 단위로 관리하기 쉽기 때문이다.
&lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;가상환경-활성화-및-비활성화&quot;&gt;가상환경 활성화 및 비활성화&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;가상환경 리스트 확인&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda info &lt;span class=&quot;nt&quot;&gt;--envs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;가상환경 활성화&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;문법:&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;activate 가상환경명
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;예시: project01라는 가상환경을 활성화하는 경우&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;activate project01
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;가상환경 비활성화&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;문법:&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;deactivate 가상환경명
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;예시: project01라는 가상환경을 비활성화하는 경우&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;deactivate project01
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;vscode에-anaconda-연동&quot;&gt;VScode에 Anaconda 연동&lt;/h1&gt;

&lt;h2 id=&quot;1-vscode-설치&quot;&gt;1. VScode 설치&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://code.visualstudio.com/download&quot;&gt;VSCode 홈페이지&lt;/a&gt;에서 설치&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-extensions&quot;&gt;2. Extensions&lt;/h2&gt;

&lt;p&gt;Extensions에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Python&lt;/code&gt;과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Code Runner&lt;/code&gt;를 설치한 뒤, VScode 재실행&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-interpreter-선택&quot;&gt;3. Interpreter 선택&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctrl+shift+p&lt;/code&gt; 누른 뒤, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Python: Select Interpreter&lt;/code&gt; 선택&lt;/p&gt;

&lt;p&gt;이후 원하는 아나콘다 가상환경 선택&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;기타-명령어&quot;&gt;기타 명령어&lt;/h1&gt;

&lt;h2 id=&quot;python&quot;&gt;Python&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;패키지 버전 변경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시: python을 3.5가 아닌 3.7.0버전으로 변경하고 싶은 경우&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pyton&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3.7.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;라이브러리 설치&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;문법:&lt;/p&gt;

    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;설치할라이브러리
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;예시:&lt;/p&gt;

    &lt;p&gt;matplotlib을 pip 패키지로 설치하려는 경우&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;matplotlib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;numpy를 pip 패키지로 설치하려는 경우&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;numpy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 25 Jul 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//etc-environment/anaconda/</link>
        <guid isPermaLink="true">https://dazory.github.io//etc-environment/anaconda/</guid>
        
        <category>environment</category>
        
        <category>anaconda</category>
        
        <category>vscode</category>
        
        
        <category>etc-environment</category>
        
      </item>
    
      <item>
        <title>[CVPR2021-03] CVPR2021-3D Vision 논문 훑기</title>
        <description>&lt;!-- 3DVR2021-CVPR 2021 Workshop on 3D Vision and Robotics --&gt;

&lt;p&gt;&lt;strong&gt;CVPR(IEEE Conference on Computer Vision and Pattern Recognition)&lt;/strong&gt;은 컴퓨터 비전과 패턴 인식 분야에서 세계적으로 최고 수준의(Top-tier) 학회이다. 이번 포스팅에서는 2021년도 CVPR 학회에서 3D Vision 주제에 관한 몇 가지 논문들을 Abstract 위주로 살펴본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;관심-논문&quot;&gt;관심 논문&lt;/h1&gt;

&lt;p&gt;내가 관심있는 주제에 관련된 몇 가지 논문을 abstract 위주로 살펴본다.&lt;/p&gt;

&lt;h2 id=&quot;3d3차원-비전&quot;&gt;3D(3차원 비전)&lt;/h2&gt;

&lt;div class=&quot;index&quot; style=&quot;background-color:white; color:#808080; padding:5px 15px; border: solid 1px #808080&quot;&gt;
	&lt;span style=&quot;font-weight:bold;&quot;&gt;차례&lt;/span&gt;
    &lt;ol style=&quot;margin-top:0;&quot;&gt;
        &lt;li&gt;A Deep Emulator for Secondary Motion of 3D Characters&lt;/li&gt;
		&lt;li&gt;Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction&lt;/li&gt;
    	&lt;li&gt;Deep Implicit Templates for 3D Shape Representation&lt;/li&gt;        
        &lt;li&gt;SMPLicit: Topology-aware Generative Model for Clothed People&lt;/li&gt;
        &lt;li&gt;Picasso: A CUDA-based Library for Deep Learning over 3D Meshes&lt;/li&gt;
        &lt;li&gt;Semi-supervised Synthesis of High-Resolution Editable Textures for 3D Humans&lt;/li&gt;
        &lt;li&gt;RGB-D Local Implicit Function for Depth Completion of Transparent Objects&lt;/li&gt;
        &lt;li&gt;Deep Two-View Structure-from-Motion Revisited&lt;/li&gt;
        &lt;li&gt;Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence&lt;/li&gt;
        &lt;li&gt;S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling&lt;/li&gt;
        &lt;li&gt;Plan2Scene: Converting Floorplans to 3D Scenes&lt;/li&gt;
        &lt;li&gt;Shelf-Supervised Mesh Prediction in the Wild&lt;/li&gt;
        &lt;li&gt;Unsupervised Learning of 3D Object Categories from Videos in the Wild&lt;/li&gt;
    &lt;/ol&gt;
&lt;/div&gt;

&lt;h3 id=&quot;a-deep-emulator-for-secondary-motion-of-3d-characters&quot;&gt;&lt;strong&gt;A Deep Emulator for Secondary Motion of 3D Characters&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Mianlun Zheng, Yi Zhou, Duygu Ceylan, Jernej Barbič
(&lt;a href=&quot;https://arxiv.org/abs/2103.01261&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-07-22-computerVision-papers-CVPR202103/image-20210715101000104.png&quot; alt=&quot;image-20210715101000104&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3차원 캐릭터를 애니메이션화하기 위한 빠르고 가벼운 방법은 컴퓨터 게임과 같은 다양한 애플리케이션에 적합하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;생생한 2차 모션 효과로 3D 캐릭터의 skinning 기반 애니메이션을 향상시키는 학습 기반 접근 방식을 제안한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;인접 정점(vertices) 사이의 내부 힘을 암시적으로 인코딩하는 캐릭터 시뮬레이션 메쉬의 각 로컬 패치를 인코딩하는 신경망을 설계하였다.&lt;/p&gt;

    &lt;p&gt;이 네트워크는 캐릭터 역학의 상미분 방정식(ordinary differential equations)을 emulate하여, 현재의 가속도, 속도, 그리고 위치로부터 새로운 정점(vertex)의 위치를 예측한다.&lt;/p&gt;

    &lt;p&gt;로컬 방식이므로, 이러한 네트워크는 mesh topology에 독립적이며, 테스트시 임의의 모양을 갖는 3D character meshes로 일반화된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;정점(vertex)당 제약 조건과 강성(stiffness)과 같은 재료 속성을 통해, mesh의 서로 다른 부분에서 역학을 쉽게 조정할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이러한 방식을 다양한 캐릭터 메쉬와 복잡한 모션 시퀀스에 대해 평가하였다.&lt;/li&gt;
  &lt;li&gt;실제(ground-truth) 물리적 기반 시뮬레이션에 비해 30배 더 효율적일 수 있으며, 빠른 근사치를 제공하는 대체 솔루션보다 성능이 뛰어남을 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
    &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
컴퓨터 게임과 같은 애플리케이션에서는 빠르고 가벼운 3D 캐릭터 애니메이션 기법이 요구된다. 이에, 캐릭터 역학의 상미분 방정식을 에뮬레이트하여, 현재의 가속도, 속도, 그리고 위치 정보로부터 새로운 정점의 위치를 예측하는 로컬 방식의 네트워크를 제안한다. 이를 통해 정점당 제약 조건과 강성과 같은 재료 속성을 통해 역학을 쉽게 조정할 수 있다.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;neural-deformation-graphs-for-globally-consistent-non-rigid-reconstruction&quot;&gt;&lt;strong&gt;Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Aljaž Božič, Pablo Palafox, Michael Zollhöfer, Justus Thies, Angela Dai, Matthias Nießner
(&lt;a href=&quot;https://arxiv.org/abs/2012.01451&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-07-22-computerVision-papers-CVPR202103/image-20210715102703134.png&quot; alt=&quot;image-20210715102703134&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;단단하지 않은 변형 표면을 캡처하는 것은 종종 매우 역동적인 실제 환경을 재구성하고 이해하는 데 필수적이다.&lt;/li&gt;
  &lt;li&gt;정적 3D 씬(scene) 재구성에 인상적인 발전이 이루어졌지만, 동적 추적 및 재구성은 여전히 매우 어려운 과제이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;전역적으로 일관된(globally-consistent) 변형 추적(deformation tracking) 및 비강체(non-rigid) 객체의 3D 재구성을 위한 Neural Deformation Graphs를 제안한다.&lt;/p&gt;

    &lt;p&gt;특히 심층 신경망(deep neural network)을 통해 변형 그래프를 암시적으로 모델링한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;신경 변형 그래프(neural deformation graph)는 어떤 객체의 특정 구조에 의존하지 않으므로, 일반 비강성 변형 추정에 적용할 수 있다.&lt;/p&gt;

    &lt;p&gt;움직이지 않는 물체(non-rigidly moving)의 주어진 깊이 카메라 관찰 시퀀스에서 이러한 신경 그래프를 전역적으로 최적화한다.&lt;/p&gt;

    &lt;p&gt;명시적 관점에서의 일관성과 프레임간 그래프 및 표면 일관성 제약을 기반으로, 기본 네트워크가 자체 감독(self-supervised) 방식으로 학습된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;암시적으로 변형 가능한 다중 MLP 모양 표현을 사용하여, 객체의 기하학에 대해 추가로 최적화한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;순차적 입력 데이터를 가정하지 않으므로, 빠른 동작 또는 일시적으로 연결이 끊긴 녹음에 대해 강력한 추적을 가능하게 해준다.&lt;/li&gt;
  &lt;li&gt;실험을 통해, Neural Deformation Graphs가 64% 개선된 재구성과, 62% 개선된 변형 추적 성능임을 증명했다. 이는, 질적 및 양적으로 최첨단 비강체 재구성 접근 방식을 능가함을 보여주는 결과이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
    &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
단단하지 않은 변형 표면을 캡처하는 것은 역동적인 실제 환경을 재구성하는 데에 필수적이지만, 여전히 어려운 과제이다. Neural Deformation Graphs는 명시적 관점에서 일관성과 프레임간 그래프 및 표면 일관성 제약을 기반으로 self-supervised 방식으로 학습되는 기본 네트워크를 포함한다. 순차적인 입력 데이터를 가정하지 않으므로 빠른 동작을 가능하게 한다.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;deep-implicit-templates-for-3d-shape-representation&quot;&gt;&lt;strong&gt;Deep Implicit Templates for 3D Shape Representation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Zerong Zheng, Tao Yu, Qionghai Dai, Yebin Liu
(&lt;a href=&quot;https://arxiv.org/abs/2011.14565&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-07-22-computerVision-papers-CVPR202103/supp_vid.gif&quot; alt=&quot;supp_vid&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;일종의 3D 모양 표현인 Deep implicit functions (DIFs)은 컴팩트함과 강력한 표현력덕분에 3D vision 커뮤니티에서 점점 더 대중화되고 있다.&lt;/p&gt;

    &lt;p&gt;그러나 폴리곤 메쉬 기반 템플릿(polygon mesh-based templates)과 달리, DIF로 표현되는 모양 전반에 걸쳐 dense correspondences 혹은 다른 의미론적 관계(semantic relationships)를 추론하는 것은 여전히 도전 과제로 남아있으며, 이는 텍스처 전송, 모양 분석 등에 적용하는 것을 제한한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이러한 한계를 극복하고 DIF를 더 해석하기 쉽게 만들기 위해, deep implicit representations에서 명시적 대응 추론을 지원하는 Deep Implicit Templates라는 새로운 3D 모양 표현 방법을 제안한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;핵심 아이디어는, DIF를 template implicit function의 조건부 변형(conditional deformations)으로 공식화하는 것이다.&lt;/p&gt;

    &lt;p&gt;이를 위해, 조건부 공간 변환(conditional spatial transformation)을 다중 아핀 변환(multiple affine transformations)으로 분해하고, 일반화 능력을 보장하는 공간 왜곡 LSTM(Spatial Warping LSTM)을 제안한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;training loss는 unsupervised 방식에서 정확한 대응과 그럴듯한 템플릿을 학습하면서, 높은 재구성 정확도를 달성하기 위해 신중하게 설계되었다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;실험을 통해, 이러한 방식이 모양 컬렉션에 대한 공통적인 implicit template을 학습할 수 있을 뿐만 아니라, 어떠한 supervision 없이도 모든 모양에 대해 동시에 조밀한 대응을 설정할 수 있음을 밝혔다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
    &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
3D vision 분야에서 Deep implicit funtions는 컴팩트함과 강력한 표현력으로 인해 대중화되고있다. 하지만 DIF로 표현되는 모양 전반에 걸쳐 조밀한 대응(dense correspondences)과 의미론적 관계를 추론하는 것은 여전히 도전 과제이다. 이러한 한계를 극복하고자 DIF를 조건부 변형으로 공식화하는 Deep Implicit Templates라는 새로운 3D 모양 표현 방법을 제안하여 이러한 문제를 해결하였다.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;smplicit-topology-aware-generative-model-for-clothed-people&quot;&gt;&lt;strong&gt;SMPLicit: Topology-aware Generative Model for Clothed People&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Enric Corona, Albert Pumarola, Guillem Alenyà, Gerard Pons-Moll, Francesc Moreno-Noguer
   (&lt;a href=&quot;https://arxiv.org/abs/2103.06871&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.iri.upc.edu/people/ecorona/smplicit/teaser.png&quot; alt=&quot;img&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다양한 신체 형태와 포즈로 의복 스타일과 변형을 제어할 수 있는 차별화된 저차원 생성 모델 구축을 통해, 옷을 입은 사람의 디지털 애니메이션, 3D 콘텐츠 제작 및 가상 시승과 같은 여러 가지 흥미로운 애플리케이션에 문을 열 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결: SMPLicit&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;신체 포즈, 모양, 그리고 옷의 기하학을 공동으로 표현하는 신박한 생성 모델인 SMPLicit을 제안한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;각 유형의 의류에 대해 특정 모델을 학습해야하는 기존의 학습 기반 방식과 달리, SMPLicit은 의류의 사이즈나 타이트함/루즈함과 같은 다른 특성들을 제어하면서 다양한 의류 topologies(e.g., 민소매옷부터 후드티, 오픈 재킷까지)를 통일된 방식으로 표현할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이러한 모델을 티셔츠, 후드티, 재킷, 반바지, 바지, 스커트, 신발, 심지어 헤어를 포함한 다양한 의복에 적용할 수 있음을 보였다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SMPLicit의 표현 유연성(flexibility)은 SMPL 인체 매개변수와 의미론적으로 해석가능하며 의류 속성으로 정렬된 학습 가능한 잠재 공간으로 조건화된 암시적 모델(implicit model)을 기반으로 한다.&lt;/p&gt;

    &lt;p&gt;제안된 모델은 완전히 미분가능하므로, 더 큰 end-to-end 학습 가능한 시스템에 적용할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;실험을 통해, SMPLicit이 3D 스캔을 피팅하고, 옷 입음 사람의 이미지에서 3D 재구성에 쉽게 사용할 수 있음을 보여준다.&lt;/p&gt;

    &lt;p&gt;두 경우 모두, 복답한 의복의 형상을 검색하고, 여러 겹의 의복이 있는 상황을 처리하고, 손쉬운 의상 편집을 위한 도구를 제공함으로써 최첨단 기술을 뛰어넘을 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이러한 방향의 추가 연구를 위해서, [여기](http://www.iri.upc.edu/people/ecorona/smplicit/에 있는 코드와 모델을 공개적으로 사용할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
       &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
   	신체 포즈, 모양, 그리고 옷의 기하학을 표현하는 신박한 생성 모델인 SMPLicit을 통해 다양한 애플리케이션에 문을 열 수 있을 것이라 기대된다. 기존에는 각 유형의 의류에 대해 특정 모델을 학습해야했지만, SMPLicit은 다양한 의류 topologies를 통일된 방식으로 표현할 수 있다.
   &lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;picasso-a-cuda-based-library-for-deep-learning-over-3d-meshes&quot;&gt;&lt;strong&gt;Picasso: A CUDA-based Library for Deep Learning over 3D Meshes&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Huan Lei, Naveed Akhtar, Ajmal Mian
   (link)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;계층 신경 아키텍처(Hierarchical neural architectures)는 빠른 mesh decimation의 필요성을 나타내는 다중 스케일 추출에서 효과적이라 입증되었다.&lt;/p&gt;

    &lt;p&gt;그러나 기존의 방법은 다중 해상도 메쉬를 얻기 위해 CPU 기반 구현에 의존한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결: Picasso&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;복잡한 실제 3D 메쉬에 대한 딥러닝을 위한 새로운 모듈로 구성된 CUDA 기반 라이브러리인 Picasso를 소개한다. 이처럼 GPU 가속 mesh decimation을 설계하여, 네트워크 해상도 감소를 효율적으로 즉각 촉진할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pooling 및 unpooling 모듈은 decimation 동안 수집된 정점 클러스터(vertex clusters)로 정의된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Picasso는, 메쉬에 대한 특징 학습에 대해, 소위 facet2vertex, vertex2facet, 그리고 facet2facet convolution이라 불리는 세 가지 유형의 새로운 컨볼루션을 포함한다.&lt;/p&gt;

    &lt;p&gt;따라서 mesh를 기존의 방식처럼 모서리(edges)가 있는 공간 그래프가 아니라 꼭짓점(vertices)과 면(facets)으로 구성된 기하학적 구조로 취급한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Picasso는 메쉬 샘플링 (정점 밀도)에 대한 robustness를 위해 필터에 fuzzy mechanism을 통합한다.&lt;/p&gt;

    &lt;p&gt;Gaussian mixtures를 활용하여 facet2vertex convolution에 대한 fuzzy 계수를 정의하고, 무게 중심 보간(varycentric interpolation)을 활용하여 나머지 두 컨볼루션에 대한 계수를 정의한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이 release에서는, S3DIS에 대한 경쟁력있는 세분화 결과와 함께 제안된 모듈의 효율성을 보여주었다.&lt;/li&gt;
  &lt;li&gt;해당 라이브러리는 &lt;a href=&quot;https://github.com/hlei-ziyan/Picasso&quot;&gt;여기&lt;/a&gt;에서 공개하였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
       &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
   	계층 신경 아키텍처는 다중 스케일 추출에서 효과적이라 입증되었다. 하지만 기존의 방법은 CPU 기반으로 구현되었다. 이에, 복잡한 실제 3D mesh에 대한 딥러닝을 위한 새로운 모듈로 구성된 Picasso라는 CUDA GPU기반 라이브러리를 제안하였다.
   &lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;semi-supervised-synthesis-of-high-resolution-editable-textures-for-3d-humans&quot;&gt;&lt;strong&gt;Semi-supervised Synthesis of High-Resolution Editable Textures for 3D Humans&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Bindita Chaudhuri, Nikolaos Sarafianos, Linda Shapiro, Tony Tung
   (&lt;a href=&quot;https://arxiv.org/abs/2103.17266&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-07-22-computerVision-papers-CVPR202103/image-20210716142830429.png&quot; alt=&quot;image-20210716142830429&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;최근 AR/VR 기기 및 가상 커뮤니케이션의 사용이 증가함에 따라 3D Human Avatar 제작이 인기를 얻고 있다.&lt;/li&gt;
  &lt;li&gt;인체는 모양을 모델링한 3D 표면 메시와 3D 표면에 매핑된 모양을 인코딩한 텍스처 맵(UV 공간의 이미지)으로 표현되며, 이때 몰입적인 경험을 위해서는 아바타의 사실적인 질감이 매우 중요하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;semi-supervised 설정에서 3D human meshes에 대한 다양한 고충실도(high fidelity) 텍스처 맵을 생성하는 새로운 접근 방식이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;텍스처맵에서 의미 영역(semantic regions)의 레이아웃을 정의하는 분할 마스크(segmentation mask)가 주어졌을 때, 제안된 네트워크는 렌더링을 목적으로 사용되는 다양한 스타일을 갖는 고해상도 텍스처를 생성한다.&lt;/p&gt;

    &lt;p&gt;이를 달성하기 위해, 각 영역의 스타일 확률 분포를 개별적으로 학습하여 지역별 분포로부터 샘플링하여 생성된 텍스처의 스타일을 제어할 수 있는 Region-adaptive Adverserial Variational AutoEncoder(ReAVAE)를 제안한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;또한, single-view RGB 입력으로부터 가져온 데이터로 학습 세트를 보강(augment)할 수 있는 데이터 생성 기술을 제안한다.&lt;/p&gt;

    &lt;p&gt;이러한 학습 전략은 참조 이미지 스타일을 다양한 영역에 대한 임의의 스타일과 혼합할 수 있도록 하며, 이는 가상 체험 AR/VR 어플리케이션에 유용한 속성이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;실험을 통해, 이러한 방식이 이전의 작업과 비교했을 때 더 나은 텍스처맵을 합성함과 동시에 독립적인 레이아웃과 스타일 제어 능력을 가능하게함을 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
       &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
   	레이아웃을 정의하는 분할 마스크가 주어졌을 때, 다양한 스타일을 갖는 고해상도 텍스처를 생성하는 접근 방식을 제안한다. 각 스타일 확률 분포를 개별적으로 학습하여, 생성된 텍스처의 스타일을 제어할 수 있는 Region-adaptive Adverserial Variational AutoEncoder(ReAVAE)를 통해 달성할 수 있다. 또한 signle-view RGB 입력으로부터 학습 세트를 보강할 수 있는 데이터 생성 기술을 제안한다. 이를 통해 더 나은 텍스처맵을 합성할 수 있으며, 독립적인 레이아웃과 스타일 제어 능력을 보여주었다.
   &lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rgb-d-local-implicit-function-for-depth-completion-of-transparent-objects&quot;&gt;&lt;strong&gt;RGB-D Local Implicit Function for Depth Completion of Transparent Objects&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Luyang Zhu, Arsalan Mousavian, Yu Xiang, Hammad Mazhar, Jozef van Eenbergen, Shoubhik Debnath, Dieter Fox
   (&lt;a href=&quot;https://arxiv.org/abs/2104.00622&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://research.nvidia.com/sites/default/files/publications/teaser_6.png&quot; alt=&quot;img&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;로봇공학에서 대부분의 인지(perception) 방법은 RGB-D 카메라에 의한 깊이 정보를 필요로 한다. 그러나, 표준 3D 센서는 빛의 굴절(refraction)와 흡수(absorption) 문제로 인해 투명한 물체에 대한 깊이 정보를 취득하지 못한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;접근 방식의 핵심은 제안된 방식이 보이지 않는 물체를 일반화하고 빠른 추론 속도를 달성하도록 하는 ray-voxel pairs 기반 local implicit neural representation이다. 이러한 표현법에 기반하여, 노이지한 RGB-D 입력이 주어졌을 때, 누락된 깊이 정보를 완성할 수 있는 신박한 프레임워크를 제안한다.&lt;/p&gt;

    &lt;p&gt;또한, self-correcting refinement model을 사용하여 반복적으로 깊이 추정을 더욱 개선할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;전체 파이프라인을 학습하기 위해, 투명한 객체를 갖는 대규모 합성 데이터 세트를 구축하였다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;실험을 통해, 이러한 방법이 합성 데이터와 실제 데이터 모두에서 최신 최첨단 기법을 능가하는 성능을 보임을 밝혔다. 또한, 이전의 가장 뛰어난 방법인 ClearGrasp와 비교했을 때 추론 속도를 20배 향상시킴을 보였다.&lt;/li&gt;
  &lt;li&gt;코드와 데이터세트는 &lt;a href=&quot;https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit&quot;&gt;여기&lt;/a&gt;에서 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
       &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
   	로봇 공학에서 대부분의 인지 기법은 깊이 정보를 필요로하지만, RGB-D 카메라의 한계로 인해 투명한 물체에 대한 깊이 정보는 취득하지 못한다. 이에 ray-voxel pairs 기반 local implicit neural representation을 통해 누락된 깊이 정보를 완성할 수 있는 프레임워크를 제안한다. 실험을 통해 해당 방법이 합성 데이터와 실제 데이터 모두에서 뛰어남을 증명하였다.
   &lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;deep-two-view-structure-from-motion-revisited&quot;&gt;&lt;strong&gt;Deep Two-View Structure-from-Motion Revisited&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Jianyuan Wang, Yiran Zhong, Yuchao Dai, Stan Birchfield, Kaihao Zhang, Nikolai Smolyanskiy, Hongdong Li
   (&lt;a href=&quot;https://arxiv.org/abs/2104.00556&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-07-22-computerVision-papers-CVPR202103/image-20210716152328782.png&quot; alt=&quot;image-20210716152328782&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Two-view structure-from-motion(SfM)은 3D 재구성 및 시각적 SLAM의 초석이다.&lt;/li&gt;
  &lt;li&gt;기존의 딥러닝 기반 방식은 이러한 문제를 두 개의 연속 프레임으로 부터 절대적인 포즈 스케일을 복구하거나, 단일 이미지로부터 깊이 맵을 예측하는 방식으로 해결했다. 하지만 두 방법 모두 잘못된 문제이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;고전적인 파이프라인의 좋은 자세(well-posedness)를 활용하여 깊은 두 시점 SfM의 문제를 재검토할 것을 제안한다.&lt;/li&gt;
  &lt;li&gt;제안된 방법은 다음으로 구성된다.
    &lt;ol&gt;
      &lt;li&gt;두 프레임 사이의 조밀한 대응을 예측하는 광류(optical flow) 추정 네트워크&lt;/li&gt;
      &lt;li&gt;2D 광류 대응으로부터 상대적인 카메라 포즈를 계산하는 정규화된 포즈 추정 모듈&lt;/li&gt;
      &lt;li&gt;탐색 공간을 줄이고, 조밀한 대응을 정제하고, 상대적인 깊이맵을 추정하기위해 에피폴라 기하학(epipolar geometry)을 활용하는 스케일 불변 깊이 추정 네트워크&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;광범위한 실험을 통해, 제안된 방법이 상대적 포즈와 깊이 추정 모두에 대해 KITTI depth, KITTI VO, MVS, Scenes11, 그리고 SUN3D 데이터세트에 대한 명확한 마진으로 모든 최첨단 두 시점 SfM 방법을 능가함을 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
       &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
   	SfM은 3D 재구성과 visual SLAM의 초석이다.그러나  기존의 딥러닝 기반 방법은 SfM 문제를 잘못해결했으며, 이에 고전적인 파이프라인 well-posedness를 활용하여 SfM 문제를 재검토할 것을 제안한다. 실험을 통해 상대적 포즈와 깊이 추정 모두에서 최첨단 방법을 능가했음을 보였다.
   &lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;deformed-implicit-field-modeling-3d-shapes-with-learned-dense-correspondence&quot;&gt;&lt;strong&gt;Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Yu Deng, Jiaolong Yang, Xin Tong
   (&lt;a href=&quot;https://arxiv.org/abs/2011.13650&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-07-22-computerVision-papers-CVPR202103/image-20210716154006096.png&quot; alt=&quot;image-20210716154006096&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;동일한 클래스의 3D 객체는 몇 가지 공통 모양 특징과 의미적 대응성을 공유한다. 따라서 형상 이해(shape understanding), 재구성(reconstruction), 조작(manipulation), 이미지 합성(image synthesis)과 같은 3D 및 2D 영역에서 다양한 다운스트림 작업에 유용한 변형 형상 모델(deformable shape model)을 구축하는 데 사용될 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결: Deformed Implicit Field(DIF)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;카테고리의 3D 모양을 모델링하고 모양간의 dense correspondences를 생성하기 위한 새로운 Deformed Implicit Field(DIF) 표현법을 제안한다.&lt;/li&gt;
  &lt;li&gt;DIF를 이용하여, 3D 모양을 각 모양 인스턴스에 대한 3D 변형 필드 및 수정 필드와 함께 카테고리 전체에서 공유되는 template implicit field로 표현할 수 있다. 모양 일치는 변형 필드를 사용하여 쉽게 설정할 수 있다.&lt;/li&gt;
  &lt;li&gt;DIF-Net이라 불리는 제안된 신경망은, 어떠한 대응이나 부분 라벨도 없이 카테고리에 속한 3D 객체에 대한 모양 잠재 공간과 이러한 필드를 공동으로 학습한다. 학습된 DIF-Net은 또한 모양 구조 불일치를 반영하는 신뢰할 수 있는 대응 불확실성 측정을 제공할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;실험을 통해, DIF-Net이 충실도가 높은 3D 모양을 생성할 뿐만 아니라 다양한 모양에 걸쳐 고품질의 조밀한 대응 관계를 구축함을 보였다.&lt;/li&gt;
  &lt;li&gt;이러한 방법이 이전 방법으로는 달성할 수 없었던 텍스처 전송(texture transfer) 및 모양 편집(shape editing)과 같은 여러 응용 프로그램을 시연함을 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
       &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
   	동일한 클래스의 3D 객체는 변형 형상 모델(deformable shape model)을 구축하는 데에 사용될 수 있다. 카테고리에서 3D 모양을 모델링하고 모양간 dense correspondences를 생성하기 위한 새로운 Deformed Implicit Field(DIF) 표현법을 통해, 카테고리 전체에서 공유되는 3D 모양을 생성할 뿐만 아니라 다양한 모양에 걸쳐 고품질의 dense correspondences를 구축할 수 있다.
   &lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;s3-neural-shape-skeleton-and-skinning-fields-for-3d-human-modeling&quot;&gt;&lt;strong&gt;S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, Raquel Urtasun
(&lt;a href=&quot;https://arxiv.org/abs/2101.06571&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-07-22-computerVision-papers-CVPR202103/image-20210717111102745.png&quot; alt=&quot;image-20210717111102745&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;인간을 구성하고 애니메이션에 적용하는 것은 VR 혹은 시뮬레이션에서 로봇공학 테스트와 같이 다양한 응용 분야에서 가상 세계를 구축하는 데에 중요한 구성 요소이다. 다양한 모양, 자세, 그리고 옷을 가진 인간의 변형이 기하급수적으로 많으므로, 실제 데이터로부터 대규모로 인간을 자동으로 재구성하고 애니메이션할 수 있는 방법을 개발하는 것은 중요하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;보행자의 모양, 자세, 그리고 스키닝 가중치(skinning weights)를 데이터로부터 직접적으로 학습된 neural implicit functions로 표현하는 방법을 제안한다.&lt;/p&gt;

    &lt;p&gt;이러한 표현 방법은 human paremetric body model에 명시적으로 맞추지(fit) 않으면서 다양한 보행자의 모양과 자세를 처리할 수 있으므로, 더 넓은 범위의 인간 기하학과 토폴로지를 처리할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다양한 데이터 세트에 대해 이러한 접근 방식이 효율적임을 보였다.&lt;/li&gt;
  &lt;li&gt;이러한 재구성이 기존의 최첨단 방법을 능가함을 보였다.&lt;/li&gt;
  &lt;li&gt;재애니메이션(re-animation) 실험을 통해, 단일 RGB 이미지 (및/또는 선택적인 LiDAR sweep)을 입력으로부터 3D 인간 애니메이션을 대규모로 생성할 수 있음을 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
    &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
    단일 RGB 이미지 (및/또는 optional LiDAR sweep) 입력으로 부터 보행자의 모양, 자세, 그리고 skinning weights을 직접적으로 학습하여 neural implicit function으로 표현하는 방법을 제안하였다.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;plan2scene-converting-floorplans-to-3d-scenes&quot;&gt;&lt;strong&gt;Plan2Scene: Converting Floorplans to 3D Scenes&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa, Angel X. Chang, Manolis Savva
(&lt;a href=&quot;https://arxiv.org/abs/2106.05375&quot;&gt;link&lt;/a&gt;, &lt;a href=&quot;https://3dlg-hcvc.github.io/plan2scene/&quot;&gt;project&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-07-22-computerVision-papers-CVPR202103/image-20210717112544062.png&quot; alt=&quot;image-20210717112544062&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;해결: Plan2Scene&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Plan2Scene은 평면도와 주택 관련 사진 세트를 질감이 있는 3D mesh 모델로 변환하는 작업이다.&lt;/li&gt;
  &lt;li&gt;제안된 시스템은 1) 평면도 이미지를 3D mesh 모델로 들어올리며(lift), 2) 입력 사진을 기반으로 표면 질감을 합성하고, 3) 그래프 신경망 아키텍처를 사용하여 관찰되지 않은 표면에 대한 텍스처를 추론한다.&lt;/li&gt;
  &lt;li&gt;시스템을 훈련하고 평가하기 위해 실내 표면 텍스처 데이터 세트를 만들고, 수정된 표면 작물(surface crops) 및 추가적인 주석을 통해 이전 작업의 평면도 및 사진 데이터 세트를 보강(augment)한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;제안된 접근 방식은 거주지를 일부가 덮인 정렬되지 않은 드문드문한 사진 세트로부터 바닥, 벽, 그리고 천장과 같은 지배적인 표면에 대해 타일링 가능한 텍스처를 생성하는 문제를 처리한다.&lt;/li&gt;
  &lt;li&gt;정성적·정량적 평가를 통해, 이러한 시스템이 사실적인 3D 인테리어 모델을 생성하고, 텍스처 품질 메트릭 세트에 대한 baseline 접근 방식을 능가함을 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
    &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
    평면도 사진과 거주지의 일부 사진세트로부터 바닥, 벽, 그리고 천장과 같은 타일링 가능한 텍스처를 생성하는 시스템을 제안한다. 이를 통해 사실적인 3D 인테리어 모델을 생성할 수 있으며, 텍스처 품질 메트릭 세트에 대한 baseline 접근 방식을 능가할 수 있다.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;shelf-supervised-mesh-prediction-in-the-wild&quot;&gt;&lt;strong&gt;Shelf-Supervised Mesh Prediction in the Wild&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Yufei Ye, Shubham Tulsiani, Abhinav Gupta
(&lt;a href=&quot;https://arxiv.org/abs/2102.06195&quot;&gt;link&lt;/a&gt;, &lt;a href=&quot;https://judyye.github.io/ShSMesh/&quot;&gt;project&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-07-22-computerVision-papers-CVPR202103/image-20210718112911180.png&quot; alt=&quot;image-20210718112911180&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;대부분의 야생에서의 컴퓨터 비전 시스템은 여전히 2D 의미 인식(분류/탐지)(2D semantic recognition)을 수행한다.&lt;/li&gt;
  &lt;li&gt;최근 2D 인식의 발전은 지도 학습(supervised learning)에서 비롯되었지만, 2D semantic tasks와 달리 3D understanding에 대해 감독(supervision)을 받는 것은 여전히 확장 가능하지 않다.&lt;/li&gt;
  &lt;li&gt;최근 2D 접근방식의 감독을 갖는 3D 접근 방식을 구축하려는 시도가 있었으나, 초기 접근법은 다중 뷰 감독을 사용하는 것에 초점을 맞추었다. 그러나 동일한 개체/장면에 대한 다중 뷰를 얻는 것은 쉽지 않다는 문제가 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;단일 이미지로부터 객체의 3D 모양과 자세을 추론하는 것을 돕고, off-the-shelf 인식 시스템(i.e. ‘shelf-supervised’)의 분할 출력만으로 감독되는 구조화되지 않은 이미지 컬렉션으로부터 학습할 수 있는 학습기반 접근방식을 제안한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;먼저, 카메라 포즈와 함께 표준 프레임에서 체적 표현(volumetric representation)을 추론한다. 모양과 마스크 모두와 기하학적으로 일치하는 표현을 시행하고, 합성된 새로운 뷰가 이미지 컬렉션과 구별되지 않도록 한다. 거친 체적 예측(the coarse volumetric predictions)은 메쉬 기반 표현으로 변환되어, 예측된 카메라 프레임에서 더욱 세분화된다.&lt;/p&gt;

    &lt;p&gt;이러한 두 단계를 통해 이미지 컬렉션의 모양-포즈 인수분해와 인스턴스별 재구성을 보다 세부적으로 수행할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;합성 데이터 세트와 실제 데이터 세트 모두에서 방법을 조사하고, 기존 작업보다 훨씬 더 많은 클래스인 야생에서의 50가지 범주에서 확장성을 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
    &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
    기존의 2D 접근 방식의 감독을 갖는 3D 접근 방식은 다중 뷰 감독을 사용하는 것에 초점을 맞추었지만, 동일한 개체/장면에 대해 다중 뷰를 갖는 것은 쉽지 않다. 단일 이미지로부터 3D 모양과 자세를 추론하는 것을 돕고, off-the-shelf 인식 시스템의 분할 출력만으로 감독되는 학습 기반 3D 접근 방식을 통해 이러한 문제에 접근할 수 있다. 실험 결과, 기존 작업보다 훨씬 더 많은 클래스에 대해 확장성을 보였다.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;unsupervised-learning-of-3d-object-categories-from-videos-in-the-wild&quot;&gt;&lt;strong&gt;Unsupervised Learning of 3D Object Categories from Videos in the Wild&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman Shapovalov, Tobias Ritschel, Andrea Vedaldi, David Novotny
(&lt;a href=&quot;https://arxiv.org/abs/2103.16552&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/2021-07-22-computerVision-papers-CVPR202103/image-20210719114117898.png&quot; alt=&quot;image-20210719114117898&quot; style=&quot;width:600px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;최근 합성 데이터를 사용하거나 키포인트와 같은 2D 기본 요소가 사용가능하다고 가정하여 유사한 결과를 얻었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문은 수동적인 주석 없이 도전적인 실제 데이터로 학습하는 방식에 주목했다. 따라서 대규모 객체 인스턴스 컬렉션의 다중 뷰로부터 모델을 학습하는 것에 중점을 두었다.&lt;/li&gt;
  &lt;li&gt;주어진 카테고리의 객체에 대한 적은 수의 이미지가 주어졌을 때, 그것을 3D로 재구성하는 심층 네트워크를 학습하는 것을 목표로 한다.&lt;/li&gt;
  &lt;li&gt;모델들의 이러한 클래스를 훈련하고 벤치마킹하는 데에 적합한 객체 중심 비디오의 새로운 대규모 데이터 세트를 제공한다.&lt;/li&gt;
  &lt;li&gt;마지막으로 WCR(warp-conditional ray embedding)이라는 새로운 신경망 설계를 제안한다. WCR은 객체의 표면과 질감의 상세한 implicit representation을 얻는 동안 재구성을 크게 개선하고, 학습 프로세스를 부트스트랩(bootstrap)한 초기 SfM 재구성 노이즈를 보상한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;결론&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;격리된 객체를 재구성하는 데에 잘 작동하는 메쉬, 복셀, 또는 암시적인 표면(implicit surfaces)을 활용하는 기존의 기술은 제안된 대규모 데이터 세트에 적합하지 않다.&lt;/li&gt;
  &lt;li&gt;기존 벤치마크와 새로운 데이터 세트에 대한 여러 심층 단안 재구성 기준선(deep monocular reconstruction baselines)에 대한 성능 향상을 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;callout&quot; style=&quot;background-color:#E9E9E9; color:#808080; border-radius:10px; padding:5px 15px;&quot;&gt;
    &lt;span style=&quot;font-weight:bold;&quot;&gt;요약&lt;/span&gt;&lt;br /&gt;
    최근의 3D reconstruction 문제는 합성 데이터를 사용하거나, 키포인트와 같은 2D 기본 요소가 사용가능하다 가정하여 문제를 해결하였다. 본 논문은 대규모 객체 인스턴스 컬렉션의 다중 뷰가 주어졌을 때, 수동적인 주석 없이 실제 데이터로 학습하는 도전적인 방법에 주목하였다. 주어진 카테고리에 대해 적은 수의 이미지가 주어졌을 때 이를 3D로 재구성하는 심층 네트워크로 학습하여 이를 달성한다.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
    ​&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;CVPR 2021: &lt;a href=&quot;http://cvpr2021.thecvf.com/&quot;&gt;http://cvpr2021.thecvf.com/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;brunch/kakao-it: &lt;a href=&quot;https://brunch.co.kr/@kakao-it/297&quot;&gt;https://brunch.co.kr/@kakao-it/297&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;CVPR-2021-Paper-Satistics : &lt;a href=&quot;https://github.com/hoya012/CVPR-2021-Paper-Statistics?fbclid=IwAR0MGG3x-9bDU8YjVp-UGHcJDAXUspNwJ3Iy-o17oi7UFbTSFGcKS_OqbaQ&quot;&gt;https://github.com/hoya012/CVPR-2021-Paper-Statistics?fbclid=IwAR0MGG3x-9bDU8YjVp-UGHcJDAXUspNwJ3Iy-o17oi7UFbTSFGcKS_OqbaQ&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;52CV/CVPR-2021-Papers: &lt;a href=&quot;https://github.com/52CV/CVPR-2021-Papers&quot;&gt;https://github.com/52CV/CVPR-2021-Papers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;amusi/CVPR2021-Papers-with-Code: &lt;a href=&quot;https://github.com/amusi/CVPR2021-Papers-with-Code&quot;&gt;https://github.com/amusi/CVPR2021-Papers-with-Code&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 22 Jul 2021 00:00:00 +0900</pubDate>
        <link>https://dazory.github.io//ai-computervision/CVPR202103/</link>
        <guid isPermaLink="true">https://dazory.github.io//ai-computervision/CVPR202103/</guid>
        
        <category>Computer Vision</category>
        
        <category>CVPR</category>
        
        <category>Papers</category>
        
        <category>Review</category>
        
        <category>3D Vision</category>
        
        
        <category>ai-computerVision</category>
        
      </item>
    
  </channel>
</rss>